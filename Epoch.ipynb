{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "gWEOQlBReKfG",
        "zKdbEpn5xp75"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIAO9aFM9Bd-",
        "outputId": "6ee52496-07fa-4db6-e155-23095ddf8360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/drive/MyDrive/Colab\\ Notebooks/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XxH24e9j9c-k",
        "outputId": "48456020-f75c-4a05-e626-67bfc9534859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==1.2.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 1))\n",
            "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/123.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 2)) (1.6.3)\n",
            "Collecting cachelib==0.9.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 3))\n",
            "  Downloading cachelib-0.9.0-py3-none-any.whl (15 kB)\n",
            "Collecting cachetools==5.2.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 4))\n",
            "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting certifi==2022.6.15 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 5))\n",
            "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer==2.1.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 6))\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: click==8.1.3 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 7)) (8.1.3)\n",
            "Collecting colorama==0.4.5 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 8))\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: cycler==0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 9)) (0.11.0)\n",
            "Collecting Flask==2.2.2 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 10))\n",
            "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Flask-MySQLdb==1.0.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 11))\n",
            "  Downloading Flask-MySQLdb-1.0.1.tar.gz (4.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Flask-Session==0.4.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 12))\n",
            "  Downloading Flask_Session-0.4.0-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting flatbuffers==1.12 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 13))\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting fonttools==4.37.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 14))\n",
            "  Downloading fonttools-4.37.1-py3-none-any.whl (957 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m957.2/957.2 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 15)) (0.4.0)\n",
            "Collecting google-auth==2.11.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 16))\n",
            "  Downloading google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.2/167.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib==0.4.6 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 17))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 18)) (0.2.0)\n",
            "Collecting grpcio==1.47.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 19))\n",
            "  Downloading grpcio-1.47.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py==3.7.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 20))\n",
            "  Downloading h5py-3.7.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==3.3 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 21))\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous==2.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 22)) (2.1.2)\n",
            "Requirement already satisfied: Jinja2==3.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 23)) (3.1.2)\n",
            "Collecting keras==2.9.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 24))\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Keras-Preprocessing==1.1.2 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 25))\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver==1.4.4 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 26)) (1.4.4)\n",
            "Collecting libclang==14.0.6 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 27))\n",
            "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Markdown==3.4.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 28))\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe==2.1.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 29))\n",
            "  Downloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting matplotlib==3.5.3 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 30))\n",
            "  Downloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mysqlclient==2.1.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 31))\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.23.2 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 32))\n",
            "  Downloading numpy-1.23.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oauthlib==3.2.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 33))\n",
            "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.5/151.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.6.0.66 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 34))\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 35)) (3.3.0)\n",
            "Collecting packaging==21.3 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 36))\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==9.2.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 37))\n",
            "  Downloading Pillow-9.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.19.4 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 38))\n",
            "  Downloading protobuf-3.19.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyasn1==0.4.8 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 39))\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyasn1-modules==0.2.8 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 40))\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing==3.0.9 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 41))\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 42)) (2.8.2)\n",
            "Collecting requests==2.28.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 43))\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 44)) (1.3.1)\n",
            "Requirement already satisfied: rsa==4.9 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 45)) (4.9)\n",
            "Requirement already satisfied: six==1.16.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 46)) (1.16.0)\n",
            "Collecting tensorboard==2.9.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 47))\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server==0.6.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 48))\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit==1.8.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 49))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.9.1 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 50))\n",
            "  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==2.9.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 51))\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.26.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 52))\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor==1.1.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 53))\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing_extensions==4.3.0 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 54))\n",
            "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
            "Collecting urllib3==1.26.12 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 55))\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Werkzeug==2.2.2 (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 56))\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt==1.14.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 57)) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse==1.6.3->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 2)) (0.40.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.9.1->-r /content/drive/MyDrive/Colab Notebooks/requirements.txt (line 47)) (67.7.2)\n",
            "Building wheels for collected packages: Flask-MySQLdb, mysqlclient, termcolor\n",
            "  Building wheel for Flask-MySQLdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Flask-MySQLdb: filename=Flask_MySQLdb-1.0.1-py3-none-any.whl size=4673 sha256=58ece73db6bb37f3df5cf2c66c02bb8427f60f66d849c030b9d25d30f182b3c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/7f/62/bb7b4e2a4a6324b42bc6cf1f7dc7946781dbe81f2ce1e59bee\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp310-cp310-linux_x86_64.whl size=108355 sha256=65a8509752f765a02a3cb64a4e8b7b07565787cbab906fd9c30ce2dd570f0fed\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/34/ba/a769c165b01646816afdf9bf792e847ef149693fee432b6b65\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=62e4e1dd463026c803a5e6f3e65437e3bb8704a1c7e313ea549bb5d5323c8d9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "Successfully built Flask-MySQLdb mysqlclient termcolor\n",
            "Installing collected packages: termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, urllib3, typing_extensions, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pyasn1-modules, protobuf, Pillow, oauthlib, numpy, mysqlclient, MarkupSafe, Markdown, idna, grpcio, fonttools, colorama, charset-normalizer, certifi, cachetools, cachelib, absl-py, Werkzeug, requests, packaging, opencv-python, Keras-Preprocessing, h5py, google-auth, matplotlib, Flask, google-auth-oauthlib, Flask-Session, Flask-MySQLdb, tensorboard, tensorflow\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.3.0\n",
            "    Uninstalling termcolor-2.3.0:\n",
            "      Successfully uninstalled termcolor-2.3.0\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.5.0\n",
            "    Uninstalling pyasn1-0.5.0:\n",
            "      Successfully uninstalled pyasn1-0.5.0\n",
            "  Attempting uninstall: libclang\n",
            "    Found existing installation: libclang 16.0.0\n",
            "    Uninstalling libclang-16.0.0:\n",
            "      Successfully uninstalled libclang-16.0.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.5.26\n",
            "    Uninstalling flatbuffers-23.5.26:\n",
            "      Successfully uninstalled flatbuffers-23.5.26\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.16\n",
            "    Uninstalling urllib3-1.26.16:\n",
            "      Successfully uninstalled urllib3-1.26.16\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.6.3\n",
            "    Uninstalling typing_extensions-4.6.3:\n",
            "      Successfully uninstalled typing_extensions-4.6.3\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.32.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.32.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.32.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.1\n",
            "    Uninstalling tensorboard-data-server-0.7.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.0\n",
            "    Uninstalling pyparsing-3.1.0:\n",
            "      Successfully uninstalled pyparsing-3.1.0\n",
            "  Attempting uninstall: pyasn1-modules\n",
            "    Found existing installation: pyasn1-modules 0.3.0\n",
            "    Uninstalling pyasn1-modules-0.3.0:\n",
            "      Successfully uninstalled pyasn1-modules-0.3.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.2\n",
            "    Uninstalling oauthlib-3.2.2:\n",
            "      Successfully uninstalled oauthlib-3.2.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.3\n",
            "    Uninstalling MarkupSafe-2.1.3:\n",
            "      Successfully uninstalled MarkupSafe-2.1.3\n",
            "  Attempting uninstall: Markdown\n",
            "    Found existing installation: Markdown 3.4.3\n",
            "    Uninstalling Markdown-3.4.3:\n",
            "      Successfully uninstalled Markdown-3.4.3\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.56.0\n",
            "    Uninstalling grpcio-1.56.0:\n",
            "      Successfully uninstalled grpcio-1.56.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.40.0\n",
            "    Uninstalling fonttools-4.40.0:\n",
            "      Successfully uninstalled fonttools-4.40.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.0.12\n",
            "    Uninstalling charset-normalizer-2.0.12:\n",
            "      Successfully uninstalled charset-normalizer-2.0.12\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.5.7\n",
            "    Uninstalling certifi-2023.5.7:\n",
            "      Successfully uninstalled certifi-2023.5.7\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.1\n",
            "    Uninstalling cachetools-5.3.1:\n",
            "      Successfully uninstalled cachetools-5.3.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 2.3.6\n",
            "    Uninstalling Werkzeug-2.3.6:\n",
            "      Successfully uninstalled Werkzeug-2.3.6\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.1\n",
            "    Uninstalling packaging-23.1:\n",
            "      Successfully uninstalled packaging-23.1\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.7.0.72\n",
            "    Uninstalling opencv-python-4.7.0.72:\n",
            "      Successfully uninstalled opencv-python-4.7.0.72\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.8.0\n",
            "    Uninstalling h5py-3.8.0:\n",
            "      Successfully uninstalled h5py-3.8.0\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.3\n",
            "    Uninstalling google-auth-2.17.3:\n",
            "      Successfully uninstalled google-auth-2.17.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 2.2.5\n",
            "    Uninstalling Flask-2.2.5:\n",
            "      Successfully uninstalled Flask-2.2.5\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-api-core 2.11.1 requires google-auth<3.0.dev0,>=2.14.1, but you have google-auth 2.11.0 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-bigquery 3.10.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.20.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-functions 1.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-language 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-cloud-translate 3.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 2.11.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.28.1 which is incompatible.\n",
            "googleapis-common-protos 1.59.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "grpc-google-iam-v1 0.12.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.19.4 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.47.0 which is incompatible.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow-hub 0.13.0 requires protobuf>=3.19.6, but you have protobuf 3.19.4 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Flask-2.2.2 Flask-MySQLdb-1.0.1 Flask-Session-0.4.0 Keras-Preprocessing-1.1.2 Markdown-3.4.1 MarkupSafe-2.1.1 Pillow-9.2.0 Werkzeug-2.2.2 absl-py-1.2.0 cachelib-0.9.0 cachetools-5.2.0 certifi-2022.6.15 charset-normalizer-2.1.1 colorama-0.4.5 flatbuffers-1.12 fonttools-4.37.1 google-auth-2.11.0 google-auth-oauthlib-0.4.6 grpcio-1.47.0 h5py-3.7.0 idna-3.3 keras-2.9.0 libclang-14.0.6 matplotlib-3.5.3 mysqlclient-2.1.1 numpy-1.23.2 oauthlib-3.2.0 opencv-python-4.6.0.66 packaging-21.3 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 requests-2.28.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing_extensions-4.3.0 urllib3-1.26.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "google",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GPU_TYPE'] = str(os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read())\n",
        "\n",
        "def getGPUArch(argument):\n",
        "  try:\n",
        "    argument = argument.strip()\n",
        "    # All Colab GPUs\n",
        "    archTypes = {\n",
        "        \"Tesla V100-SXM2-16GB\": \"-gencode arch=compute_70,code=[sm_70,compute_70]\",\n",
        "        \"Tesla K80\": \"-gencode arch=compute_37,code=sm_37\",\n",
        "        \"Tesla T4\": \"-gencode arch=compute_75,code=[sm_75,compute_75]\",\n",
        "        \"Tesla P40\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P4\": \"-gencode arch=compute_61,code=sm_61\",\n",
        "        \"Tesla P100-PCIE-16GB\": \"-gencode arch=compute_60,code=sm_60\"\n",
        "\n",
        "      }\n",
        "    return archTypes[argument]\n",
        "  except KeyError:\n",
        "    return \"GPU must be added to GPU Commands\"\n",
        "os.environ['ARCH_VALUE'] = getGPUArch(os.environ['GPU_TYPE'])\n",
        "\n",
        "print(\"GPU Type: \" + os.environ['GPU_TYPE'])\n",
        "print(\"ARCH Value: \" + os.environ['ARCH_VALUE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU7Rfm-h-o10",
        "outputId": "f6f6d7be-cb81-42c6-8c1c-929eff539733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Type: Tesla T4\n",
            "\n",
            "ARCH Value: -gencode arch=compute_75,code=[sm_75,compute_75]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMvNnrv2-rE-",
        "outputId": "08cb4e17-1c97-4d3c-c6b3-cd8425838976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul  2 09:41:40 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Reshape, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.resnet import ResNet50\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import itertools\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "8Bhitbv7-t_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Path ke file ZIP\n",
        "zip_path = '/content/drive/MyDrive/bagi.zip'\n",
        "\n",
        "# Path untuk mengekstrak file ZIP\n",
        "extract_path = '/content/drive/MyDrive/SplitDataset/'\n",
        "\n",
        "# Membuka file ZIP\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    # Mengetahui total jumlah file yang akan diekstrak\n",
        "    total_files = len(zip_ref.namelist())\n",
        "\n",
        "    # Menggunakan tqdm untuk menampilkan loading progress\n",
        "    with tqdm(total=total_files, desc='Extracting ZIP') as pbar:\n",
        "        # Mengekstrak seluruh isi file ZIP ke direktori tujuan\n",
        "        for file in zip_ref.namelist():\n",
        "            zip_ref.extract(file, extract_path)\n",
        "            pbar.update(1)\n",
        "\n",
        "print(\"File ZIP telah berhasil diekstrak.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZJncKnV-zbd",
        "outputId": "6e687962-fce5-4856-889b-fc1e5b783450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting ZIP: 100%|██████████| 2513/2513 [00:14<00:00, 170.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ZIP telah berhasil diekstrak.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Train :\")\n",
        "print(os.listdir('/content/drive/MyDrive/SplitDataset/bagi/train'))\n",
        "print(\"Dataset Validasi\")\n",
        "print(os.listdir('/content/drive/MyDrive/SplitDataset/bagi/validation'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok8CefuZAHu9",
        "outputId": "73f7cd42-caa2-47e6-a94a-142e83ca5568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Train :\n",
            "['Pa Lulun Pao', 'Pa Somba', 'Pa Tangke Lumu', 'Pa Tumuru', 'Tidak Terdeteksi']\n",
            "Dataset Validasi\n",
            "['Pa Lulun Pao', 'Pa Somba', 'Pa Tangke Lumu', 'Pa Tumuru', 'Tidak Terdeteksi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Direktori train\n",
        "train_directory = \"/content/drive/MyDrive/SplitDataset/bagi/train\"\n",
        "# Direktori validasi\n",
        "valid_directory = \"/content/drive/MyDrive/SplitDataset/bagi/validation\"\n",
        "\n",
        "# Mendapatkan daftar kelas dalam direktori train\n",
        "train_classes = os.listdir(train_directory)\n",
        "\n",
        "# Mendapatkan daftar kelas dalam direktori validasi\n",
        "valid_classes = os.listdir(valid_directory)\n",
        "\n",
        "# Menampilkan nama kelas dan jumlah file dalam direktori train\n",
        "print(\"Total jumlah file dalam setiap kelas di direktori train:\")\n",
        "total_train_files = 0\n",
        "for class_name in train_classes:\n",
        "    class_path = os.path.join(train_directory, class_name)\n",
        "    num_files = len(os.listdir(class_path))\n",
        "    total_train_files += num_files\n",
        "    print(f\"Kelas: {class_name}, Jumlah file: {num_files}\")\n",
        "\n",
        "# Menampilkan total jumlah file dalam direktori train\n",
        "print(f\"Total jumlah file dalam direktori train: {total_train_files}\\n\")\n",
        "\n",
        "# Menampilkan nama kelas dan jumlah file dalam direktori validasi\n",
        "print(\"Total jumlah file dalam setiap kelas di direktori validasi:\")\n",
        "total_valid_files = 0\n",
        "for class_name in valid_classes:\n",
        "    class_path = os.path.join(valid_directory, class_name)\n",
        "    num_files = len(os.listdir(class_path))\n",
        "    total_valid_files += num_files\n",
        "    print(f\"Kelas: {class_name}, Jumlah file: {num_files}\")\n",
        "\n",
        "# Menampilkan total jumlah file dalam direktori validasi\n",
        "print(f\"Total jumlah file dalam direktori validasi: {total_valid_files}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb5uPC5KAWw0",
        "outputId": "b199adfa-050f-4d1e-aa92-ce92fc346c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total jumlah file dalam setiap kelas di direktori train:\n",
            "Kelas: Pa Lulun Pao, Jumlah file: 400\n",
            "Kelas: Pa Somba, Jumlah file: 400\n",
            "Kelas: Pa Tangke Lumu, Jumlah file: 400\n",
            "Kelas: Pa Tumuru, Jumlah file: 400\n",
            "Kelas: Tidak Terdeteksi, Jumlah file: 400\n",
            "Total jumlah file dalam direktori train: 2000\n",
            "\n",
            "Total jumlah file dalam setiap kelas di direktori validasi:\n",
            "Kelas: Pa Lulun Pao, Jumlah file: 100\n",
            "Kelas: Pa Somba, Jumlah file: 100\n",
            "Kelas: Pa Tangke Lumu, Jumlah file: 100\n",
            "Kelas: Pa Tumuru, Jumlah file: 100\n",
            "Kelas: Tidak Terdeteksi, Jumlah file: 100\n",
            "Total jumlah file dalam direktori validasi: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EPOCH 100"
      ],
      "metadata": {
        "id": "nB0BS-L8QooS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 224\n",
        "img_cols = 224\n",
        "input_shape = (img_rows,img_cols,3)\n",
        "epochs = 100\n",
        "batch_size = 8\n",
        "num_of_classes = 5\n",
        "num_of_train_samples = 2000\n",
        "num_of_valid_samples = 500\n",
        "\n",
        "lr = 0.00001\n",
        "print('Learning rate: ', lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSf8IQtoAa5F",
        "outputId": "c0cc09a8-e272-4eb5-b0c4-8237eda33ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=train_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)\n",
        "\n",
        "\n",
        "valid_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=valid_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFs275dXAq6d",
        "outputId": "193635af-e607-4157-cfe4-399304fb00ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 5 classes.\n",
            "Found 500 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def getResNet50Model(lastFourTrainable=False):\n",
        "  resnet_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=True)\n",
        "  for layer in resnet_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  output = resnet_model.get_layer('avg_pool').output\n",
        "  output = Flatten(name='new_flatten')(output)\n",
        "  output = Dense(units=1024, activation='relu', name='new_fc')(output)\n",
        "  output = Dense(units=5, activation='softmax')(output)\n",
        "  resnet_model = Model(resnet_model.input, output)\n",
        "\n",
        "  if lastFourTrainable == True:\n",
        "    resnet_model.get_layer('conv5_block3_2_bn').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_conv').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_bn').trainable = True\n",
        "    resnet_model.get_layer('new_fc').trainable = True\n",
        "\n",
        "\n",
        "  resnet_model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "  resnet_model.summary()\n",
        "\n",
        "\n",
        "  return resnet_model"
      ],
      "metadata": {
        "id": "XC3-erYPAwUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = getResNet50Model(lastFourTrainable=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb3p2j5qAymN",
        "outputId": "29321cd5-9ec0-43d6-a094-0ccad7401591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102967424/102967424 [==============================] - 5s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " new_flatten (Flatten)          (None, 2048)         0           ['avg_pool[0][0]']               \n",
            "                                                                                                  \n",
            " new_fc (Dense)                 (None, 1024)         2098176     ['new_flatten[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            5125        ['new_fc[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,691,013\n",
            "Trainable params: 3,159,045\n",
            "Non-trainable params: 22,531,968\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Menjalankan proses training dan menyimpan hasil history\n",
        "history = resnet_model.fit_generator(train_generator,\n",
        "                                     epochs=epochs,\n",
        "                                     steps_per_epoch=num_of_train_samples//batch_size,\n",
        "                                     validation_data=valid_generator,\n",
        "                                     validation_steps=num_of_valid_samples // batch_size)\n",
        "\n",
        "# Membuat DataFrame dari history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "# Menyimpan DataFrame ke dalam file CSV\n",
        "history_df.to_csv('/content/drive/MyDrive/Colab Notebooks/History/history(0.01).csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDJWwaXgA3tF",
        "outputId": "944db242-315e-4373-9e92-853bce1b92d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.7715 - accuracy: 0.7540 - val_loss: 0.6874 - val_accuracy: 0.8004\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.7167 - accuracy: 0.7675 - val_loss: 0.6386 - val_accuracy: 0.8185\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.6659 - accuracy: 0.7915 - val_loss: 0.6056 - val_accuracy: 0.8185\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.6272 - accuracy: 0.7970 - val_loss: 0.5700 - val_accuracy: 0.8387\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.5854 - accuracy: 0.8195 - val_loss: 0.5669 - val_accuracy: 0.8286\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.5522 - accuracy: 0.8275 - val_loss: 0.5095 - val_accuracy: 0.8609\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.5158 - accuracy: 0.8505 - val_loss: 0.4798 - val_accuracy: 0.8589\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.4816 - accuracy: 0.8585 - val_loss: 0.4779 - val_accuracy: 0.8569\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.4562 - accuracy: 0.8710 - val_loss: 0.4651 - val_accuracy: 0.8548\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.4457 - accuracy: 0.8670 - val_loss: 0.4318 - val_accuracy: 0.8831\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.4163 - accuracy: 0.8770 - val_loss: 0.4141 - val_accuracy: 0.8911\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 16s 66ms/step - loss: 0.3937 - accuracy: 0.8860 - val_loss: 0.4144 - val_accuracy: 0.8750\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.3734 - accuracy: 0.9030 - val_loss: 0.3809 - val_accuracy: 0.9012\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.3625 - accuracy: 0.8975 - val_loss: 0.3846 - val_accuracy: 0.8851\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.3385 - accuracy: 0.9065 - val_loss: 0.3585 - val_accuracy: 0.8992\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.3401 - accuracy: 0.9010 - val_loss: 0.3655 - val_accuracy: 0.8952\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.3182 - accuracy: 0.9065 - val_loss: 0.3471 - val_accuracy: 0.9012\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.3081 - accuracy: 0.9110 - val_loss: 0.3342 - val_accuracy: 0.9052\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.2967 - accuracy: 0.9170 - val_loss: 0.3411 - val_accuracy: 0.8972\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.2917 - accuracy: 0.9170 - val_loss: 0.3228 - val_accuracy: 0.9093\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.2651 - accuracy: 0.9210 - val_loss: 0.3235 - val_accuracy: 0.8952\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.2751 - accuracy: 0.9190 - val_loss: 0.3077 - val_accuracy: 0.9073\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.2439 - accuracy: 0.9370 - val_loss: 0.2948 - val_accuracy: 0.9073\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.2474 - accuracy: 0.9300 - val_loss: 0.2974 - val_accuracy: 0.8952\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.2448 - accuracy: 0.9320 - val_loss: 0.2905 - val_accuracy: 0.9012\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.2352 - accuracy: 0.9280 - val_loss: 0.2819 - val_accuracy: 0.9113\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.2304 - accuracy: 0.9385 - val_loss: 0.2693 - val_accuracy: 0.9214\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.2182 - accuracy: 0.9450 - val_loss: 0.2663 - val_accuracy: 0.9173\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.2105 - accuracy: 0.9455 - val_loss: 0.2548 - val_accuracy: 0.9173\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.2043 - accuracy: 0.9455 - val_loss: 0.2898 - val_accuracy: 0.9093\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1942 - accuracy: 0.9495 - val_loss: 0.2563 - val_accuracy: 0.9214\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1880 - accuracy: 0.9520 - val_loss: 0.2488 - val_accuracy: 0.9254\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1935 - accuracy: 0.9445 - val_loss: 0.2500 - val_accuracy: 0.9214\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.1814 - accuracy: 0.9510 - val_loss: 0.2479 - val_accuracy: 0.9254\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.1667 - accuracy: 0.9565 - val_loss: 0.2434 - val_accuracy: 0.9214\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1706 - accuracy: 0.9510 - val_loss: 0.2528 - val_accuracy: 0.9214\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1640 - accuracy: 0.9560 - val_loss: 0.2507 - val_accuracy: 0.9274\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.1598 - accuracy: 0.9565 - val_loss: 0.2305 - val_accuracy: 0.9234\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1549 - accuracy: 0.9640 - val_loss: 0.2463 - val_accuracy: 0.9254\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.1474 - accuracy: 0.9585 - val_loss: 0.2388 - val_accuracy: 0.9153\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.1511 - accuracy: 0.9630 - val_loss: 0.2584 - val_accuracy: 0.9194\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1453 - accuracy: 0.9620 - val_loss: 0.2676 - val_accuracy: 0.9073\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1349 - accuracy: 0.9645 - val_loss: 0.2269 - val_accuracy: 0.9254\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.1278 - accuracy: 0.9745 - val_loss: 0.2201 - val_accuracy: 0.9254\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1331 - accuracy: 0.9640 - val_loss: 0.2133 - val_accuracy: 0.9355\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1307 - accuracy: 0.9655 - val_loss: 0.2156 - val_accuracy: 0.9234\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1287 - accuracy: 0.9625 - val_loss: 0.2110 - val_accuracy: 0.9315\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1162 - accuracy: 0.9780 - val_loss: 0.2267 - val_accuracy: 0.9234\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1237 - accuracy: 0.9645 - val_loss: 0.2089 - val_accuracy: 0.9315\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1085 - accuracy: 0.9715 - val_loss: 0.2193 - val_accuracy: 0.9375\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1088 - accuracy: 0.9775 - val_loss: 0.2050 - val_accuracy: 0.9355\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.1085 - accuracy: 0.9750 - val_loss: 0.2009 - val_accuracy: 0.9294\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1007 - accuracy: 0.9755 - val_loss: 0.2033 - val_accuracy: 0.9335\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.1022 - accuracy: 0.9765 - val_loss: 0.2143 - val_accuracy: 0.9274\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.1042 - accuracy: 0.9740 - val_loss: 0.2054 - val_accuracy: 0.9335\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.1033 - accuracy: 0.9755 - val_loss: 0.1895 - val_accuracy: 0.9315\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0917 - accuracy: 0.9820 - val_loss: 0.2022 - val_accuracy: 0.9274\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0894 - accuracy: 0.9805 - val_loss: 0.2050 - val_accuracy: 0.9214\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0980 - accuracy: 0.9790 - val_loss: 0.2114 - val_accuracy: 0.9274\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0790 - accuracy: 0.9845 - val_loss: 0.1903 - val_accuracy: 0.9415\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0833 - accuracy: 0.9815 - val_loss: 0.1849 - val_accuracy: 0.9375\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0819 - accuracy: 0.9830 - val_loss: 0.1795 - val_accuracy: 0.9375\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0905 - accuracy: 0.9790 - val_loss: 0.1956 - val_accuracy: 0.9294\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0832 - accuracy: 0.9805 - val_loss: 0.1890 - val_accuracy: 0.9335\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0799 - accuracy: 0.9825 - val_loss: 0.2125 - val_accuracy: 0.9254\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0820 - accuracy: 0.9800 - val_loss: 0.1908 - val_accuracy: 0.9355\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0746 - accuracy: 0.9850 - val_loss: 0.1821 - val_accuracy: 0.9355\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0678 - accuracy: 0.9860 - val_loss: 0.1747 - val_accuracy: 0.9375\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0687 - accuracy: 0.9815 - val_loss: 0.1900 - val_accuracy: 0.9375\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0693 - accuracy: 0.9840 - val_loss: 0.1920 - val_accuracy: 0.9335\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0657 - accuracy: 0.9870 - val_loss: 0.1850 - val_accuracy: 0.9456\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0699 - accuracy: 0.9870 - val_loss: 0.1863 - val_accuracy: 0.9375\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0595 - accuracy: 0.9905 - val_loss: 0.1771 - val_accuracy: 0.9476\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0630 - accuracy: 0.9860 - val_loss: 0.1754 - val_accuracy: 0.9476\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0588 - accuracy: 0.9865 - val_loss: 0.1871 - val_accuracy: 0.9375\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0550 - accuracy: 0.9910 - val_loss: 0.2086 - val_accuracy: 0.9274\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0570 - accuracy: 0.9880 - val_loss: 0.1888 - val_accuracy: 0.9375\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0566 - accuracy: 0.9900 - val_loss: 0.1764 - val_accuracy: 0.9456\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0484 - accuracy: 0.9905 - val_loss: 0.1943 - val_accuracy: 0.9476\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0481 - accuracy: 0.9925 - val_loss: 0.1828 - val_accuracy: 0.9395\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0577 - accuracy: 0.9870 - val_loss: 0.1742 - val_accuracy: 0.9355\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0488 - accuracy: 0.9910 - val_loss: 0.1674 - val_accuracy: 0.9456\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0535 - accuracy: 0.9850 - val_loss: 0.1835 - val_accuracy: 0.9294\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0448 - accuracy: 0.9915 - val_loss: 0.2005 - val_accuracy: 0.9375\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0431 - accuracy: 0.9940 - val_loss: 0.1723 - val_accuracy: 0.9415\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0463 - accuracy: 0.9905 - val_loss: 0.1771 - val_accuracy: 0.9415\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0507 - accuracy: 0.9875 - val_loss: 0.1933 - val_accuracy: 0.9395\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0436 - accuracy: 0.9905 - val_loss: 0.1722 - val_accuracy: 0.9415\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0371 - accuracy: 0.9945 - val_loss: 0.1864 - val_accuracy: 0.9355\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0387 - accuracy: 0.9935 - val_loss: 0.1785 - val_accuracy: 0.9395\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0439 - accuracy: 0.9900 - val_loss: 0.1840 - val_accuracy: 0.9355\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0380 - accuracy: 0.9915 - val_loss: 0.1727 - val_accuracy: 0.9395\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0411 - accuracy: 0.9935 - val_loss: 0.1928 - val_accuracy: 0.9395\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0383 - accuracy: 0.9920 - val_loss: 0.1710 - val_accuracy: 0.9435\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0359 - accuracy: 0.9945 - val_loss: 0.1657 - val_accuracy: 0.9435\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0338 - accuracy: 0.9940 - val_loss: 0.1905 - val_accuracy: 0.9355\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 0.1789 - val_accuracy: 0.9395\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0357 - accuracy: 0.9935 - val_loss: 0.1875 - val_accuracy: 0.9395\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0336 - accuracy: 0.9925 - val_loss: 0.1934 - val_accuracy: 0.9395\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0349 - accuracy: 0.9930 - val_loss: 0.2009 - val_accuracy: 0.9254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Akurasi')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Akurasi')\n",
        "plt.title('Akurasi Training dan Validasi')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Akurasi')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "rVl140p9I3FX",
        "outputId": "89fe77d1-c45a-447b-fb69-b613a7f2d62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_json=resnet_model.to_json()\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch100.json\", \"w\") as json_file:\n",
        "    json_file.write(classifier_json)\n",
        "# serialize weights to HDF5\n",
        "    resnet_model.save_weights(\"/content/drive/MyDrive/Colab Notebooks/Models/my_model_Batch100.h5\")\n",
        "    resnet_model.save(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch100.h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdZaOTXMPyu5",
        "outputId": "810e93cd-9f8b-4c8d-b577-eea7e79b8a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = (train_generator.class_indices)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndDCMzybQBpX",
        "outputId": "f0195b58-8adf-4647-fd04-c5e86037e41b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Pa Lulun Pao': 0, 'Pa Somba': 1, 'Pa Tangke Lumu': 2, 'Pa Tumuru': 3, 'Tidak Terdeteksi': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/Models/model_Batch100.h5', compile=False)"
      ],
      "metadata": {
        "id": "fV1S-pKhQDpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memprediksi kelas menggunakan model pada data validasi\n",
        "valid_predictions = model.predict_generator(valid_generator)\n",
        "\n",
        "# Mengambil indeks kelas dengan nilai probabilitas terbesar\n",
        "valid_predicted_classes = np.argmax(valid_predictions, axis=1)\n",
        "\n",
        "# Mengambil daftar nama kelas\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Menghitung ground truth kelas pada data validasi\n",
        "valid_true_classes = valid_generator.classes\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat classification report\n",
        "classification_rep = classification_report(valid_true_classes, valid_predicted_classes, target_names=class_names)\n",
        "\n",
        "# Menghitung akurasi\n",
        "accuracy = accuracy_score(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcE4_4ScQSXI",
        "outputId": "72da20c2-f311-4a7e-82e0-e848d7a49713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[21 24 20 17 18]\n",
            " [15 28 17 25 15]\n",
            " [18 23 20 20 19]\n",
            " [18 17 11 25 29]\n",
            " [23 22 24 14 17]]\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    Pa Lulun Pao       0.22      0.21      0.22       100\n",
            "        Pa Somba       0.25      0.28      0.26       100\n",
            "  Pa Tangke Lumu       0.22      0.20      0.21       100\n",
            "       Pa Tumuru       0.25      0.25      0.25       100\n",
            "Tidak Terdeteksi       0.17      0.17      0.17       100\n",
            "\n",
            "        accuracy                           0.22       500\n",
            "       macro avg       0.22      0.22      0.22       500\n",
            "    weighted avg       0.22      0.22      0.22       500\n",
            "\n",
            "\n",
            "Accuracy: 0.222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat label untuk sumbu x dan y\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Membuat plot menggunakan heatmap dari seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"True Class\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XhV8S_QWQY1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sn7grgOYQk0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TukQ9ttQwHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EPOCH 200"
      ],
      "metadata": {
        "id": "yphNrPa4QzKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 224\n",
        "img_cols = 224\n",
        "input_shape = (img_rows,img_cols,3)\n",
        "epochs = 200\n",
        "batch_size = 8\n",
        "num_of_classes = 5\n",
        "num_of_train_samples = 2000\n",
        "num_of_valid_samples = 500\n",
        "\n",
        "lr = 0.00001\n",
        "print('Learning rate: ', lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f21932b-0ccb-4857-8750-76d9104123a2",
        "id": "UzQ13ClyQzKW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=train_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)\n",
        "\n",
        "\n",
        "valid_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=valid_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f82a2a-4ce8-4b1f-bf1a-ecb88a677caf",
        "id": "7GuR1GYoQzKW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 5 classes.\n",
            "Found 500 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def getResNet50Model(lastFourTrainable=False):\n",
        "  resnet_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=True)\n",
        "  for layer in resnet_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  output = resnet_model.get_layer('avg_pool').output\n",
        "  output = Flatten(name='new_flatten')(output)\n",
        "  output = Dense(units=1024, activation='relu', name='new_fc')(output)\n",
        "  output = Dense(units=5, activation='softmax')(output)\n",
        "  resnet_model = Model(resnet_model.input, output)\n",
        "\n",
        "  if lastFourTrainable == True:\n",
        "    resnet_model.get_layer('conv5_block3_2_bn').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_conv').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_bn').trainable = True\n",
        "    resnet_model.get_layer('new_fc').trainable = True\n",
        "\n",
        "\n",
        "  resnet_model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "  resnet_model.summary()\n",
        "\n",
        "\n",
        "  return resnet_model"
      ],
      "metadata": {
        "id": "rYAtoiz1QzKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = getResNet50Model(lastFourTrainable=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920a53c8-5dff-468c-803c-bb7473ff8958",
        "id": "tFMMHlqfQzKX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " new_flatten (Flatten)          (None, 2048)         0           ['avg_pool[0][0]']               \n",
            "                                                                                                  \n",
            " new_fc (Dense)                 (None, 1024)         2098176     ['new_flatten[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 5)            5125        ['new_fc[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,691,013\n",
            "Trainable params: 3,159,045\n",
            "Non-trainable params: 22,531,968\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Menjalankan proses training dan menyimpan hasil history\n",
        "history = resnet_model.fit_generator(train_generator,\n",
        "                                     epochs=epochs,\n",
        "                                     steps_per_epoch=num_of_train_samples//batch_size,\n",
        "                                     validation_data=valid_generator,\n",
        "                                     validation_steps=num_of_valid_samples // batch_size)\n",
        "\n",
        "# Membuat DataFrame dari history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "# Menyimpan DataFrame ke dalam file CSV\n",
        "history_df.to_csv('/content/drive/MyDrive/Colab Notebooks/History/history(0.01).csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9154ecb-ee5b-4680-c438-8586e9b40e8d",
        "id": "ltraW9jsQzKX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "250/250 [==============================] - 19s 62ms/step - loss: 1.3333 - accuracy: 0.4655 - val_loss: 1.3188 - val_accuracy: 0.5222\n",
            "Epoch 2/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 1.0650 - accuracy: 0.6060 - val_loss: 0.9584 - val_accuracy: 0.7036\n",
            "Epoch 3/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.9462 - accuracy: 0.6700 - val_loss: 0.8206 - val_accuracy: 0.7762\n",
            "Epoch 4/200\n",
            "250/250 [==============================] - 16s 66ms/step - loss: 0.8451 - accuracy: 0.7155 - val_loss: 0.7492 - val_accuracy: 0.7984\n",
            "Epoch 5/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.7649 - accuracy: 0.7615 - val_loss: 0.6783 - val_accuracy: 0.8065\n",
            "Epoch 6/200\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.7173 - accuracy: 0.7665 - val_loss: 0.6337 - val_accuracy: 0.8327\n",
            "Epoch 7/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.6645 - accuracy: 0.7900 - val_loss: 0.5990 - val_accuracy: 0.8266\n",
            "Epoch 8/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.6189 - accuracy: 0.8090 - val_loss: 0.5529 - val_accuracy: 0.8508\n",
            "Epoch 9/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.5756 - accuracy: 0.8250 - val_loss: 0.5353 - val_accuracy: 0.8569\n",
            "Epoch 10/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.5421 - accuracy: 0.8370 - val_loss: 0.5039 - val_accuracy: 0.8750\n",
            "Epoch 11/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.5133 - accuracy: 0.8445 - val_loss: 0.4785 - val_accuracy: 0.8750\n",
            "Epoch 12/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.4910 - accuracy: 0.8575 - val_loss: 0.4610 - val_accuracy: 0.8730\n",
            "Epoch 13/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.4822 - accuracy: 0.8525 - val_loss: 0.4454 - val_accuracy: 0.8649\n",
            "Epoch 14/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.4320 - accuracy: 0.8750 - val_loss: 0.4262 - val_accuracy: 0.8952\n",
            "Epoch 15/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.4170 - accuracy: 0.8840 - val_loss: 0.4092 - val_accuracy: 0.8750\n",
            "Epoch 16/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.3964 - accuracy: 0.8865 - val_loss: 0.3985 - val_accuracy: 0.8891\n",
            "Epoch 17/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.3879 - accuracy: 0.8850 - val_loss: 0.3833 - val_accuracy: 0.9052\n",
            "Epoch 18/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.3760 - accuracy: 0.8890 - val_loss: 0.3721 - val_accuracy: 0.9073\n",
            "Epoch 19/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.3502 - accuracy: 0.8960 - val_loss: 0.3623 - val_accuracy: 0.8911\n",
            "Epoch 20/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.3413 - accuracy: 0.8980 - val_loss: 0.3620 - val_accuracy: 0.9113\n",
            "Epoch 21/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.3354 - accuracy: 0.9005 - val_loss: 0.3479 - val_accuracy: 0.8952\n",
            "Epoch 22/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.3117 - accuracy: 0.9090 - val_loss: 0.3275 - val_accuracy: 0.9073\n",
            "Epoch 23/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.2979 - accuracy: 0.9145 - val_loss: 0.3625 - val_accuracy: 0.8931\n",
            "Epoch 24/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.2959 - accuracy: 0.9050 - val_loss: 0.3248 - val_accuracy: 0.9032\n",
            "Epoch 25/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.2795 - accuracy: 0.9215 - val_loss: 0.3131 - val_accuracy: 0.9052\n",
            "Epoch 26/200\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.2772 - accuracy: 0.9205 - val_loss: 0.2985 - val_accuracy: 0.9153\n",
            "Epoch 27/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.2500 - accuracy: 0.9325 - val_loss: 0.2936 - val_accuracy: 0.9113\n",
            "Epoch 28/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.2487 - accuracy: 0.9325 - val_loss: 0.2891 - val_accuracy: 0.9194\n",
            "Epoch 29/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.2446 - accuracy: 0.9325 - val_loss: 0.3269 - val_accuracy: 0.8851\n",
            "Epoch 30/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.2428 - accuracy: 0.9340 - val_loss: 0.2848 - val_accuracy: 0.9093\n",
            "Epoch 31/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.2146 - accuracy: 0.9425 - val_loss: 0.2894 - val_accuracy: 0.9113\n",
            "Epoch 32/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.2260 - accuracy: 0.9310 - val_loss: 0.2707 - val_accuracy: 0.9093\n",
            "Epoch 33/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.2137 - accuracy: 0.9410 - val_loss: 0.2766 - val_accuracy: 0.9032\n",
            "Epoch 34/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.2081 - accuracy: 0.9440 - val_loss: 0.2550 - val_accuracy: 0.9234\n",
            "Epoch 35/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1958 - accuracy: 0.9510 - val_loss: 0.2858 - val_accuracy: 0.9173\n",
            "Epoch 36/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.1944 - accuracy: 0.9450 - val_loss: 0.2613 - val_accuracy: 0.9113\n",
            "Epoch 37/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1847 - accuracy: 0.9490 - val_loss: 0.2578 - val_accuracy: 0.9254\n",
            "Epoch 38/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1920 - accuracy: 0.9520 - val_loss: 0.2438 - val_accuracy: 0.9294\n",
            "Epoch 39/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1853 - accuracy: 0.9510 - val_loss: 0.2475 - val_accuracy: 0.9294\n",
            "Epoch 40/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1784 - accuracy: 0.9505 - val_loss: 0.2467 - val_accuracy: 0.9254\n",
            "Epoch 41/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1652 - accuracy: 0.9570 - val_loss: 0.2319 - val_accuracy: 0.9355\n",
            "Epoch 42/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1561 - accuracy: 0.9600 - val_loss: 0.2379 - val_accuracy: 0.9194\n",
            "Epoch 43/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1571 - accuracy: 0.9535 - val_loss: 0.2206 - val_accuracy: 0.9234\n",
            "Epoch 44/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1451 - accuracy: 0.9655 - val_loss: 0.2229 - val_accuracy: 0.9315\n",
            "Epoch 45/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1473 - accuracy: 0.9640 - val_loss: 0.2175 - val_accuracy: 0.9254\n",
            "Epoch 46/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1389 - accuracy: 0.9680 - val_loss: 0.2216 - val_accuracy: 0.9335\n",
            "Epoch 47/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1467 - accuracy: 0.9655 - val_loss: 0.2121 - val_accuracy: 0.9274\n",
            "Epoch 48/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.1356 - accuracy: 0.9650 - val_loss: 0.2143 - val_accuracy: 0.9355\n",
            "Epoch 49/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1348 - accuracy: 0.9635 - val_loss: 0.2135 - val_accuracy: 0.9335\n",
            "Epoch 50/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1326 - accuracy: 0.9655 - val_loss: 0.2379 - val_accuracy: 0.9315\n",
            "Epoch 51/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.1213 - accuracy: 0.9760 - val_loss: 0.2242 - val_accuracy: 0.9355\n",
            "Epoch 52/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1104 - accuracy: 0.9760 - val_loss: 0.2000 - val_accuracy: 0.9395\n",
            "Epoch 53/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1194 - accuracy: 0.9690 - val_loss: 0.2125 - val_accuracy: 0.9254\n",
            "Epoch 54/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1130 - accuracy: 0.9725 - val_loss: 0.1978 - val_accuracy: 0.9395\n",
            "Epoch 55/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.1154 - accuracy: 0.9710 - val_loss: 0.2060 - val_accuracy: 0.9335\n",
            "Epoch 56/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1047 - accuracy: 0.9760 - val_loss: 0.1950 - val_accuracy: 0.9375\n",
            "Epoch 57/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1058 - accuracy: 0.9785 - val_loss: 0.2086 - val_accuracy: 0.9335\n",
            "Epoch 58/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1016 - accuracy: 0.9785 - val_loss: 0.1962 - val_accuracy: 0.9375\n",
            "Epoch 59/200\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.1016 - accuracy: 0.9735 - val_loss: 0.1923 - val_accuracy: 0.9395\n",
            "Epoch 60/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0960 - accuracy: 0.9795 - val_loss: 0.1914 - val_accuracy: 0.9435\n",
            "Epoch 61/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0993 - accuracy: 0.9750 - val_loss: 0.1893 - val_accuracy: 0.9395\n",
            "Epoch 62/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1016 - accuracy: 0.9730 - val_loss: 0.1932 - val_accuracy: 0.9456\n",
            "Epoch 63/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0863 - accuracy: 0.9810 - val_loss: 0.1980 - val_accuracy: 0.9315\n",
            "Epoch 64/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0839 - accuracy: 0.9820 - val_loss: 0.1923 - val_accuracy: 0.9395\n",
            "Epoch 65/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0812 - accuracy: 0.9850 - val_loss: 0.1842 - val_accuracy: 0.9395\n",
            "Epoch 66/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0825 - accuracy: 0.9835 - val_loss: 0.2231 - val_accuracy: 0.9153\n",
            "Epoch 67/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0848 - accuracy: 0.9810 - val_loss: 0.1896 - val_accuracy: 0.9456\n",
            "Epoch 68/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0837 - accuracy: 0.9800 - val_loss: 0.1839 - val_accuracy: 0.9476\n",
            "Epoch 69/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0794 - accuracy: 0.9785 - val_loss: 0.1699 - val_accuracy: 0.9456\n",
            "Epoch 70/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0695 - accuracy: 0.9860 - val_loss: 0.1795 - val_accuracy: 0.9435\n",
            "Epoch 71/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0773 - accuracy: 0.9840 - val_loss: 0.1788 - val_accuracy: 0.9375\n",
            "Epoch 72/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0698 - accuracy: 0.9820 - val_loss: 0.1865 - val_accuracy: 0.9395\n",
            "Epoch 73/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0736 - accuracy: 0.9815 - val_loss: 0.1930 - val_accuracy: 0.9335\n",
            "Epoch 74/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0676 - accuracy: 0.9855 - val_loss: 0.1680 - val_accuracy: 0.9435\n",
            "Epoch 75/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0639 - accuracy: 0.9860 - val_loss: 0.1813 - val_accuracy: 0.9395\n",
            "Epoch 76/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0661 - accuracy: 0.9860 - val_loss: 0.1621 - val_accuracy: 0.9496\n",
            "Epoch 77/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0659 - accuracy: 0.9860 - val_loss: 0.1656 - val_accuracy: 0.9415\n",
            "Epoch 78/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0634 - accuracy: 0.9850 - val_loss: 0.1824 - val_accuracy: 0.9395\n",
            "Epoch 79/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0597 - accuracy: 0.9890 - val_loss: 0.1945 - val_accuracy: 0.9395\n",
            "Epoch 80/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0587 - accuracy: 0.9880 - val_loss: 0.1614 - val_accuracy: 0.9456\n",
            "Epoch 81/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0607 - accuracy: 0.9870 - val_loss: 0.1695 - val_accuracy: 0.9476\n",
            "Epoch 82/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0494 - accuracy: 0.9890 - val_loss: 0.1703 - val_accuracy: 0.9476\n",
            "Epoch 83/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0551 - accuracy: 0.9900 - val_loss: 0.1763 - val_accuracy: 0.9456\n",
            "Epoch 84/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0506 - accuracy: 0.9895 - val_loss: 0.1670 - val_accuracy: 0.9456\n",
            "Epoch 85/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0563 - accuracy: 0.9875 - val_loss: 0.1660 - val_accuracy: 0.9395\n",
            "Epoch 86/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0489 - accuracy: 0.9895 - val_loss: 0.1820 - val_accuracy: 0.9355\n",
            "Epoch 87/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0498 - accuracy: 0.9900 - val_loss: 0.1742 - val_accuracy: 0.9435\n",
            "Epoch 88/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0439 - accuracy: 0.9935 - val_loss: 0.1627 - val_accuracy: 0.9456\n",
            "Epoch 89/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0438 - accuracy: 0.9900 - val_loss: 0.1672 - val_accuracy: 0.9415\n",
            "Epoch 90/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0421 - accuracy: 0.9890 - val_loss: 0.2111 - val_accuracy: 0.9415\n",
            "Epoch 91/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0434 - accuracy: 0.9920 - val_loss: 0.1755 - val_accuracy: 0.9415\n",
            "Epoch 92/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0448 - accuracy: 0.9890 - val_loss: 0.1842 - val_accuracy: 0.9395\n",
            "Epoch 93/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0462 - accuracy: 0.9915 - val_loss: 0.1660 - val_accuracy: 0.9496\n",
            "Epoch 94/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0471 - accuracy: 0.9895 - val_loss: 0.1662 - val_accuracy: 0.9415\n",
            "Epoch 95/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0382 - accuracy: 0.9915 - val_loss: 0.1564 - val_accuracy: 0.9496\n",
            "Epoch 96/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0389 - accuracy: 0.9940 - val_loss: 0.1725 - val_accuracy: 0.9456\n",
            "Epoch 97/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0331 - accuracy: 0.9950 - val_loss: 0.1591 - val_accuracy: 0.9516\n",
            "Epoch 98/200\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0411 - accuracy: 0.9915 - val_loss: 0.1568 - val_accuracy: 0.9496\n",
            "Epoch 99/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0382 - accuracy: 0.9930 - val_loss: 0.2079 - val_accuracy: 0.9294\n",
            "Epoch 100/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0367 - accuracy: 0.9925 - val_loss: 0.1575 - val_accuracy: 0.9476\n",
            "Epoch 101/200\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0299 - accuracy: 0.9955 - val_loss: 0.1559 - val_accuracy: 0.9496\n",
            "Epoch 102/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0375 - accuracy: 0.9920 - val_loss: 0.1987 - val_accuracy: 0.9375\n",
            "Epoch 103/200\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0373 - accuracy: 0.9930 - val_loss: 0.1925 - val_accuracy: 0.9435\n",
            "Epoch 104/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0353 - accuracy: 0.9905 - val_loss: 0.1630 - val_accuracy: 0.9435\n",
            "Epoch 105/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0271 - accuracy: 0.9980 - val_loss: 0.1676 - val_accuracy: 0.9435\n",
            "Epoch 106/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0284 - accuracy: 0.9955 - val_loss: 0.1614 - val_accuracy: 0.9395\n",
            "Epoch 107/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0291 - accuracy: 0.9955 - val_loss: 0.1644 - val_accuracy: 0.9456\n",
            "Epoch 108/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0358 - accuracy: 0.9930 - val_loss: 0.1791 - val_accuracy: 0.9456\n",
            "Epoch 109/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0254 - accuracy: 0.9980 - val_loss: 0.1669 - val_accuracy: 0.9476\n",
            "Epoch 110/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0330 - accuracy: 0.9930 - val_loss: 0.1633 - val_accuracy: 0.9435\n",
            "Epoch 111/200\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0307 - accuracy: 0.9930 - val_loss: 0.1794 - val_accuracy: 0.9456\n",
            "Epoch 112/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0322 - accuracy: 0.9920 - val_loss: 0.1739 - val_accuracy: 0.9476\n",
            "Epoch 113/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0246 - accuracy: 0.9955 - val_loss: 0.1632 - val_accuracy: 0.9456\n",
            "Epoch 114/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0260 - accuracy: 0.9950 - val_loss: 0.1595 - val_accuracy: 0.9496\n",
            "Epoch 115/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0252 - accuracy: 0.9960 - val_loss: 0.1600 - val_accuracy: 0.9516\n",
            "Epoch 116/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0206 - accuracy: 0.9970 - val_loss: 0.1766 - val_accuracy: 0.9456\n",
            "Epoch 117/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0226 - accuracy: 0.9965 - val_loss: 0.1714 - val_accuracy: 0.9476\n",
            "Epoch 118/200\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0236 - accuracy: 0.9965 - val_loss: 0.1708 - val_accuracy: 0.9476\n",
            "Epoch 119/200\n",
            "250/250 [==============================] - 17s 66ms/step - loss: 0.0218 - accuracy: 0.9980 - val_loss: 0.1629 - val_accuracy: 0.9516\n",
            "Epoch 120/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0200 - accuracy: 0.9985 - val_loss: 0.1678 - val_accuracy: 0.9496\n",
            "Epoch 121/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0284 - accuracy: 0.9940 - val_loss: 0.1886 - val_accuracy: 0.9456\n",
            "Epoch 122/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0241 - accuracy: 0.9950 - val_loss: 0.1616 - val_accuracy: 0.9496\n",
            "Epoch 123/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0241 - accuracy: 0.9950 - val_loss: 0.1633 - val_accuracy: 0.9476\n",
            "Epoch 124/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0232 - accuracy: 0.9960 - val_loss: 0.1700 - val_accuracy: 0.9456\n",
            "Epoch 125/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0181 - accuracy: 0.9965 - val_loss: 0.1779 - val_accuracy: 0.9415\n",
            "Epoch 126/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0198 - accuracy: 0.9970 - val_loss: 0.1609 - val_accuracy: 0.9415\n",
            "Epoch 127/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0194 - accuracy: 0.9970 - val_loss: 0.1676 - val_accuracy: 0.9435\n",
            "Epoch 128/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0230 - accuracy: 0.9940 - val_loss: 0.1633 - val_accuracy: 0.9496\n",
            "Epoch 129/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0166 - accuracy: 0.9980 - val_loss: 0.1712 - val_accuracy: 0.9435\n",
            "Epoch 130/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0206 - accuracy: 0.9955 - val_loss: 0.1626 - val_accuracy: 0.9516\n",
            "Epoch 131/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0183 - accuracy: 0.9970 - val_loss: 0.1691 - val_accuracy: 0.9375\n",
            "Epoch 132/200\n",
            "250/250 [==============================] - 17s 68ms/step - loss: 0.0184 - accuracy: 0.9975 - val_loss: 0.1539 - val_accuracy: 0.9496\n",
            "Epoch 133/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0172 - accuracy: 0.9970 - val_loss: 0.1733 - val_accuracy: 0.9476\n",
            "Epoch 134/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0160 - accuracy: 0.9970 - val_loss: 0.1755 - val_accuracy: 0.9415\n",
            "Epoch 135/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 0.1750 - val_accuracy: 0.9476\n",
            "Epoch 136/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0166 - accuracy: 0.9980 - val_loss: 0.1812 - val_accuracy: 0.9395\n",
            "Epoch 137/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0176 - accuracy: 0.9980 - val_loss: 0.1988 - val_accuracy: 0.9415\n",
            "Epoch 138/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0200 - accuracy: 0.9970 - val_loss: 0.1746 - val_accuracy: 0.9516\n",
            "Epoch 139/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0131 - accuracy: 0.9985 - val_loss: 0.1709 - val_accuracy: 0.9476\n",
            "Epoch 140/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0134 - accuracy: 0.9995 - val_loss: 0.1727 - val_accuracy: 0.9456\n",
            "Epoch 141/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0208 - accuracy: 0.9945 - val_loss: 0.2003 - val_accuracy: 0.9375\n",
            "Epoch 142/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0124 - accuracy: 0.9995 - val_loss: 0.1759 - val_accuracy: 0.9476\n",
            "Epoch 143/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0131 - accuracy: 0.9975 - val_loss: 0.2568 - val_accuracy: 0.9315\n",
            "Epoch 144/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0210 - accuracy: 0.9960 - val_loss: 0.1801 - val_accuracy: 0.9516\n",
            "Epoch 145/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0145 - accuracy: 0.9995 - val_loss: 0.1772 - val_accuracy: 0.9516\n",
            "Epoch 146/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0146 - accuracy: 0.9970 - val_loss: 0.1960 - val_accuracy: 0.9476\n",
            "Epoch 147/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0131 - accuracy: 0.9995 - val_loss: 0.1688 - val_accuracy: 0.9476\n",
            "Epoch 148/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0155 - accuracy: 0.9990 - val_loss: 0.1682 - val_accuracy: 0.9476\n",
            "Epoch 149/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0143 - accuracy: 0.9995 - val_loss: 0.1756 - val_accuracy: 0.9496\n",
            "Epoch 150/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0148 - accuracy: 0.9975 - val_loss: 0.1615 - val_accuracy: 0.9556\n",
            "Epoch 151/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0166 - accuracy: 0.9960 - val_loss: 0.2395 - val_accuracy: 0.9375\n",
            "Epoch 152/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.1762 - val_accuracy: 0.9496\n",
            "Epoch 153/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0123 - accuracy: 0.9975 - val_loss: 0.1795 - val_accuracy: 0.9456\n",
            "Epoch 154/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0158 - accuracy: 0.9960 - val_loss: 0.2034 - val_accuracy: 0.9355\n",
            "Epoch 155/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0140 - accuracy: 0.9980 - val_loss: 0.2429 - val_accuracy: 0.9375\n",
            "Epoch 156/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.1819 - val_accuracy: 0.9516\n",
            "Epoch 157/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1733 - val_accuracy: 0.9476\n",
            "Epoch 158/200\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0142 - accuracy: 0.9970 - val_loss: 0.1661 - val_accuracy: 0.9476\n",
            "Epoch 159/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 0.1694 - val_accuracy: 0.9456\n",
            "Epoch 160/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0128 - accuracy: 0.9975 - val_loss: 0.1774 - val_accuracy: 0.9516\n",
            "Epoch 161/200\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 0.1870 - val_accuracy: 0.9456\n",
            "Epoch 162/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0105 - accuracy: 0.9980 - val_loss: 0.1723 - val_accuracy: 0.9456\n",
            "Epoch 163/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0080 - accuracy: 0.9995 - val_loss: 0.1759 - val_accuracy: 0.9476\n",
            "Epoch 164/200\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0102 - accuracy: 0.9995 - val_loss: 0.1878 - val_accuracy: 0.9496\n",
            "Epoch 165/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0193 - accuracy: 0.9950 - val_loss: 0.1873 - val_accuracy: 0.9395\n",
            "Epoch 166/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 0.1808 - val_accuracy: 0.9496\n",
            "Epoch 167/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0095 - accuracy: 0.9990 - val_loss: 0.2059 - val_accuracy: 0.9476\n",
            "Epoch 168/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.1905 - val_accuracy: 0.9496\n",
            "Epoch 169/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0091 - accuracy: 0.9985 - val_loss: 0.2063 - val_accuracy: 0.9415\n",
            "Epoch 170/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0100 - accuracy: 0.9995 - val_loss: 0.1775 - val_accuracy: 0.9516\n",
            "Epoch 171/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0092 - accuracy: 0.9990 - val_loss: 0.1933 - val_accuracy: 0.9496\n",
            "Epoch 172/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0137 - accuracy: 0.9975 - val_loss: 0.1854 - val_accuracy: 0.9496\n",
            "Epoch 173/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.1751 - val_accuracy: 0.9435\n",
            "Epoch 174/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0087 - accuracy: 0.9995 - val_loss: 0.1728 - val_accuracy: 0.9516\n",
            "Epoch 175/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0086 - accuracy: 0.9995 - val_loss: 0.1786 - val_accuracy: 0.9536\n",
            "Epoch 176/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 0.1729 - val_accuracy: 0.9516\n",
            "Epoch 177/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 0.1660 - val_accuracy: 0.9536\n",
            "Epoch 178/200\n",
            "250/250 [==============================] - 17s 66ms/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 0.1799 - val_accuracy: 0.9435\n",
            "Epoch 179/200\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0079 - accuracy: 0.9995 - val_loss: 0.1783 - val_accuracy: 0.9577\n",
            "Epoch 180/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0069 - accuracy: 0.9995 - val_loss: 0.1634 - val_accuracy: 0.9496\n",
            "Epoch 181/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0073 - accuracy: 0.9995 - val_loss: 0.1937 - val_accuracy: 0.9456\n",
            "Epoch 182/200\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.1705 - val_accuracy: 0.9496\n",
            "Epoch 183/200\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.1657 - val_accuracy: 0.9476\n",
            "Epoch 184/200\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0084 - accuracy: 0.9980 - val_loss: 0.1962 - val_accuracy: 0.9435\n",
            "Epoch 185/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.1766 - val_accuracy: 0.9456\n",
            "Epoch 186/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0060 - accuracy: 0.9995 - val_loss: 0.1807 - val_accuracy: 0.9375\n",
            "Epoch 187/200\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.1569 - val_accuracy: 0.9516\n",
            "Epoch 188/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0080 - accuracy: 0.9990 - val_loss: 0.1876 - val_accuracy: 0.9496\n",
            "Epoch 189/200\n",
            "250/250 [==============================] - 17s 69ms/step - loss: 0.0109 - accuracy: 0.9975 - val_loss: 0.2007 - val_accuracy: 0.9415\n",
            "Epoch 190/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2078 - val_accuracy: 0.9456\n",
            "Epoch 191/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.1832 - val_accuracy: 0.9476\n",
            "Epoch 192/200\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.1769 - val_accuracy: 0.9435\n",
            "Epoch 193/200\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.1823 - val_accuracy: 0.9456\n",
            "Epoch 194/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0074 - accuracy: 0.9990 - val_loss: 0.2119 - val_accuracy: 0.9395\n",
            "Epoch 195/200\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.2176 - val_accuracy: 0.9456\n",
            "Epoch 196/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.2052 - val_accuracy: 0.9476\n",
            "Epoch 197/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.1843 - val_accuracy: 0.9496\n",
            "Epoch 198/200\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 0.2118 - val_accuracy: 0.9476\n",
            "Epoch 199/200\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 0.2045 - val_accuracy: 0.9435\n",
            "Epoch 200/200\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.1925 - val_accuracy: 0.9456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Akurasi')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Akurasi')\n",
        "plt.title('Akurasi Training dan Validasi')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Akurasi')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "e3886ea5-0223-4cbe-b7a9-7068f47f6ed9",
        "id": "g0BhpwodQzKX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_json=resnet_model.to_json()\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch200.json\", \"w\") as json_file:\n",
        "    json_file.write(classifier_json)\n",
        "# serialize weights to HDF5\n",
        "    resnet_model.save_weights(\"/content/drive/MyDrive/Colab Notebooks/Models/my_model_Batch200.h5\")\n",
        "    resnet_model.save(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch200.h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010e397c-ca84-4d74-b9b7-6baff722a3f2",
        "id": "eSKUSBNgQzKX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = (train_generator.class_indices)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca85e64-aa61-47ff-9d1f-120a7d49576b",
        "id": "V8ZuIfE2QzKX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Pa Lulun Pao': 0, 'Pa Somba': 1, 'Pa Tangke Lumu': 2, 'Pa Tumuru': 3, 'Tidak Terdeteksi': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/Models/model_Batch200.h5', compile=False)"
      ],
      "metadata": {
        "id": "oRdEjdNvQzKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memprediksi kelas menggunakan model pada data validasi\n",
        "valid_predictions = model.predict_generator(valid_generator)\n",
        "\n",
        "# Mengambil indeks kelas dengan nilai probabilitas terbesar\n",
        "valid_predicted_classes = np.argmax(valid_predictions, axis=1)\n",
        "\n",
        "# Mengambil daftar nama kelas\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Menghitung ground truth kelas pada data validasi\n",
        "valid_true_classes = valid_generator.classes\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat classification report\n",
        "classification_rep = classification_report(valid_true_classes, valid_predicted_classes, target_names=class_names)\n",
        "\n",
        "# Menghitung akurasi\n",
        "accuracy = accuracy_score(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9896a5d9-7db7-4a40-919c-9aac77d5abcf",
        "id": "2Njca-aTQzKX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[21 18 20 23 18]\n",
            " [24 16 20 14 26]\n",
            " [12 20 22 28 18]\n",
            " [21 27 14 17 21]\n",
            " [20 22 19 23 16]]\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    Pa Lulun Pao       0.21      0.21      0.21       100\n",
            "        Pa Somba       0.16      0.16      0.16       100\n",
            "  Pa Tangke Lumu       0.23      0.22      0.23       100\n",
            "       Pa Tumuru       0.16      0.17      0.17       100\n",
            "Tidak Terdeteksi       0.16      0.16      0.16       100\n",
            "\n",
            "        accuracy                           0.18       500\n",
            "       macro avg       0.18      0.18      0.18       500\n",
            "    weighted avg       0.18      0.18      0.18       500\n",
            "\n",
            "\n",
            "Accuracy: 0.184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat label untuk sumbu x dan y\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Membuat plot menggunakan heatmap dari seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"True Class\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WpWF2H7_QzKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgKrDi1PQzKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EPOCH 300"
      ],
      "metadata": {
        "id": "gWEOQlBReKfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 224\n",
        "img_cols = 224\n",
        "input_shape = (img_rows,img_cols,3)\n",
        "epochs = 300\n",
        "batch_size = 8\n",
        "num_of_classes = 5\n",
        "num_of_train_samples = 2000\n",
        "num_of_valid_samples = 500\n",
        "\n",
        "lr = 0.00001\n",
        "\n",
        "print('Learning rate: ', lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90b33be-716a-4583-9081-d10b817f8dfe",
        "id": "U2wjHslseKfM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=train_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)\n",
        "\n",
        "\n",
        "valid_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=valid_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddc86fc-1875-4ff0-ccd4-db69cf75e670",
        "id": "K3dYIg_reKfM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 5 classes.\n",
            "Found 500 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def getResNet50Model(lastFourTrainable=False):\n",
        "  resnet_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=True)\n",
        "  for layer in resnet_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  output = resnet_model.get_layer('avg_pool').output\n",
        "  output = Flatten(name='new_flatten')(output)\n",
        "  output = Dense(units=1024, activation='relu', name='new_fc')(output)\n",
        "  output = Dense(units=5, activation='softmax')(output)\n",
        "  resnet_model = Model(resnet_model.input, output)\n",
        "\n",
        "  if lastFourTrainable == True:\n",
        "    resnet_model.get_layer('conv5_block3_2_bn').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_conv').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_bn').trainable = True\n",
        "    resnet_model.get_layer('new_fc').trainable = True\n",
        "\n",
        "\n",
        "  resnet_model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "  resnet_model.summary()\n",
        "\n",
        "\n",
        "  return resnet_model"
      ],
      "metadata": {
        "id": "jVYuTqxoeKfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = getResNet50Model(lastFourTrainable=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07da5fbb-f27b-4c14-e388-397074c559f3",
        "id": "5HpEY0APeKfN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " new_flatten (Flatten)          (None, 2048)         0           ['avg_pool[0][0]']               \n",
            "                                                                                                  \n",
            " new_fc (Dense)                 (None, 1024)         2098176     ['new_flatten[0][0]']            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 5)            5125        ['new_fc[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,691,013\n",
            "Trainable params: 3,159,045\n",
            "Non-trainable params: 22,531,968\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Menjalankan proses training dan menyimpan hasil history\n",
        "history = resnet_model.fit_generator(train_generator,\n",
        "                                     epochs=epochs,\n",
        "                                     steps_per_epoch=num_of_train_samples//batch_size,\n",
        "                                     validation_data=valid_generator,\n",
        "                                     validation_steps=num_of_valid_samples // batch_size)\n",
        "\n",
        "# Membuat DataFrame dari history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "# Menyimpan DataFrame ke dalam file CSV\n",
        "history_df.to_csv('/content/drive/MyDrive/Colab Notebooks/History/history(0.01).csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b7c174-7e88-463a-865a-a4ad26c71ec4",
        "id": "K-t16C68eKfN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "250/250 [==============================] - 22s 71ms/step - loss: 1.3235 - accuracy: 0.4570 - val_loss: 1.2999 - val_accuracy: 0.4859\n",
            "Epoch 2/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 1.0606 - accuracy: 0.6250 - val_loss: 0.9383 - val_accuracy: 0.7036\n",
            "Epoch 3/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.9285 - accuracy: 0.6785 - val_loss: 0.8165 - val_accuracy: 0.7440\n",
            "Epoch 4/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.8475 - accuracy: 0.7225 - val_loss: 0.7330 - val_accuracy: 0.8044\n",
            "Epoch 5/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.7617 - accuracy: 0.7575 - val_loss: 0.6740 - val_accuracy: 0.8226\n",
            "Epoch 6/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.6988 - accuracy: 0.7780 - val_loss: 0.6304 - val_accuracy: 0.8306\n",
            "Epoch 7/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.6519 - accuracy: 0.8065 - val_loss: 0.6045 - val_accuracy: 0.8327\n",
            "Epoch 8/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.6145 - accuracy: 0.8095 - val_loss: 0.5548 - val_accuracy: 0.8407\n",
            "Epoch 9/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.5779 - accuracy: 0.8175 - val_loss: 0.5518 - val_accuracy: 0.8387\n",
            "Epoch 10/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.5544 - accuracy: 0.8370 - val_loss: 0.4956 - val_accuracy: 0.8609\n",
            "Epoch 11/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.4975 - accuracy: 0.8580 - val_loss: 0.4838 - val_accuracy: 0.8649\n",
            "Epoch 12/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.4856 - accuracy: 0.8660 - val_loss: 0.4622 - val_accuracy: 0.8871\n",
            "Epoch 13/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.4473 - accuracy: 0.8630 - val_loss: 0.4512 - val_accuracy: 0.8569\n",
            "Epoch 14/300\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.4393 - accuracy: 0.8700 - val_loss: 0.4356 - val_accuracy: 0.8710\n",
            "Epoch 15/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.4160 - accuracy: 0.8755 - val_loss: 0.4103 - val_accuracy: 0.8851\n",
            "Epoch 16/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.3962 - accuracy: 0.8900 - val_loss: 0.3994 - val_accuracy: 0.8871\n",
            "Epoch 17/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.3828 - accuracy: 0.8875 - val_loss: 0.3814 - val_accuracy: 0.8770\n",
            "Epoch 18/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.3547 - accuracy: 0.9060 - val_loss: 0.3747 - val_accuracy: 0.8770\n",
            "Epoch 19/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.3472 - accuracy: 0.8970 - val_loss: 0.3618 - val_accuracy: 0.9032\n",
            "Epoch 20/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.3280 - accuracy: 0.9020 - val_loss: 0.3563 - val_accuracy: 0.9012\n",
            "Epoch 21/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.3191 - accuracy: 0.9100 - val_loss: 0.3334 - val_accuracy: 0.9093\n",
            "Epoch 22/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.3264 - accuracy: 0.9035 - val_loss: 0.3474 - val_accuracy: 0.8992\n",
            "Epoch 23/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.3110 - accuracy: 0.9105 - val_loss: 0.3244 - val_accuracy: 0.9052\n",
            "Epoch 24/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.2772 - accuracy: 0.9210 - val_loss: 0.3209 - val_accuracy: 0.9052\n",
            "Epoch 25/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.2722 - accuracy: 0.9310 - val_loss: 0.3208 - val_accuracy: 0.9012\n",
            "Epoch 26/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.2705 - accuracy: 0.9265 - val_loss: 0.3064 - val_accuracy: 0.8952\n",
            "Epoch 27/300\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.2600 - accuracy: 0.9265 - val_loss: 0.2928 - val_accuracy: 0.9214\n",
            "Epoch 28/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.2489 - accuracy: 0.9315 - val_loss: 0.2950 - val_accuracy: 0.9052\n",
            "Epoch 29/300\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.2368 - accuracy: 0.9370 - val_loss: 0.2806 - val_accuracy: 0.9173\n",
            "Epoch 30/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.2310 - accuracy: 0.9425 - val_loss: 0.2796 - val_accuracy: 0.9133\n",
            "Epoch 31/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.2479 - accuracy: 0.9270 - val_loss: 0.2781 - val_accuracy: 0.9194\n",
            "Epoch 32/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.2122 - accuracy: 0.9410 - val_loss: 0.2879 - val_accuracy: 0.9133\n",
            "Epoch 33/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.2200 - accuracy: 0.9405 - val_loss: 0.2728 - val_accuracy: 0.9254\n",
            "Epoch 34/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.2011 - accuracy: 0.9455 - val_loss: 0.2581 - val_accuracy: 0.9274\n",
            "Epoch 35/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.2004 - accuracy: 0.9410 - val_loss: 0.2480 - val_accuracy: 0.9294\n",
            "Epoch 36/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1891 - accuracy: 0.9520 - val_loss: 0.2644 - val_accuracy: 0.9214\n",
            "Epoch 37/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1826 - accuracy: 0.9545 - val_loss: 0.2411 - val_accuracy: 0.9294\n",
            "Epoch 38/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1842 - accuracy: 0.9485 - val_loss: 0.2531 - val_accuracy: 0.9254\n",
            "Epoch 39/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1803 - accuracy: 0.9510 - val_loss: 0.2434 - val_accuracy: 0.9315\n",
            "Epoch 40/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1690 - accuracy: 0.9565 - val_loss: 0.2462 - val_accuracy: 0.9274\n",
            "Epoch 41/300\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.1689 - accuracy: 0.9580 - val_loss: 0.2265 - val_accuracy: 0.9335\n",
            "Epoch 42/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1671 - accuracy: 0.9535 - val_loss: 0.2477 - val_accuracy: 0.9093\n",
            "Epoch 43/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1507 - accuracy: 0.9645 - val_loss: 0.2355 - val_accuracy: 0.9234\n",
            "Epoch 44/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1481 - accuracy: 0.9630 - val_loss: 0.2324 - val_accuracy: 0.9173\n",
            "Epoch 45/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1559 - accuracy: 0.9555 - val_loss: 0.2134 - val_accuracy: 0.9294\n",
            "Epoch 46/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1495 - accuracy: 0.9645 - val_loss: 0.2272 - val_accuracy: 0.9234\n",
            "Epoch 47/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.1506 - accuracy: 0.9595 - val_loss: 0.2175 - val_accuracy: 0.9375\n",
            "Epoch 48/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1273 - accuracy: 0.9695 - val_loss: 0.2214 - val_accuracy: 0.9294\n",
            "Epoch 49/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1296 - accuracy: 0.9685 - val_loss: 0.2207 - val_accuracy: 0.9254\n",
            "Epoch 50/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.1364 - accuracy: 0.9660 - val_loss: 0.2106 - val_accuracy: 0.9355\n",
            "Epoch 51/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.1205 - accuracy: 0.9715 - val_loss: 0.2396 - val_accuracy: 0.9194\n",
            "Epoch 52/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1327 - accuracy: 0.9630 - val_loss: 0.2138 - val_accuracy: 0.9335\n",
            "Epoch 53/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1170 - accuracy: 0.9735 - val_loss: 0.2247 - val_accuracy: 0.9294\n",
            "Epoch 54/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1156 - accuracy: 0.9710 - val_loss: 0.2032 - val_accuracy: 0.9375\n",
            "Epoch 55/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.1150 - accuracy: 0.9725 - val_loss: 0.2035 - val_accuracy: 0.9395\n",
            "Epoch 56/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.1097 - accuracy: 0.9750 - val_loss: 0.2048 - val_accuracy: 0.9415\n",
            "Epoch 57/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1018 - accuracy: 0.9790 - val_loss: 0.1979 - val_accuracy: 0.9355\n",
            "Epoch 58/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.1050 - accuracy: 0.9785 - val_loss: 0.2047 - val_accuracy: 0.9335\n",
            "Epoch 59/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1007 - accuracy: 0.9775 - val_loss: 0.1964 - val_accuracy: 0.9415\n",
            "Epoch 60/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0969 - accuracy: 0.9815 - val_loss: 0.2195 - val_accuracy: 0.9274\n",
            "Epoch 61/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0994 - accuracy: 0.9755 - val_loss: 0.1893 - val_accuracy: 0.9335\n",
            "Epoch 62/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0910 - accuracy: 0.9795 - val_loss: 0.1921 - val_accuracy: 0.9315\n",
            "Epoch 63/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0822 - accuracy: 0.9850 - val_loss: 0.1962 - val_accuracy: 0.9415\n",
            "Epoch 64/300\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0869 - accuracy: 0.9790 - val_loss: 0.1938 - val_accuracy: 0.9395\n",
            "Epoch 65/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0852 - accuracy: 0.9800 - val_loss: 0.1820 - val_accuracy: 0.9435\n",
            "Epoch 66/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0772 - accuracy: 0.9845 - val_loss: 0.1842 - val_accuracy: 0.9375\n",
            "Epoch 67/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0780 - accuracy: 0.9825 - val_loss: 0.1784 - val_accuracy: 0.9435\n",
            "Epoch 68/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0754 - accuracy: 0.9845 - val_loss: 0.1808 - val_accuracy: 0.9395\n",
            "Epoch 69/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0777 - accuracy: 0.9860 - val_loss: 0.1930 - val_accuracy: 0.9335\n",
            "Epoch 70/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0688 - accuracy: 0.9830 - val_loss: 0.1880 - val_accuracy: 0.9355\n",
            "Epoch 71/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0802 - accuracy: 0.9775 - val_loss: 0.1889 - val_accuracy: 0.9375\n",
            "Epoch 72/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0696 - accuracy: 0.9820 - val_loss: 0.1769 - val_accuracy: 0.9435\n",
            "Epoch 73/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0644 - accuracy: 0.9890 - val_loss: 0.1921 - val_accuracy: 0.9375\n",
            "Epoch 74/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0669 - accuracy: 0.9880 - val_loss: 0.1860 - val_accuracy: 0.9435\n",
            "Epoch 75/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0668 - accuracy: 0.9815 - val_loss: 0.1792 - val_accuracy: 0.9415\n",
            "Epoch 76/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0655 - accuracy: 0.9850 - val_loss: 0.1840 - val_accuracy: 0.9415\n",
            "Epoch 77/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0605 - accuracy: 0.9865 - val_loss: 0.1837 - val_accuracy: 0.9415\n",
            "Epoch 78/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0637 - accuracy: 0.9850 - val_loss: 0.1854 - val_accuracy: 0.9415\n",
            "Epoch 79/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0583 - accuracy: 0.9865 - val_loss: 0.1757 - val_accuracy: 0.9435\n",
            "Epoch 80/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0586 - accuracy: 0.9870 - val_loss: 0.1748 - val_accuracy: 0.9395\n",
            "Epoch 81/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0692 - accuracy: 0.9820 - val_loss: 0.2224 - val_accuracy: 0.9254\n",
            "Epoch 82/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0504 - accuracy: 0.9915 - val_loss: 0.1617 - val_accuracy: 0.9415\n",
            "Epoch 83/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0515 - accuracy: 0.9885 - val_loss: 0.1707 - val_accuracy: 0.9415\n",
            "Epoch 84/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0541 - accuracy: 0.9885 - val_loss: 0.1957 - val_accuracy: 0.9375\n",
            "Epoch 85/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0484 - accuracy: 0.9890 - val_loss: 0.1687 - val_accuracy: 0.9375\n",
            "Epoch 86/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0475 - accuracy: 0.9910 - val_loss: 0.1765 - val_accuracy: 0.9395\n",
            "Epoch 87/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0502 - accuracy: 0.9900 - val_loss: 0.1661 - val_accuracy: 0.9435\n",
            "Epoch 88/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0501 - accuracy: 0.9880 - val_loss: 0.1696 - val_accuracy: 0.9476\n",
            "Epoch 89/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0437 - accuracy: 0.9915 - val_loss: 0.1774 - val_accuracy: 0.9415\n",
            "Epoch 90/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0479 - accuracy: 0.9875 - val_loss: 0.1772 - val_accuracy: 0.9435\n",
            "Epoch 91/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0459 - accuracy: 0.9925 - val_loss: 0.1720 - val_accuracy: 0.9395\n",
            "Epoch 92/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0465 - accuracy: 0.9900 - val_loss: 0.1724 - val_accuracy: 0.9415\n",
            "Epoch 93/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0455 - accuracy: 0.9920 - val_loss: 0.1663 - val_accuracy: 0.9476\n",
            "Epoch 94/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0371 - accuracy: 0.9945 - val_loss: 0.1766 - val_accuracy: 0.9415\n",
            "Epoch 95/300\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0363 - accuracy: 0.9930 - val_loss: 0.1749 - val_accuracy: 0.9435\n",
            "Epoch 96/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0428 - accuracy: 0.9910 - val_loss: 0.1812 - val_accuracy: 0.9375\n",
            "Epoch 97/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0403 - accuracy: 0.9910 - val_loss: 0.1660 - val_accuracy: 0.9435\n",
            "Epoch 98/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0392 - accuracy: 0.9945 - val_loss: 0.1711 - val_accuracy: 0.9435\n",
            "Epoch 99/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0420 - accuracy: 0.9900 - val_loss: 0.1864 - val_accuracy: 0.9375\n",
            "Epoch 100/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0390 - accuracy: 0.9925 - val_loss: 0.1605 - val_accuracy: 0.9435\n",
            "Epoch 101/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0339 - accuracy: 0.9945 - val_loss: 0.1584 - val_accuracy: 0.9456\n",
            "Epoch 102/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0385 - accuracy: 0.9910 - val_loss: 0.2002 - val_accuracy: 0.9395\n",
            "Epoch 103/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0288 - accuracy: 0.9965 - val_loss: 0.1643 - val_accuracy: 0.9415\n",
            "Epoch 104/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0336 - accuracy: 0.9915 - val_loss: 0.1633 - val_accuracy: 0.9476\n",
            "Epoch 105/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0347 - accuracy: 0.9935 - val_loss: 0.1763 - val_accuracy: 0.9415\n",
            "Epoch 106/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0293 - accuracy: 0.9955 - val_loss: 0.1630 - val_accuracy: 0.9435\n",
            "Epoch 107/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0293 - accuracy: 0.9935 - val_loss: 0.1751 - val_accuracy: 0.9456\n",
            "Epoch 108/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0292 - accuracy: 0.9940 - val_loss: 0.1751 - val_accuracy: 0.9415\n",
            "Epoch 109/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0285 - accuracy: 0.9950 - val_loss: 0.1629 - val_accuracy: 0.9395\n",
            "Epoch 110/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0298 - accuracy: 0.9940 - val_loss: 0.1797 - val_accuracy: 0.9335\n",
            "Epoch 111/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0299 - accuracy: 0.9950 - val_loss: 0.1623 - val_accuracy: 0.9476\n",
            "Epoch 112/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0244 - accuracy: 0.9975 - val_loss: 0.2085 - val_accuracy: 0.9375\n",
            "Epoch 113/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0222 - accuracy: 0.9965 - val_loss: 0.1689 - val_accuracy: 0.9496\n",
            "Epoch 114/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0274 - accuracy: 0.9945 - val_loss: 0.1718 - val_accuracy: 0.9415\n",
            "Epoch 115/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0245 - accuracy: 0.9945 - val_loss: 0.1697 - val_accuracy: 0.9435\n",
            "Epoch 116/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0228 - accuracy: 0.9960 - val_loss: 0.1660 - val_accuracy: 0.9375\n",
            "Epoch 117/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0265 - accuracy: 0.9970 - val_loss: 0.1658 - val_accuracy: 0.9375\n",
            "Epoch 118/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0248 - accuracy: 0.9950 - val_loss: 0.1735 - val_accuracy: 0.9375\n",
            "Epoch 119/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0212 - accuracy: 0.9980 - val_loss: 0.1837 - val_accuracy: 0.9415\n",
            "Epoch 120/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0252 - accuracy: 0.9965 - val_loss: 0.1820 - val_accuracy: 0.9375\n",
            "Epoch 121/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0211 - accuracy: 0.9960 - val_loss: 0.1769 - val_accuracy: 0.9375\n",
            "Epoch 122/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0189 - accuracy: 0.9985 - val_loss: 0.2265 - val_accuracy: 0.9254\n",
            "Epoch 123/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0211 - accuracy: 0.9970 - val_loss: 0.1704 - val_accuracy: 0.9395\n",
            "Epoch 124/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0190 - accuracy: 0.9970 - val_loss: 0.1971 - val_accuracy: 0.9294\n",
            "Epoch 125/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0194 - accuracy: 0.9970 - val_loss: 0.1872 - val_accuracy: 0.9355\n",
            "Epoch 126/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0232 - accuracy: 0.9945 - val_loss: 0.1887 - val_accuracy: 0.9355\n",
            "Epoch 127/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0180 - accuracy: 0.9985 - val_loss: 0.1656 - val_accuracy: 0.9415\n",
            "Epoch 128/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0195 - accuracy: 0.9970 - val_loss: 0.1741 - val_accuracy: 0.9435\n",
            "Epoch 129/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0197 - accuracy: 0.9975 - val_loss: 0.2117 - val_accuracy: 0.9234\n",
            "Epoch 130/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0188 - accuracy: 0.9980 - val_loss: 0.1936 - val_accuracy: 0.9476\n",
            "Epoch 131/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0176 - accuracy: 0.9980 - val_loss: 0.1741 - val_accuracy: 0.9456\n",
            "Epoch 132/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0218 - accuracy: 0.9950 - val_loss: 0.1869 - val_accuracy: 0.9476\n",
            "Epoch 133/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0178 - accuracy: 0.9960 - val_loss: 0.1720 - val_accuracy: 0.9456\n",
            "Epoch 134/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0190 - accuracy: 0.9960 - val_loss: 0.1853 - val_accuracy: 0.9415\n",
            "Epoch 135/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0170 - accuracy: 0.9985 - val_loss: 0.1785 - val_accuracy: 0.9456\n",
            "Epoch 136/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0176 - accuracy: 0.9975 - val_loss: 0.2012 - val_accuracy: 0.9395\n",
            "Epoch 137/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0199 - accuracy: 0.9950 - val_loss: 0.1725 - val_accuracy: 0.9435\n",
            "Epoch 138/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0190 - accuracy: 0.9955 - val_loss: 0.1622 - val_accuracy: 0.9456\n",
            "Epoch 139/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0159 - accuracy: 0.9980 - val_loss: 0.1650 - val_accuracy: 0.9435\n",
            "Epoch 140/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0167 - accuracy: 0.9980 - val_loss: 0.1964 - val_accuracy: 0.9415\n",
            "Epoch 141/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0168 - accuracy: 0.9965 - val_loss: 0.1818 - val_accuracy: 0.9435\n",
            "Epoch 142/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0120 - accuracy: 0.9995 - val_loss: 0.1728 - val_accuracy: 0.9435\n",
            "Epoch 143/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0155 - accuracy: 0.9985 - val_loss: 0.1635 - val_accuracy: 0.9375\n",
            "Epoch 144/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0163 - accuracy: 0.9980 - val_loss: 0.1805 - val_accuracy: 0.9395\n",
            "Epoch 145/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0147 - accuracy: 0.9985 - val_loss: 0.1652 - val_accuracy: 0.9476\n",
            "Epoch 146/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0110 - accuracy: 0.9980 - val_loss: 0.1607 - val_accuracy: 0.9476\n",
            "Epoch 147/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0145 - accuracy: 0.9975 - val_loss: 0.1834 - val_accuracy: 0.9496\n",
            "Epoch 148/300\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0129 - accuracy: 0.9980 - val_loss: 0.1773 - val_accuracy: 0.9415\n",
            "Epoch 149/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0199 - accuracy: 0.9945 - val_loss: 0.1642 - val_accuracy: 0.9456\n",
            "Epoch 150/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0167 - accuracy: 0.9975 - val_loss: 0.1670 - val_accuracy: 0.9476\n",
            "Epoch 151/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0112 - accuracy: 0.9980 - val_loss: 0.1703 - val_accuracy: 0.9435\n",
            "Epoch 152/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0138 - accuracy: 0.9975 - val_loss: 0.1698 - val_accuracy: 0.9435\n",
            "Epoch 153/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0129 - accuracy: 0.9980 - val_loss: 0.1665 - val_accuracy: 0.9456\n",
            "Epoch 154/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0156 - accuracy: 0.9965 - val_loss: 0.1952 - val_accuracy: 0.9415\n",
            "Epoch 155/300\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0138 - accuracy: 0.9975 - val_loss: 0.1661 - val_accuracy: 0.9435\n",
            "Epoch 156/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0089 - accuracy: 0.9995 - val_loss: 0.1871 - val_accuracy: 0.9375\n",
            "Epoch 157/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0136 - accuracy: 0.9980 - val_loss: 0.1752 - val_accuracy: 0.9496\n",
            "Epoch 158/300\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.0145 - accuracy: 0.9965 - val_loss: 0.1819 - val_accuracy: 0.9435\n",
            "Epoch 159/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0127 - accuracy: 0.9975 - val_loss: 0.1826 - val_accuracy: 0.9476\n",
            "Epoch 160/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0126 - accuracy: 0.9975 - val_loss: 0.1885 - val_accuracy: 0.9456\n",
            "Epoch 161/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.1869 - val_accuracy: 0.9415\n",
            "Epoch 162/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0144 - accuracy: 0.9970 - val_loss: 0.2088 - val_accuracy: 0.9355\n",
            "Epoch 163/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.1774 - val_accuracy: 0.9415\n",
            "Epoch 164/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0073 - accuracy: 0.9995 - val_loss: 0.1759 - val_accuracy: 0.9496\n",
            "Epoch 165/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0161 - accuracy: 0.9960 - val_loss: 0.1806 - val_accuracy: 0.9415\n",
            "Epoch 166/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0100 - accuracy: 0.9995 - val_loss: 0.1886 - val_accuracy: 0.9415\n",
            "Epoch 167/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0088 - accuracy: 0.9995 - val_loss: 0.1792 - val_accuracy: 0.9415\n",
            "Epoch 168/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0093 - accuracy: 0.9995 - val_loss: 0.1715 - val_accuracy: 0.9516\n",
            "Epoch 169/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0081 - accuracy: 0.9995 - val_loss: 0.1962 - val_accuracy: 0.9355\n",
            "Epoch 170/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0108 - accuracy: 0.9980 - val_loss: 0.1934 - val_accuracy: 0.9435\n",
            "Epoch 171/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0176 - accuracy: 0.9945 - val_loss: 0.2129 - val_accuracy: 0.9335\n",
            "Epoch 172/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.1968 - val_accuracy: 0.9335\n",
            "Epoch 173/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0120 - accuracy: 0.9985 - val_loss: 0.1755 - val_accuracy: 0.9496\n",
            "Epoch 174/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0079 - accuracy: 0.9990 - val_loss: 0.1852 - val_accuracy: 0.9435\n",
            "Epoch 175/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0088 - accuracy: 0.9975 - val_loss: 0.1914 - val_accuracy: 0.9476\n",
            "Epoch 176/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 0.1805 - val_accuracy: 0.9516\n",
            "Epoch 177/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.1741 - val_accuracy: 0.9395\n",
            "Epoch 178/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 0.2005 - val_accuracy: 0.9415\n",
            "Epoch 179/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0073 - accuracy: 0.9995 - val_loss: 0.1748 - val_accuracy: 0.9476\n",
            "Epoch 180/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.1838 - val_accuracy: 0.9476\n",
            "Epoch 181/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0160 - accuracy: 0.9965 - val_loss: 0.2148 - val_accuracy: 0.9355\n",
            "Epoch 182/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0089 - accuracy: 0.9990 - val_loss: 0.1707 - val_accuracy: 0.9536\n",
            "Epoch 183/300\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0117 - accuracy: 0.9970 - val_loss: 0.2010 - val_accuracy: 0.9456\n",
            "Epoch 184/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1819 - val_accuracy: 0.9415\n",
            "Epoch 185/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0077 - accuracy: 0.9990 - val_loss: 0.1796 - val_accuracy: 0.9516\n",
            "Epoch 186/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0110 - accuracy: 0.9975 - val_loss: 0.2011 - val_accuracy: 0.9415\n",
            "Epoch 187/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0072 - accuracy: 0.9990 - val_loss: 0.1851 - val_accuracy: 0.9496\n",
            "Epoch 188/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.1816 - val_accuracy: 0.9435\n",
            "Epoch 189/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0068 - accuracy: 0.9995 - val_loss: 0.1894 - val_accuracy: 0.9456\n",
            "Epoch 190/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0065 - accuracy: 0.9995 - val_loss: 0.1955 - val_accuracy: 0.9456\n",
            "Epoch 191/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0075 - accuracy: 0.9990 - val_loss: 0.2157 - val_accuracy: 0.9395\n",
            "Epoch 192/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0077 - accuracy: 0.9990 - val_loss: 0.1815 - val_accuracy: 0.9476\n",
            "Epoch 193/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0083 - accuracy: 0.9985 - val_loss: 0.1983 - val_accuracy: 0.9375\n",
            "Epoch 194/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0117 - accuracy: 0.9970 - val_loss: 0.2042 - val_accuracy: 0.9375\n",
            "Epoch 195/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.1992 - val_accuracy: 0.9496\n",
            "Epoch 196/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.1825 - val_accuracy: 0.9435\n",
            "Epoch 197/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 0.1799 - val_accuracy: 0.9476\n",
            "Epoch 198/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0061 - accuracy: 0.9995 - val_loss: 0.1834 - val_accuracy: 0.9415\n",
            "Epoch 199/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.1926 - val_accuracy: 0.9476\n",
            "Epoch 200/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.1836 - val_accuracy: 0.9395\n",
            "Epoch 201/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.2200 - val_accuracy: 0.9315\n",
            "Epoch 202/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0083 - accuracy: 0.9990 - val_loss: 0.1729 - val_accuracy: 0.9456\n",
            "Epoch 203/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.1684 - val_accuracy: 0.9476\n",
            "Epoch 204/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0099 - accuracy: 0.9975 - val_loss: 0.1768 - val_accuracy: 0.9456\n",
            "Epoch 205/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.1777 - val_accuracy: 0.9456\n",
            "Epoch 206/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.1816 - val_accuracy: 0.9435\n",
            "Epoch 207/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1801 - val_accuracy: 0.9496\n",
            "Epoch 208/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.2311 - val_accuracy: 0.9375\n",
            "Epoch 209/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1967 - val_accuracy: 0.9395\n",
            "Epoch 210/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0075 - accuracy: 0.9990 - val_loss: 0.1945 - val_accuracy: 0.9415\n",
            "Epoch 211/300\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 0.1827 - val_accuracy: 0.9435\n",
            "Epoch 212/300\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9415\n",
            "Epoch 213/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.1809 - val_accuracy: 0.9435\n",
            "Epoch 214/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.1965 - val_accuracy: 0.9435\n",
            "Epoch 215/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.1960 - val_accuracy: 0.9516\n",
            "Epoch 216/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.2033 - val_accuracy: 0.9415\n",
            "Epoch 217/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.2271 - val_accuracy: 0.9294\n",
            "Epoch 218/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.2287 - val_accuracy: 0.9395\n",
            "Epoch 219/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.2025 - val_accuracy: 0.9355\n",
            "Epoch 220/300\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 0.2030 - val_accuracy: 0.9435\n",
            "Epoch 221/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2050 - val_accuracy: 0.9395\n",
            "Epoch 222/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 0.1799 - val_accuracy: 0.9496\n",
            "Epoch 223/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 0.2105 - val_accuracy: 0.9375\n",
            "Epoch 224/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2056 - val_accuracy: 0.9415\n",
            "Epoch 225/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0055 - accuracy: 0.9995 - val_loss: 0.1989 - val_accuracy: 0.9456\n",
            "Epoch 226/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.1937 - val_accuracy: 0.9415\n",
            "Epoch 227/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.2016 - val_accuracy: 0.9415\n",
            "Epoch 228/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 0.1972 - val_accuracy: 0.9456\n",
            "Epoch 229/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.2270 - val_accuracy: 0.9375\n",
            "Epoch 230/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9415\n",
            "Epoch 231/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1819 - val_accuracy: 0.9496\n",
            "Epoch 232/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0064 - accuracy: 0.9990 - val_loss: 0.2271 - val_accuracy: 0.9375\n",
            "Epoch 233/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 0.2028 - val_accuracy: 0.9315\n",
            "Epoch 234/300\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9415\n",
            "Epoch 235/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.2293 - val_accuracy: 0.9335\n",
            "Epoch 236/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.1991 - val_accuracy: 0.9375\n",
            "Epoch 237/300\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2848 - val_accuracy: 0.9254\n",
            "Epoch 238/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.1974 - val_accuracy: 0.9435\n",
            "Epoch 239/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9435\n",
            "Epoch 240/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.1913 - val_accuracy: 0.9435\n",
            "Epoch 241/300\n",
            "250/250 [==============================] - 17s 66ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2038 - val_accuracy: 0.9355\n",
            "Epoch 242/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9415\n",
            "Epoch 243/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.2083 - val_accuracy: 0.9395\n",
            "Epoch 244/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.2014 - val_accuracy: 0.9435\n",
            "Epoch 245/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9375\n",
            "Epoch 246/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.1914 - val_accuracy: 0.9375\n",
            "Epoch 247/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 0.2095 - val_accuracy: 0.9375\n",
            "Epoch 248/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1653 - val_accuracy: 0.9476\n",
            "Epoch 249/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 0.2066 - val_accuracy: 0.9375\n",
            "Epoch 250/300\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2147 - val_accuracy: 0.9375\n",
            "Epoch 251/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1833 - val_accuracy: 0.9476\n",
            "Epoch 252/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.2440 - val_accuracy: 0.9294\n",
            "Epoch 253/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.2194 - val_accuracy: 0.9335\n",
            "Epoch 254/300\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2042 - val_accuracy: 0.9395\n",
            "Epoch 255/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.1915 - val_accuracy: 0.9456\n",
            "Epoch 256/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1888 - val_accuracy: 0.9415\n",
            "Epoch 257/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.1768 - val_accuracy: 0.9476\n",
            "Epoch 258/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1828 - val_accuracy: 0.9415\n",
            "Epoch 259/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1828 - val_accuracy: 0.9476\n",
            "Epoch 260/300\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2076 - val_accuracy: 0.9435\n",
            "Epoch 261/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1900 - val_accuracy: 0.9456\n",
            "Epoch 262/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9415\n",
            "Epoch 263/300\n",
            "250/250 [==============================] - 18s 74ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2644 - val_accuracy: 0.9375\n",
            "Epoch 264/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.2201 - val_accuracy: 0.9435\n",
            "Epoch 265/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 0.2558 - val_accuracy: 0.9415\n",
            "Epoch 266/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.9435\n",
            "Epoch 267/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0176 - accuracy: 0.9955 - val_loss: 0.3214 - val_accuracy: 0.9294\n",
            "Epoch 268/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.2170 - val_accuracy: 0.9476\n",
            "Epoch 269/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.2276 - val_accuracy: 0.9456\n",
            "Epoch 270/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2186 - val_accuracy: 0.9476\n",
            "Epoch 271/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2184 - val_accuracy: 0.9435\n",
            "Epoch 272/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.2225 - val_accuracy: 0.9415\n",
            "Epoch 273/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2345 - val_accuracy: 0.9375\n",
            "Epoch 274/300\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2112 - val_accuracy: 0.9496\n",
            "Epoch 275/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0072 - accuracy: 0.9965 - val_loss: 0.2868 - val_accuracy: 0.9234\n",
            "Epoch 276/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2105 - val_accuracy: 0.9435\n",
            "Epoch 277/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.2245 - val_accuracy: 0.9395\n",
            "Epoch 278/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.2120 - val_accuracy: 0.9456\n",
            "Epoch 279/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9456\n",
            "Epoch 280/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.2001 - val_accuracy: 0.9435\n",
            "Epoch 281/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2110 - val_accuracy: 0.9415\n",
            "Epoch 282/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.2058 - val_accuracy: 0.9375\n",
            "Epoch 283/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.2289 - val_accuracy: 0.9435\n",
            "Epoch 284/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.9496\n",
            "Epoch 285/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.1835 - val_accuracy: 0.9496\n",
            "Epoch 286/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9435\n",
            "Epoch 287/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.2073 - val_accuracy: 0.9375\n",
            "Epoch 288/300\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9496\n",
            "Epoch 289/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2031 - val_accuracy: 0.9415\n",
            "Epoch 290/300\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.2125 - val_accuracy: 0.9375\n",
            "Epoch 291/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0067 - accuracy: 0.9975 - val_loss: 0.2205 - val_accuracy: 0.9435\n",
            "Epoch 292/300\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.1991 - val_accuracy: 0.9415\n",
            "Epoch 293/300\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2287 - val_accuracy: 0.9435\n",
            "Epoch 294/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2881 - val_accuracy: 0.9294\n",
            "Epoch 295/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2096 - val_accuracy: 0.9456\n",
            "Epoch 296/300\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2104 - val_accuracy: 0.9456\n",
            "Epoch 297/300\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9415\n",
            "Epoch 298/300\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2114 - val_accuracy: 0.9395\n",
            "Epoch 299/300\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 0.9415\n",
            "Epoch 300/300\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2255 - val_accuracy: 0.9415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Akurasi')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Akurasi')\n",
        "plt.title('Akurasi Training dan Validasi')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Akurasi')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "490eb409-7eeb-4c07-fe32-db8cf7465818",
        "id": "Xox44VqEeKfN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACFxklEQVR4nO3dd3gU1RoG8Hd3k9303iGEktCbVOkoHURp0iWggkgTuBa4SlGvoqCIBUGRYkFBEBAFaaEoRTrSETAhCElISG+b7O65f0xmSxoJbLIheX/Ps092Z87MnJnM7nzznTMzCiGEABEREVElobR1BYiIiIisicENERERVSoMboiIiKhSYXBDRERElQqDGyIiIqpUGNwQERFRpcLghoiIiCoVBjdERERUqTC4ISIiokqFwQ1RCUVFRUGhUOCDDz6wdVVKZf78+VAoFLauBgCgZs2aGDt27H1N27VrV3Tt2tWq9XlQCoUC8+fPt3U1Htj+/fuhUCiwf/9+47CxY8eiZs2a95xW/l6sWbOmTOpW0noQmWNwQ5Tn888/h0KhQNu2bW1dlXIlH9hK8iLba9q0KWrUqIHinpzToUMH+Pv7Q6fTlWPNiCoOO1tXgKiiWLt2LWrWrIljx47h2rVrCA0NtXWVrOKNN97ArFmzihzfoEEDfPvttxbDZs+eDRcXF7z++utWrcuVK1egVN7fOdWuXbusWpeH1ahRozBr1iz88ccf6Ny5c4HxUVFROHLkCKZMmQI7u/v/iV+xYgUMBsODVNUqKko96OHC4IYIQGRkJA4fPoxNmzbhhRdewNq1azFv3rxyr0dGRgacnZ2tOk87O7tiD3L+/v4YPXq0xbD33nsPPj4+BYabMxgMyMnJgYODQ4nrotFoSlw2P7Vafd/TViYjR47E7Nmz8f333xca3Pzwww8QQmDUqFEPtBx7e/sHmt5aKko96OHCZikiSFkbT09P9OvXD0OGDMHatWtLNJ0QAhMmTIBarcamTZsAFN0PI39/kzVr1kChUODAgQOYNGkS/Pz8UL16dQDAjRs3MGnSJNSrVw+Ojo7w9vbG008/jaioKIt55ubm4s0330RYWBgcHBzg7e2Njh07Yvfu3cYy1upzo1AoMGXKFKxduxaNGjWCRqPBjh07AAAffPAB2rdvD29vbzg6OqJly5bYuHFjibfBoUOHMHPmTPj6+sLZ2RkDBw5EfHy8xbT5+9zIzWk//vgj3nnnHVSvXh0ODg7o1q0brl27VmDZS5cuRe3ateHo6Ig2bdrgjz/+KHE/Hq1WixkzZsDX1xeurq548skn8e+//xYoV9L/W2nWO7/g4GB07twZGzduRG5uboHx33//PerUqYO2bduWuD6FKayvS3JyMsaOHQt3d3d4eHggPDwcycnJBaY9e/Ysxo4di9q1a8PBwQEBAQF49tlncffuXYtyaWlpmD59OmrWrAmNRgM/Pz/06NEDp06dKrYeRPfCzA0RpOBm0KBBUKvVGDFiBJYtW4bjx4+jdevWRU6j1+vx7LPPYv369di8eTP69et3X8ueNGkSfH19MXfuXGRkZAAAjh8/jsOHD2P48OGoXr06oqKisGzZMnTt2hUXL16Ek5MTAClwWbBgAZ5//nm0adMGqampOHHiBE6dOoUePXrcV32Ks3fvXvz444+YMmUKfHx8jAedjz/+GE8++SRGjRqFnJwcrFu3Dk8//TR+/fXXEm2XqVOnwtPTE/PmzUNUVBSWLFmCKVOmYP369fec9r333oNSqcTLL7+MlJQULFy4EKNGjcLRo0eNZZYtW4YpU6agU6dOmDFjBqKiojBgwAB4enoaA8riPP/88/juu+8wcuRItG/fHnv37i10vUr6f3vQ9R41ahQmTJiAnTt34oknnjAOP3fuHM6fP4+5c+feV32KI4TAU089hYMHD2LixIlo0KABNm/ejPDw8AJld+/ejX/++Qfjxo1DQEAALly4gC+//BIXLlzAn3/+aQy2J06ciI0bN2LKlClo2LAh7t69i4MHD+LSpUto0aJFietGVIAgquJOnDghAIjdu3cLIYQwGAyievXq4qWXXrIoFxkZKQCIRYsWidzcXDFs2DDh6Ogodu7caVEOgJg3b16B5YSEhIjw8HDj59WrVwsAomPHjkKn01mUzczMLDD9kSNHBADxzTffGIc1a9ZM9OvXr9j1mzdvnijtV71Ro0aiS5cuFsMACKVSKS5cuFCgfP765uTkiMaNG4vHH3/cYnhR26B79+7CYDAYh8+YMUOoVCqRnJxsHNalSxeLOu3bt08AEA0aNBBardY4/OOPPxYAxLlz54QQQmi1WuHt7S1at24tcnNzjeXWrFkjABRYz/zOnDkjAIhJkyZZDB85cmSB/3VJ/2+lWe/CJCYmCo1GI0aMGGExfNasWQKAuHLlSqnqI2/Lffv2GYeFh4eLkJAQ4+ctW7YIAGLhwoXGYTqdTnTq1EkAEKtXry52O/zwww8CgPj999+Nw9zd3cXkyZOLXdf89SAqCTZLUZW3du1a+Pv747HHHgMgNb8MGzYM69atg16vL1A+JyfHmJXYvn07evbs+UDLHz9+PFQqlcUwR0dH4/vc3FzcvXsXoaGh8PDwsEjZe3h44MKFC7h69eoD1aGkunTpgoYNGxYYbl7fpKQkpKSkoFOnThZ1Lc6ECRMsms46deoEvV6PGzdu3HPacePGWfTH6dSpEwDgn3/+AQCcOHECd+/exfjx4y36Ho0aNQqenp73nP/27dsBANOmTbMYPn369AJlS/p/k93vent6eqJv377YunWrMdsnhMC6devQqlUr1K1b977qU5zt27fDzs4OL774onGYSqXC1KlTC5Q1X252djYSEhLw6KOPAkCB/ffo0aO4fft2qepCdC8MbqhK0+v1WLduHR577DFERkbi2rVruHbtGtq2bYu4uDhEREQUmGbBggXYsmULNm7caJX7rtSqVavAsKysLMydOxfBwcHQaDTw8fGBr68vkpOTkZKSYiz31ltvITk5GXXr1kWTJk3wyiuv4OzZsw9cp9LUFQB+/fVXPProo3BwcICXlxd8fX2xbNkyi7oWp0aNGhaf5aAjKSnpgaeVA4X8V7/Z2dmVqC/HjRs3oFQqUadOHYvh9erVK1C2pP+3kta9OKNGjUJGRgZ+/vlnAMDhw4cRFRVl0ZG4tPUpzo0bNxAYGAgXFxeL4YVth8TERLz00kvw9/eHo6MjfH19jfuO+XIXLlyI8+fPIzg4GG3atMH8+fONQSnRg2BwQ1Xa3r17ERMTg3Xr1iEsLMz4Gjp0KAAU2rG4V69ecHZ2xsKFC5GdnV3iZRWWBQIsz3JlU6dOxTvvvIOhQ4fixx9/xK5du7B79254e3tbXBbbuXNnXL9+HatWrULjxo3x1VdfoUWLFvjqq69KXK/SKKyuf/zxB5588kk4ODjg888/x/bt27F7926MHDmy2HuxmMufuZKVZPoHmdbaSvp/kz1I3Z944gm4u7vj+++/ByB1JFapVBg+fPh918dahg4dihUrVmDixInYtGkTdu3aZex8br7coUOH4p9//sGnn36KoKAgLFq0CI0aNcJvv/1WZnWjqoEdiqlKW7t2Lfz8/LB06dIC4zZt2oTNmzdj+fLlFgf1Rx99FBMnTsQTTzyBp59+Gps3b7Zo7vD09CxwBUlOTg5iYmJKXK+NGzciPDwcH374oXFYdnZ2oVemeHl5Ydy4cRg3bhzS09PRuXNnzJ8/H88//3yJl/cgfvrpJzg4OGDnzp0Wl3qvXr26XJZ/LyEhIQCAa9euGZseAUCn0yEqKgpNmza95/QGgwHXr1+3yFJcuXKlQNnS/N8elEajwZAhQ/DNN98gLi4OGzZswOOPP46AgIAyqU9ISAgiIiKQnp5ukb3Jvx2SkpIQERGBN99809ixGUCRTaeBgYGYNGkSJk2ahDt37qBFixZ455130KdPn1LXkUjGzA1VWVlZWdi0aROeeOIJDBkypMBrypQpSEtLw9atWwtM2717d6xbtw47duzAM888Y3E2WqdOHfz+++8W5b/88ssiMzeFUalUBc7eP/300wLzyH9prYuLC0JDQ6HVaku8rAelUqmgUCgs6hYVFYUtW7aUWx2K06pVK3h7e2PFihUWd+xdu3ZtiZp/5IPsJ598YjF8yZIlBcqW9P9mLaNGjUJubi5eeOEFxMfHF7i3jTXr07dvX+h0Oixbtsw4TK/X49NPPy2wTKBg9in/9tLr9QWaxvz8/BAUFFSu+y9VTszcUJW1detWpKWl4cknnyx0/KOPPgpfX1+sXbsWw4YNKzB+wIABWL16NcaMGQM3Nzd88cUXAKTLhidOnIjBgwejR48e+Ouvv7Bz5074+PiUuG5PPPEEvv32W7i7u6Nhw4Y4cuQI9uzZA29vb4tyDRs2RNeuXdGyZUt4eXnhxIkTxktry0u/fv2wePFi9O7dGyNHjsSdO3ewdOlShIaGlmn/n5JSq9WYP38+pk6discffxxDhw5FVFQU1qxZgzp16tzzHkDNmzfHiBEj8PnnnyMlJQXt27dHREREoffSKen/zVq6dOmC6tWr4+eff4ajoyMGDRpUZvXp378/OnTogFmzZiEqKgoNGzbEpk2bCgQobm5u6Ny5MxYuXIjc3FxUq1YNu3btQmRkpEW5tLQ0VK9eHUOGDEGzZs3g4uKCPXv24Pjx4xaZJqL7weCGqqy1a9fCwcGhyPvBKJVK9OvXD2vXri2QIZGNHj0aaWlpmDRpEtzc3LBo0SKMHz8ekZGRWLlyJXbs2IFOnTph9+7d6NatW4nr9vHHH0OlUmHt2rXIzs5Ghw4dsGfPHvTq1cui3LRp07B161bs2rULWq0WISEh+N///odXXnml5BviAT3++ONYuXIl3nvvPUyfPh21atXC+++/j6ioqAoR3ADAlClTIITAhx9+iJdffhnNmjXD1q1bMW3atBLdYXnVqlXGQHfLli14/PHHsW3bNgQHB1uUK+n/zVqUSiVGjBiBRYsWoX///nB1dS2z+iiVSmzduhXTp0/Hd999B4VCgSeffBIffvghHnnkEYuy33//PaZOnYqlS5dCCIGePXvit99+Q1BQkLGMk5MTJk2ahF27dmHTpk0wGAwIDQ3F559/bnFFFtH9UAhb9LojIrIxg8EAX19fDBo0CCtWrLB1dYjIitjnhogqvezs7AJ9QL755hskJiZa5XJ+IqpYmLkhokpv//79mDFjBp5++ml4e3vj1KlTWLlyJRo0aICTJ0/yoZxElQz73BBRpVezZk0EBwfjk08+QWJiIry8vDBmzBi89957DGyIKiFmboiIiKhSYZ8bIiIiqlQY3BAREVGlUuX63BgMBty+fRuurq73vHkXERERVQxCCKSlpSEoKAhKZfG5mSoX3Ny+fbvAjbeIiIjo4XDz5k1Ur1692DJVLriR7+B58+ZNuLm52bg2REREVBKpqakIDg4ucCfuwlS54EZuinJzc2NwQ0RE9JApSZcSdigmIiKiSoXBDREREVUqDG6IiIioUmFwQ0RERJUKgxsiIiKqVBjcEBERUaXC4IaIiIgqFQY3REREVKkwuCEiIqJKhcENERERVSo2DW5+//139O/fH0FBQVAoFNiyZcs9p9m/fz9atGgBjUaD0NBQrFmzpszrSURERA8PmwY3GRkZaNasGZYuXVqi8pGRkejXrx8ee+wxnDlzBtOnT8fzzz+PnTt3lnFNiYiI6GFh0wdn9unTB3369Clx+eXLl6NWrVr48MMPAQANGjTAwYMH8dFHH6FXr15lVU0iIqsQQsBgyIJK5XTPsnp9BhQKDZRK2/1M6/XZyM2Ng0JhB7U6CACg0yXD3t7TrEwGABVUKgfjMJ0uDSqVS7EPONTrs6BUqqFQqIyfhciBnZ17kdMIIaDV3gSggEZTDQqFdH5uMGiRkxMLQJU3XAGdLhUqlTMUClXeexcoFErodGnQ6RKhUrnC3t7LrM7pUKmczOapg8GQCTs7NwhhgFZ7C4CARlPdWEZef6XSCYABublJsLf3BmCAVnsbgKHIdbG397XYD/T6bIttKK2vATk5MRBCBzs7b9jZuUCnS4dOdxcKhQZqtX+h21jazzKhUjlDr8/Mm970oOjc3GTY2blDoVDk1d8BgBI5OXEQQmtcljmDIRc5ObeLXJ/8FAoNNJqAEpe3tofqqeBHjhxB9+7dLYb16tUL06dPL3IarVYLrVZr/JyamlpW1SOqlITQIycnDhpNUKHjc3PvQqVygVKpKdH8cnPvQqGwt/ixLUxSUgRu3HgXnp494O8/ChpNdWi1/8J0YLv3k4ELznMvoqMXwNv7KVSr9qLxwCrT6VJhMGRBrfaHTpeC2NivkZCwFdWqTYGzc2NcufI89PoUi2nU6mqoVestuLq2AABkZUVBp0tCbu4dZGZehrt7J7i6toBWexsXLgxGWtpphIYuhlZ7G6mpf8LffwT8/EZCqVQjLu47ODs3gcGQhbNne0OlckVg4AQEB89EdnYkEhK2QogcZGZeQVbWNcgHW2fnJlAo1NBoqsHNrR1cXBoDAHJy4nD16kvIyrpiVt9AuLq2gotLC6Snn0Ry8gE4OobC07M7fHwGGQ+wubl3ceJE87xtDtjZeUIIA/T6FLi4tISzcyOkpZ1AZuYlKJVO8PcfhZo15+LOnQ24fn0GnJ2bwsXlEWRn/wMPjy5Qqdxw9+42+PoORk7Obdy8+QGUSgc4ONSGELnIzPwbgB4ODrVgZ+cOlcoNzs6NodVGQ6VyQ40as3DlyvNISzsGAHBwqAMPjy5IT/8LGRlnIUQuAMDFpQXc3Nrh9u3l0GiqwcmpAZKSdsHNrR18fQfjn3/+CyG0ABTw8uoLX9/ByMq6jps3F8HRsTYCA1/A3bs/IzX1KAyGLHh6dkdWViSys68DAJydm6Bx482ws/PG5ctjcPfuL7C394VenwmDIQP29r4wGLKg16ffY29Uwtm5CYKDX0ZS0m7cufM96tVbDW/vvrh5czFSU/9EWtoJs/1N2u/lIAsAatV6B56e3XD9+qtwcKgBD4+ucHKqj0uXwpGdfT3vOyMFWY6OYXB1bQWt9hZSUn6Hh0dXODjURGzsGiiVzlCpnJCbGw8AUKnc0KLFUTg710dubjJu3vwAMTErkJt7pwTfMombWzu0aHG4xOWtTSGEEDZbuhmFQoHNmzdjwIABRZapW7cuxo0bh9mzZxuHbd++Hf369UNmZiYcHR0LTDN//ny8+eabBYanpKTAza34H1eiyiI5+QCSk/+An99wODmFWowTQiAj4xwcHGpCp0vCpUvhcHKqh1q13oFSqcG5c32RknIQ/v7PwN9/TF5QoYCbW1tkZV3DqVMdoFK5oEaNWcjNvYO0tOPIzLwKwABf36GoU2eh8Uw3KWkfzp17AoABfn4jULv2+1CrfZGbm4jLl8chPf00VCpn+PmNxM2bi6DXpxnrqVQ6wWDIBADY2/vDza01ACArKxKhoR/C2bkZoqPfRWrqEQihh5NTfWRlXYO9vQ9q1JiNlJSDiIycA0APAHBxaYnQ0A+RkXEJmZkXkJubgISELRBCD3//Mbh792fk5ibkLdsZjo61kZFxrogtrERo6EdwcKiN8+efhHzwkSjg6dkDaWknodPdLXRqJ6f6cHZugvj4DVAo7KBSuUKnSzKOt7f3QW5uIorLBJjz8RkIT89uiI5+zxiclISdnRfc3TvC2/sJpKYeRWzsSgByAKi/5/Slref9UQJQFKiPQqGGELoSLVuh0OQFOKVh2g5KpSMUCnWBQNdyGXZQKArPHwghCl2+vb0fHB1DkZpqHhSooFDYWZQ31V8FOzuPIver0lNCoVBCCB1cXFoiKGgCIiPnIjc37p7rlJ+ra1s88sh+K9VLkpqaCnd39xIdvyt9cFNY5iY4OJjBDT2U0tPP4cKFwQgO/g+Cgl4AAGi1t6HV3oabW6tCp0lI+BUXLgwyntkGBDyHsLDPoFRqEBe3FtHRC5CZeREaTQ3Y2bkhI+M8AEClcoG9vQ+ys6MKna9GEwx7e1+kp58qts7Vq0+HQmEHrfYWEhJ+NgYogHQWXK/eCvz992Skp58sMK2raysoFPZITT0G6WBW+IHWzs4bGk01ZGScLbYuAODp2ROpqUeLPTDJHB3rQaVyRHr6mbzleKB+/W+MWSohDIiNXYP4+PUAVHkH9zjY2/vk1SkIycn7jPNzcmoEb+8ncPPmQmg0wfD3H43Y2NXIyYkpsGxn56aoUeM1REXNy8vSAN7eT0CjqQGNJhjOzo2hUNghK+tvY9YjM/MqUlIO5B3kTetQp87CvKYHgays60hLO460tFNQqwPyMhf/4M6dtYUGQs2b/wE3tzbIyLgAQAG12h/x8T9Cp0uGi0tLuLq2RFbW37h6dZpx+wcEjIWLS0vk5sZDo6mG+PifYDBkwd29A27fXg6DIRt1666Ai0vzvKYOJZyc6kGlckFGxtm8ZqYYZGSch1pdDbGxq5CRcQ729j5o3nw/HBxq4s6dH5GVdQ0uLs3g6toaDg41kZt7B//8MxsZGecREjIHOl0KtNpouLq2wbVr05CZeRm1ar2NGjX+i6ysvxEbuwYpKYdhMGShevXpSE09guTkA/DzGwZf3yEAVIiP3wi12h9+fkOh06Xg/PmBxuyRRlMdDRuuzwtIXeDgEIKMjItQKh3h7NygQGbQnFZ7GzExX+HGjXehUjlDpXLOa3IDVCpX1KnzAVxd28DZuRGUSnvk5MQhM/NvODqGQaMJwMWLI3Hnzg95+0oz+PgMwJ073yMr6yo8PXuhbt2l0Gpvw9GxNpRKB6SlnUBa2gkoFHbw8OiKmzc/QE5OLGrVWgB7e0/o9Rlwdm4CnS4Jx483tgiuHR3roVat/8HH5ykolfbFfWXKVKUNbjp37owWLVpgyZIlxmGrV6/G9OnTkZJy7x8qoHQbh6isCCFw8+YiaLW3ERIyG2q1v3HcrVtLkZ0djdq134VCoUJGxmXcvPkBAgPH4ebND5CQsAUqlRseffQGUlOP4OLF4dDrU1G9+kx4ej6OO3c24O7dn+HoWA9OTmG4c2c9hMiFo2MosrKuAxBwcqoPpdLBeNA2Z2/vC7U60HigUqncEBr6Me7cWWdsc8/JiTFmNVQqF1SvPh3Jyb/DyakuXF1bwdm5CVJT/8T16/8pMH9Pz56oUWMWLl0aZXFQt7f3QYMG3yEz8woiI+dArQ7AI48chFotpfyzs6Pg4FALgEB6+l9ISzsBAIiJWYmMjL+MdQ8L+wwKhR0yM6/AwaEWEhI2Iz7+Rzg5NUJw8EwEBIxDTk4crl9/GXfurIWTUwN4ez8JpdIBXl49kJMTj1u3PoaXV19Urz4dOTkxOH68KfT6FDRo8AP8/YcX+F9evDgiL8ABNJoQtGlz0difIiXlMFJTj8LZuTE8PDpDqdRAq70Ne3vvvPexuHBhCNLSTqB+/TXIyYlBcvI+hIV9CgeHEOj12YiL+xqOjvXg6dn1nvtWevp5REXNh8GQATe39qhWbSrs7T3uOZ3BoENa2lEkJx9AdPRC6PUpCAgYi/r1V99zWkDqs/LPP6/BYNCibt2lRTZT6nRpMBiyoVb7lmi+8rzj4r6Dl1cPODrWKfF05gyGXOTmJkCjCbyv6WVC6JGRcR5C6ODk1KhAP5nSkptpExN34uLFoQCAevVWIzBw7D2mS8Lp0x2h12fgkUcOwMEhBEIYkJX1Dxwd69xXs60sPn4TLl4cBo0mBNWqTUa1apNK3OxcliptcPPaa69h+/btOHfOlBoeOXIkEhMTsWPHjhIth8EN2YJen4G4uO/g4zMQ9va+uH79P/j3348ASNmAunWXw89vGO7e3ZbXbAPUq7cSanUQLl4cBr0+NS/9nAK5ycPNrUNe+vreX2Ff36Fo0OA7JCcfwMWLQ41nZUqlI0JC5sDffySuXp2K1NQjaNRoE9zd2yM9/S+kp/8FN7dH4ezcwGJ+Wu1tnDnTBVlZ1xAaugTVq79U6HKvX38N//67GN7e/eHu3gn29j7w9X0aKpUDMjIu4ezZntDpkuHq2hahoR/BxaUJAMBgyMmrn/qe65aRcQEnT7aCEDo0a7YXHh6dCpTJzU2EnZ1ngR/8oobnl55+HlrtTXh7F34BRG5uEk6caAqt9l80bvwzfHyevGe9zZl3AK0IcnLikJy8Hz4+AyrEQa0qEEIgMvK/UCodEBIyt0TBicEgZWPLIpsidQp3tug8bWsPTXCTnp6Oa9ekdOsjjzyCxYsX47HHHoOXlxdq1KiB2bNn49atW/jmm28ASJeCN27cGJMnT8azzz6LvXv3Ytq0adi2bVuJr5ZicEMPQggDrlx5HgBQt+4Xxh8VIQTu3PkeOl0K/PxGwN7eE8nJB3H9+n9Qq9Y7SEjYgtu3l8LRsS7c3dsjNnYNAMDRsS6ysv4GAPj5jUJS0m5jpz07O2/o9akQIjevzV1qarC397Po2BcY+Dw8PB5DZOQbUCo1cHPrAH//EUhLOwGt9jb8/IbBza2d8cdSq41BUlIEAOR1KqxusX4l/THT6VKRkXHeYt5FbbOi5imE1D/iQX9As7L+gRCGAv2JylN29r/Qam/A3b2DzepAVJk9NMHN/v378dhjjxUYHh4ejjVr1mDs2LGIiorC/v37LaaZMWMGLl68iOrVq2POnDkYO3ZsiZfJ4IZken020tNPwcWlRYlTy+npf+HEieYAgKCgyQgL+wTZ2Tdw48ZbxoBFyobMxa1bnyIn5zbU6gDk5ibl60CoQFjY5wgMfB6Rka/j5s2FxjFOTg2h16dDq40GAPj6DkONGrNw6tSjEEKLJk1+Q1TUnLyrbpagWrXJD5SCJiJ6GDw0wY0tMLgh2d9/T8bt25/Dzs4btWsvQFDQ+CLLGgxS9uTffz/G9esziiilhKNjmMVlt+ZcXJpDp0uGTpeMBg2+t2jiuHt3B5KT90OptEdAwDikpZ3ExYtD85qT1kKptENy8u/Izo6Gv/8oGAxZMBiyLe7TQURUmTG4KQaDm6onK+sfpKYega/vEGP/Ab0+G4cP+0Ovl+97pEK7djehVgcgLu4bxMf/hMDACdDpkvDvvx8jI+MsnJ0bQ632R2LiDri4tEBGxrm8JiN7uLi0QK1a/4OnZzf8++9HuH79FSgUStSsOR+RkW8AAJo23QEPj64QwgCVquCVffnl5CTA3t6bWRkiIpTu+P1Q3cSPKL87d9YjJmYV6tdfBY2mWl7fl/XQaKrD3b0dbtxYgBs3/gchtLh79zc0aPAtFAoFEhO3Q69PhUZTHWp1NaSlHUVMzFdISfkdSUl7AAB37/5isaz09NPG9/XqfQlHx3owGLJhZ+dm0fE1OHgmvLx6QQgDXFyaQKVygV6fAU/PnqUKVNRqnwfcOkREVRODG3po6fVZuHp1CnJzExAdvRBhYR8jPn4jLl0aAUC694N8iTAA3LmzFi4uzVGjxsuIi/seAODnNxyOjmFISzuKqKh5AASUSid4e/dHQsJPUCjsERIyB3p9BqKj3wEgXd3k4tI87x4WLvmrlbfsRsb3RV1JREREZYPBDT009Pps3LjxFtzc2sHHpz/i4r4z3mslNvZr1Kr1FiIj/2ssn5HxF5RKR9Stuxx6fRquXp2CqKg58PF5ComJ2wAAfn4j4ehYG9euvQSDIRsA0LDhevj4PIHs7JtQKjVQq/2g06Xj9u3l0Onuwt29S7E35yIiItuqOBewE+UjhPQgOll09AJERy/ApUvPQK/PwL//LjGO0+tTcPZs77zb7fshLOxzeHv3xyOP/IGAgDEICpoEV9dWMBiy8ddfj8NgyIazcxO4uDSHnZ07fH2lm2cFBk6Aj490nxkHh2Co1X4AADs7F9Sq9T8ACgQEjC2vTUBERPeBHYqpwrpwYTji49dDrQ6Ep2dP3LnzA4SQbu7m7d0fd+/+ApXKFdWqTUF09ALjdHXrLjc+msBcXNz3uHRplPFz48Zb4ePTHwCg06UgKWkvvL2fKPaGWAZDrk1vP05EVFXxaqliMLh5OCQkbMX5808VGC51zjU9bbdmzbdQrdpknD3bG0qlA6pVmwxf36GFdtw1GHLx55+1kJNzC+7undC8+QFeiURE9JDg1VL0UNPp0nD16jQAQPXqM+Ht3RcxMSuRnR2F0NAlOH26Y96zksIQHPwKVCoHtGx57J7zVSrtERq6BDdvLkRY2FIGNkRElRSDG7K5hISfcffudtSq9Q5UKkecO9cPWu0NaDQ1UKvWW1CpnOHp2c1YPjBwAmJivkLdustL/dA6P78h8PMbYu1VICKiCoTNUmQTmZlXkJ5+Fr6+Q/DnnzWg1f4LtToI9va+yMj4CyqVO5o3j4Cra8sC00oPGcwyPnWZiIgqPzZLUYWTlRUFe3tv2Nm5wmDQ4ezZ3sjOjkK9equg1f4LAMjJuY2cnNuws/NCkybbCg1sAOkJ8gxsiIioKAxuqMylpBzG6dMdoVQ6ITBwHFxdWyM7OwoAEBU1FwDyHmHQDM7OTVG79gLjJdhERESlxeCGylxCwmYAAgZDBm7d+gzmt1eSszZBQS8gLOxT21SQiIgqFd7Ej6zCYMiFTpdiMUynS4UQBiQnHwAABAa+AKXSCYABCoUaCoXpeUxubh3Ks7pERFSJMbghqzh3ri8OHfJDZOQc6PVZSE09hoMHvXD5cjjS0k4BAEJCXkeTJr9CrQ5CcPAr8PDobJze3b2jrapORESVDJul6IHpdKnGJ2nfuPE/5OTEQql0BqBHXNx3AAAHh5pwcAiGg0Mw2re/BQC4efMjJCXtgUZTAw4O1W1VfSIiqmQY3NADS021vIFeXNwP0GgCLYa5u3cpMF1AQDiSkvbA1/fpMq0fERFVLQxu6L4JYYDBoEVq6p8AAF/fYUhNPQKtNhpZWdcAKABIt1Eyb4KS2dt7oWnTbeVYYyIiqgoY3NB9SUj4BVevToXBkAG1uhoAwN29PTSaQOPTul1dW8PFpVneAyn727C2RERUlTC4oVKLj9+ECxcGGz/n5iYAANzc2sHFpYUxuPH07IHatf9niyoSEVEVxqulqFh6fSbOnx+IkyfbQKdLgxACN268AwDw8upnLKdUOsDFpVle9kbqHOzt3dcmdSZ6aEVEANHRtq4F0UOPwQ0VSa/PxvnzA5CQsAVpaccRF/cdUlL+QHr6KSiVDqhffw38/EYBAFxcWkKpVEOhUKJJk+1o1OgnuLu3t/EaEJWh5GRAry95+bt3iy9/8iTQvTswcKBpmE4nLedhk5AAVK3HFlIFw+CGihQbuxpJSbuNn2/fXobo6PcAAP7+Y6BW+yA09EMEBIxD7doLjOVcXJrA13dQudeXqNwcOQL4+AD//W/Jyu/dCwQFAc8/X3SZ06elv6dOAdevS+9nzAB8fYETJx6svuVp/Xqpzp99ZuuaUBXG4IaKlJy8HwBQvfoMKJUOyMg4h8TE3wCoUL36dACAWu2P+vVXwcOjk83qSSUUFwe0aQN8Wgkec3HrFtCjh5TlMBhKP/3kyUC/fkB2dvHl/v4baN4cWLvWcviPP0pZmB07Cp/uvfeAjh2B+HhAqwUmTgRycoBffik6oyEHNICp3A8/SNmbn34CVqwAHnkEuHatZOu4YQPQrBlw/nzJyhclIgJo1Ag4fLhk5VeskP6uW3fvstOnA088IW2bkoqKAlq2BFavLvk0lc3vvwMNGgC7d9+7bH4nTwI1awJeXkCHDkBGhjT82jVpf/nhB6tW1WZEFZOSkiIAiJSUFFtXpcI7fLiG2LcPIjExQly8GC727YPYt08hYmK+tnXVKr6PPxbiww+FMBgKjrt9W4iJE4W4ePH+5//jj0LMnSuETld8uexsIV55RYhDh4T47DMhACG8ve893YM4dkyIF18U4vp1IXbvltb16lXrzHvRIiEGDBDC319aF0CI48elcadPCzFlihB//138PBITTdN++60QX38txLx5QmRkFCw7dapUrn59y+EtWkjDHR2F0OsLTifX7623hHjnHdPyACEiI6Uy27cLMWiQ9Nq+XYihQ01lHntMiAsXTJ/btROienXp/fjxBZen1wvx9ttCbNxoGtapk1T+6aeFOHVKiBdeECIuTojz56X/z507lvM4dEiICROEOHu28HUdO7b47SqEEMnJQtjZSeXt7QvfprKEBNP6HThw73nLXntNmqZp05JPcz+OHBFi5kwhMjOlz/I2/uWXoqeRy3z3nen9ypWlW+7ly9L/6vRpad+eMkWIf/6xLDN2rLQNnn++dPMWQoiBAy33xxUrpOFz5hS+r1cgpTl+M7ghCwaDTsTHbxEZGVfyghmlyM1NExkZV8Xp011FbOx3tq5ixffbb6YfjmPHCo6Xf5i6di3Z/LKyhPj+eyFiY6XPiYlCODhI81i/Xhqm1Qrx009CxMdbTvv111K5Zs1MywWE+OOP+169e3rqKdNylErpr6enEPv2SeP37bM8gN65I61fenrx871yxfJHWX7NmSOtu6OjaV1zc6VpTpyQDlLmdu82TVurlul9y5ZC3LplKmcwCBESYhovB03Jyab1AoSIirKcf3KyaVxAgKle8v9s40YhcnIsA7SwMGn58meVSoh33y18fZ2dpcD4s8+E+OgjKVjZtUsa5+IirbtOJ5UDpGAjOFh6P2mSEB06SO//8x9TndeskYIReR5vvy3EV18JsXevabmNG9/7f79unWVd9+4tuuzPP5vKffihNCw3V9oXzAOvS5ek9ZM1aGDaRnLgURZatZKWs3Sp9FnexkFBBcvu2SMFo1u3mrb5mjXSe4VCiKNHhdi/X4hz5+693IkTTdtFo5H+tmljGUQ3biwN79/fctrMTCG+/FLaL/bsKTjvW7ek7QYIMWaM9LdVK2lcnz4F9/Wi/P23EEuWCPHJJ0LcuHHvdbISBjfFYHBTvMjIt8S+fRAHD/qLffsgjh1rZusqlZxOJ2VFhJB+JGNiii6blGQ6q0xIKPrAajAUDBiKk5UlRJ06ph+J55+Xsify/paUZDrYAdJZ2r//mn64rl+XztjkZd6+Lf2wAUI88YQ0bMkS0/TdukkHgs6dTWez8oFdCOkAJv/Amh+oX31V2j5abcnXrTh6vWl7m6+/fIAHhHB1FWLtWlOwk54uBR/VqpmCEjlQSE6WtsPff5uyXx98IJVr0UI6M166VPocHGzKFsivN98U4n//k9ZbPrikpQlx964Q771XeNAASNkOeXlnz1qOkw/A27dbDt+1S9qH5O1+7FjB+XbpImVcACFmzRJiwwbpvbe3qYyTkym4AIRwcyu6nvIBChAiMFA60zcPqC9eLHw6OcAChKhbV6rvnTum7Sf/rwpbjlJpmYkxGKSDpby9rl61DGwBIebPL7ivZGRI3wN53wSEGDFCGjdrlvT50UelfSo+XggvL2nYqVNCXLtmOf/8gWt+ublC/PWXtC+dPm0ZvMpyckwnDrLUVFMAO2yYNOyNN0zLTUszlZX/3+7upsARsPyee3qa/sdyMKDXC3Hzpmlbyr9dcsYt/+vLL03bT65b69bStHFx0rj//tdymunTLX8P3npLGt6hg/R/V6ulzydPCuHnV3Bfz8w0BZparel3qXlzU9mGDaVtWA4Y3BSDwY1Ep0sXN29+LLKzTQFAbm6K+OMPj7yMjfS6cmWiDWtZSs88Ix3Idu6UmhMUCqn5Jr9bt6QfzObNpR8Ud3fpR/3PPy3LpaQI0a+f9AX+7LOS1eHNN6Xy8lmzk5N0AAsIkJb7ySeWPz41akh/X39diB9+MA13dZWCniZNTMMUCqlJQz5zlV/yWbn8WrLEVJ9evQr/oXRzk+Y3aND9bm2TmBip2QSQzk7lA+STT0pnr1lZ0o9w/jq88YbpQC6/fH2FeP99y+FffCEtRw7gPvlE+hwfb5lB6dvXFPDkfzVoIM3by0tq8gFMB3QPDyEOHjQdjL75Rpr///4nfZYzGl26SMPlA7D5gVmlkvY5IaSmLvPxdnZSduWLL6TPPXoI0b276f9unj0CpCbNov6/chMRIJ29ywd+8yDkgw+kdTBfR/N90vx1+bIpIxEaKh3A3ntPWqegIMt1AKSmKyGkA+ywYdKwgQOFGDnScr6jRkl/H3/ccl8xGKQA3NNTiNq1LQOtS5dM2xqQmnOee870ec4cKSNhvpxPPy1+33z6acvySqUU5JjX56mnpO13+LBp+I4dpmkCA6Vy8v4HWGYex40rfJ8rarvL3zn5tyI8XDpxkfc9eX946inpu/zhh6bfhJ07pf+B+e+H/Lsxe7YpQDEPkHr1koJJnc60L8n7+PDh0mfzrA0gZZUNBul7bW8vZfJq15YySdevm/Y3V1fp78KFxf8frITBTTEY3EiuXJmUF7xMEjk5ieL69Vni3LnBFoHNvn0QMTFryr9yf/whnQ3MnWsa9tln0g+7fHYjW7pUiHr1pIOE/MVs1850hurvL2UBzMk/KvKPr/kPn5ub6WV+luvsLJ1l6fVSwOPmJs37119N871+3TTNDz8UDEKefFI6gMgHYvNxHh6msyH5gC3Xzc3NFBx07Giqj/mPbZ06pr4Irq6mfijmByhAOqjkz3Jcv25ahxs3pHrMnl2y/1ViomVG6O23TfU3Pxs+flwKpgr78X/0USldb342CJgCnHr1pMyI/IMq91kRwnSm7OAg9UvQ66Uf7KAgaZu8917RGZCPPpKWKfdTkZuB/PykM3M5Y2YezLi5mbZfYKDl/Hx9pQOCfIbfr5+0bh9/LM3/5EnL8gqFlKkyPwD7+0vr0Lat9Nne3nRwUyikprnOnaW+MZmZlvu9+X720kvS+xdekOoxebIUNMpl5IPgokWm+Q8ebPm/vX1bOuiNHm06+H78sfQ9MG9Ck18qlbTdBw40Zb0cHS2zgydOFB0EyAGyr69pfubjmzUz7fNyk154uOk73b69lJkzJ6+nn59pf3rtNakfVIMGpu8MIP0fZPkzIOfOmZqIACG2bJHK5c/Eyuvh7m4KMjZtkvalhQtN67RlixA+PgW3wfDhpjJylik3Vwo25N+Gnj1N5dXqgoFJYKCUSdmwwVS3hg1NTYZeXqbmvP37LaeVt6tKJdWxsP/T/PnSXycnKQCVf4/27RPi2Wel3zI3NymAtzIGN8VgcCNETs5dceCAk9i3D+LkyQ4iMvJti4Dmxo1FeeOVIjPz+r1naE0rV5rO3tRq6excpzP94Mnt37L8B+/CXtOmmcqbn72Yv+rXL3za6tWlH1VA6vB59arl+KAgKYWdkyNE797SsMcflw5yq1dLn/v2tTyw+/pKB+vGjaUfQfN0sL19wbPTyZOlDJT5sJdeEiIiQppvt27S/HQ66QdePtjnzyDIB9xhw6TlyGeIs2aZto95s4J5vxy9XjpomwdzQpiaiuRX//6mH8f8HW1fflmq7yefmP7HdnZSXwUhpGYq+UA/ZYp0oJLPeuX+Qk2aWM5T7teweHHR+9SqVdJBIf//OCnJspxWa8omzJhhWo/bt6Wsjfm0rq6mzI756/JlU8dgObVvPn/zskOGSMPNm8natZOGnTolLWPIECmYcHeXgob8oqJM+5acBfT0NAUK335rKhsfL2XVWrc2ZYc6dTJlHgprQpLJB7R27UyZOR8f6fsYECC9l/tUCSHt/3JTzIkT0vf6rbekExbzbVCzpmVw7OAgBXDmwdPQoZYZOjs7Uwa0cWPpuyfvJ59/bqpDWprl/1rOcNSqVTAgkecrB+TySYS8bc37q5n/b+V6NGokNQEpFFKfu/nzpff5vy/yfiWfBMnZRHm+cjOlg4Pl9yc7WwoyC/uNkgNh+fXGG6bpTp40BeHyMmfMsPw/mX8vnn3WdCIllw8Ls9z+cja5eXOpjvm/G/n3ZSticFMMBjdCREUtMAYyf/zhJc6dGyT27YM4erSRuHJlotDrc0Vq6glx9+6ue8/sXq5ds+wMWBSDQboyQf5iyG3BixZJ6WJ5+IsvmqaJibH8MgUGmn6UANOBUg6ShJCudAAsU8UeHtKZTHS01MfD/KXVSmls+YdOPvupV890IBw0yHRmZW8vpddliYnS3ylTpPF160o/3kJI805PtzxIDh8uNeOY98U4e1YqKwdyU6ea2tETEy2vyEpJMWWFzDuIyvOaN0/6QUpJkc4o5R/YDz+0PIsFpDPbjz6StoPcdOHnZ1qewSCtj/kPrHw2Wq1a4f9jeXvIP9SvvVawnHmmTe6nIr/yH4ANhoKZucKkp0tBqLwtQkMLL7dokeXynnpKGp6ba7lfpKRIwV/+H/QvvzQFw4VdVSNnHgYONPVf2bPHNP3o0aayaWmmA1x2tmXfCXNPPilNu2pVwSaQ/FfkZWZK+1JUlDReqTRlE82vtMpP/t7Ir8aNTVfwZGcX3rG3Rw/Td1jORphnEgApszRokGm+b74pTavVSttZzsaZf69feUXKasj1N+/03KePlIncutWUPfLykuaZlGSZtZR/Y+rUMQVTw4ZJAbs8zvwKNvPX5MnSvteokfT5k0+kz3LAbDCY+tmZS0mxzPjNny/tB+ZXxsnfvfy0Wul3J39d5LrK7/N3cM//vzP/fRJCOjGQx33+ecH9+s8/pe9Y/uZSuS9SaqppH/T2lgK6v/+Wfk+tjMFNMap6cKPXa8WhQ0EWmZqDB33Evn2wTjCTn9xH4OBB07CEhIId0My/gPPnm/onhIVZHnQ7djRNI2czmjaVvpD//itdNSMHLykpph+tDz6Q2qrlLMl//mP6wTTP7BRFPluV+xk8/bQQ27ZZftldXKQf1cLodFI7fmH73e3bph9d+exXDvTMz36uXrU8Oy5KRoYpLQ5I2ZqaNaX327aZyuXkFGxaAaRO0OZnk488YnnQv3RJOrjIWRMXFyE2b7acR+vWxdcxNVX6EbzXJenmzRiDBhV/aXFJyB1vR44sfHx8vOXBYvv2oud1546pnHyGP3Kk6QBe2BUn//4r7QfmAan5penmTbEllZRkmqd5k4Wra/Hbt2lTy//Z5ctFlzU/kXjySen/dy9yc56875lvq9hYadumpJgyV3XqSIF9YRYulMpUq2bqzCvvu3L/JUBqOpJPAl59VforXw0khGUz9CefSIFRXJwpy2r+CgqS+riYD5MD1969pd80OWDLnwUsjpxBUipNHYp1OstsknzxQH7mV/qZnwABUjPSqVOFTydnVOV+Y+bu3jU1uZ04IQ0LDzetr7yvmgeRgNQHSqbXS//P/N0GrIzBTTGqenATG/ud2LcP4tChAHH4cHWLIMe8c7FVZGaa0plyKvTUKSmjMDFfR+Vnn5XKTZokfU5LM3VWk68iAaQsi/xlk7MhckdOIaQv2fvvm9LBX34plfH1NR20mjWTDkyXLklNJXI2oTjyD6h8Zvz669LwTz6RzrZfeEHqNHq/fvxROjOS1y05WTpDlZtsSmvaNNM2e+016T4i771XsKkoIkK6JHT0aOk1c6YUQOzda8quKBRSfwx5fjNnWgYAL7wgHcjNf/jkjIc1fPqp9CrsfjKldfeutD2uXSu6zIgR0jqEhBQfHJg3vZhvK0AKVovKtBRGvsJszZqST1OYM2ek79Lo0VJmrjjm/XU0mnvX94svpH20pP+HjRsLBgz5A3YhTPt6cd+f9HQpWDl50jRs8uTC5y+/5OBn6FDTNHKTb/6AJCdH6mcmfw+eeUYKGPV6KeAcPVpqdpIvX69bVyoDSM16pWEwSB2Fv//ecrh5U1xxJ1wrV0pZOjlbLL+KCgyFkAK4l18umLWRbd0q1Un+/UlKkr4n5sGSeTAPSFcsljMGN8WoysGNwWAQx4+3EPv2QURF/U/89Vdvs+Ypb2Eo7IZzxZk+XUqzF/WlMu9AWaeOZWfLWrWks8HGjaVUtJxRMb83g3kHSJXKlNqeOdOUSgekjnNFMQ+S5LP/e91Ppah1Nf9iy1cbVFTmaW7zfhelJV+mbd6ZUj6Au7pKZ//Xrkn7gPn2MW8+fNhcuiQ1sxXXTCNbtEg6y46NtWzyqFevdMtctkw66MuX9JaHo0dN9W3e3Przj4y03CcmTZI61ubvh3K/8l+qL/c3y/8y7xyfkCBlb+RO3qVlvk5ys2/+qyzvl5wtASyveCyKfKUTIHUELg/m/QPlixbKEYObYlSl4CY19ZRISTHtgElJB8S+fRAHDjgIrTZeXL06wxjcnD7dtWQz/fprKd18/rxpJy/qRl3yDeTk18WLpj4HCkXBzqhubgXvu/LVV9KBdciQojv95r9HRX6zZknLe+ON+z/7l68KkF9Hj97ffMrT4MGmS8rvl9yWXtjrt98sy5rfI+Wttx6s7g8j887YJWnqtDW93vQ/e+YZ68/fYLBsOikuW3a/5IDG2Vm634y9veXtEwDTHXitQaezDGJbtSr8LuT3w7zpt7i7IMvMT7jatLFOHe7FvFnPBsfQ0hy/+WypSkqnS8eZM51x+nQ7ZGRcBgBER0sPt/T3D4da7QMnp/rG8s7OTSxncOyY6bk5f/0F7NoF3LkjPfjvvfek58HI4uMtp71yRZo2/zNtNm4Ejh6V3gsBbNtmOb5PH0Ctthz23HPS/NevB5qY1dHOTvrbtCng71/stsC770pPVn77bUB5n7t848aWn+vWvb/5lKd166QnUVerdv/zaNXK9F6jMW33mjWBnj0ty4aEmN4HBd3/Mh9WmzYB//wD3LgBLFli69rcm1IJPP209L5DB+vPX6Ew7T9BQUDt2tZfxtSp0t+ePYHWrYGkJGD/fssy1lyuSiU960v2ySfSelqD+W9MSeocEGB6b/7dK0tyHQMCADe38lnmfbKzdQWobKSm/gm9Ph0AEBn5X9SoMRuJiTsAqFCjxqsAkC+4ydtphQAWLwZefVV6f+6c9IDC+Higd28gN1cqFxVlWlhMjOXCBw0CLl40BR1NmwJnzwKLFkkPEZQdOGA53ZNPFr4yrq7S38aNpYcBAsA770gPgSzJj4BC8eBfxIYNTe/9/AAPjwebX3mws8LXu2VL0/tGjaQA58gRYMKEgoFiSIgpeK2KwY1SCdSqZetalM7ChdKJSrduZTP/Nm2AnTuBLl2sFwSYGz5cOtDKB11nZ+lVu7YUaAJAnTrWXWbHjsDBg0CnTkC7dtabb9Om0l87u5LtR+YndeUV3DRrJv01/z2soBjcVFIpKX8Y3yckbEZGhpRF8fcfCUdHKSAokLmRn168Zo1pRi+/bMrMyJkcV1cgLc1U5vZt0/u7d6XABpCeQg0ACxYAzz5r+iyTn+b8wQfSk2mHDi1+peTMjb09MG4c4OtbfHlrcnEx/WDWq1d+y7U18+CmSRPgpZeArVuBGTMKlq3qmZuHkYNDwQycNc2cKT09/YUXym4ZXbsWHNaqlfRdtbcHqle37vKWLpWe0v6f/1h3vkFBwPLlgJMT4Oh47/K2yNwMHw5cvQoMHlw+y3sADG4qKTm4sbf3R25uHLKyrgJQokaN/xrL2Nv7wsXlEeTkxMHFpRkwaKh04FIqgccfB/bsMQU0Mnd3ICICmDZNCoZOnrQMbk6eLFiZ9u2lrM2YMdLnoCDTNAoF8OKL0hf6Xnr2BPr2lepWnoGNrHHjqhfc+PsDwcHAzZvS+j/yiPQqDIMbys/DQ8qylrdWrYAff5SaT1Uq6867aVNTlsXaShME2iK4cXKSTlYfAuxzUwkZDDlITf0TANCkyVaEhX2OOnU+RLNme+DsbMrWKBQKtGhxFG3bXoMqOUsKbABg+3Zg5UrLmYaHSz8S06dLZ/OHDknvASlQ2bgRmD0b+PNPy+mCg6UfuNGjpfS3i4tpOkBKGZcksAGkdPO2bdY/YyqpJ56QgrGyPNOtiEaMkJqj+vUrvpz8A2tnB3h7l329iIrSt6/Uf69HD1vXpOzYolnqIaIQQghbV6I8paamwt3dHSkpKXCr4B2i7ldKyp84fbod7Oy80aFDPBQKhRSMaLVS1qMwu3dLB+06dYBr16RhjzwCnDkj/UjcvStldBwdTW3ne/dKbfX160tNV3fvSn1bUlOltvZjx6Q05g8/SOX1eqkz3uXLQPPm0rABA4DNm8twa1hZRoYUZFUlQkhNiPc6A46MlPafxo2lPlZEtpSRIZ04lUVfn4pAp5MCHK0WiI2VThwrudIcv9ksVQnJTVLu7h2lwEankzoDa7VSvxdPT/PC0vgTJ6TP5lfHPPmkFNw8/njhXxy56eHqVSlwAaTABgDef186GDZqZCqvUkmvsDDTsPxXIVV0VS2wAaSDQ0lS+7VqSUE0m6SoIqjs31U7O6ljc25ulQhsSovBTSWUknIQAODh0UkacOcOkC5dOYVLl6Q+MACQkyMFM0lJpt7v5sHNK69IZ+1yX5n8AgOlv3JgY65Fi6KvUHJyAmrUAKKjH77ghopnzatHiKh4DRrYugYVFoObSkYIgzG4cXfvKA2MjTUV+Ptv6d4MsbFSPxq5CeqPvKurzK+OcXEB3nqr6IW5uUmBSmam5fC6de996fUrr0j3BenduwRrRUREVHLsUFzJZGZegk6XCKXSCS4uLaSB5pdg79sn3RDvwAHpsu/8WrQo+cIUCssmiF69pI6n97qkGwCmTJH67Li7l3x5REREJcDMTSWTnCxlYNzcHoVSaS8NNM/cbNpkep+TIwUoTk5S57u6dUsfbAQFmbI/48cDv/wi3VuCiIjIRpi5qWRMnYk7mQaaBzdy3xtZnz5S8xQgXeFUWuaZm8aNGdgQEZHNMXNTCaSmnoBCoYKr6yPG4MbYmRiwDG5kc+dKV0mNHy9dPeXrC4wdW/qFy52KNRrr3+aciIjoPjC4ecjlZMZA17MtDPaA7ufd0GpvQqFQw9W1ralQYcFNnz7Ao4+aPs+ff38VkDM3DRta51lGRERED4hHo4dc6q5P4XNUekbTmZ0jgUDAy6sP7OzM7nuQ/5lOKpXpAWgPqnNn6SZ/Tz1lnfkRERE9IPa5ecgkJu7C0aP1kZoqPX1ZbN1oHOd4PA6eJ4BaazXSs5wSE6URcubGx0f626hRyR7MVhJt2kg3Apw3zzrzIyIiekA2D26WLl2KmjVrwsHBAW3btsWxY8eKLJubm4u33noLderUgYODA5o1a4Yd+R/sWMnFxq5BVtYVxPz1PvTxt+Eccc04zj8CaDoLcHn/R+DVV6UncUsTSX+7d5f+tm0Lq3JwsO78iIiIHoBNg5v169dj5syZmDdvHk6dOoVmzZqhV69euHPnTqHl33jjDXzxxRf49NNPcfHiRUycOBEDBw7E6dOny7nmtpOV9Q+UOUDt/lugrFELTtGmR4N5nAUUegChoVL/l59/lh5omZIiFXjvPemmfG+/bZvKExERlQObPjizbdu2aN26NT777DMAgMFgQHBwMKZOnYpZs2YVKB8UFITXX38dkydPNg4bPHgwHB0d8d1335VomQ/7gzMPHfKDXWQ82j5jGpb1SAAcziVAodNJA9atA06dAhYulB52aTBIVzNlZVXeh8gREVGlVprjt80yNzk5OTh58iS6y00lAJRKJbp3744jR44UOo1Wq4VDviYQR0dHHDx4sMjlaLVapKamWrweVjpdOnJz46FONA0TSkDx0gwoWreWBvj6AgMHAnPmAH5+UmADAAEBDGyIiKhKsFlwk5CQAL1eD39/f4vh/v7+iC3s0mUAvXr1wuLFi3H16lUYDAbs3r0bmzZtQkxMTJHLWbBgAdzd3Y2v4OBgq65HecrOjgQAY3CT3AS4cKwfHMJfBZ54Qhr44ovS1UsuLqY+N4AU3BAREVUBNu9QXBoff/wxwsLCUL9+fajVakyZMgXjxo2DUln0asyePRspKSnG182bN8uxxtaVnf0PAMAh2QkAkOMF1AjLu0rplVek50XNnWuaYPx40/uMjPKqJhERkU3ZLLjx8fGBSqVCXL57sMTFxSGgiCyDr68vtmzZgoyMDNy4cQOXL1+Gi4sLateuXeRyNBoN3NzcLF4Pq6wsKXPjllkDAOBUqzPc3PKao+ztpXvOqFSmCcy3i7NzeVWTiIjIpmwW3KjVarRs2RIRERHGYQaDAREREWjXrl2x0zo4OKBatWrQ6XT46aef8FQVuYGcnLnRJKsBAC6hPe890V9/AT17AkuXlmXViIiIKgyb3qF45syZCA8PR6tWrdCmTRssWbIEGRkZGDduHABgzJgxqFatGhYsWAAAOHr0KG7duoXmzZvj1q1bmD9/PgwGA1599VVbrka5EMKArCwpuFEn5l3glq+/UqGaNgV27izDmhEREVUsNg1uhg0bhvj4eMydOxexsbFo3rw5duzYYexkHB0dbdGfJjs7G2+88Qb++ecfuLi4oG/fvvj222/h4eFhozUoH7GxX+Pq1SnQ66UnetslZEsj2EmYiIioAJve58YWKvR9bnQ64OpVoH59i8u2Tx/vCN35Q8gIAaAEuozyh+J2HHD8ONCqle3qS0REVE4eivvcUCHmz5eerr1hg2nYnTuo8/wJtH4WCPwNUAg1EJ93LTgzN0RERAUwuKlITpyQ/p47J/3NyYHo3BFuZ7UAgOAzDdC42koocnOl8X5+NqgkERFRxWbTPjeUz7//Sn/j46W/Z89CceWqcbTTqTg4aZtJH7y8pJv1ERERkQVmbiqS/MHN+fMAgOSmgN5BCSQmAnv3SuPYJEVERFQoBjcVRVqa6endCQnS37zgJj0U0LaoJg2T++MwuCEiIioUg5uK4tYt03s5c5PX9yajFqBr31waduiQ9JfBDRERUaEY3FQUcpMUUKBZKqMWoOjaw7I8gxsiIqJCMbipKMyDm8REqWnq9m0AQEZNwKHLEOn+N4DUkbhbt/KvIxER0UOAV0tVFObBjcEA/PEHACDbH3Dwawx7l0Apk5OWBmg0gKOjjSpKRERUsTG4qShu3rT4KPZGQAGpSapatZekgSoVUMkfNUFERPSg2CxVUZhnbgDo92wFAGTVcYS//yhb1IiIiOihxOCmosgX3NhdljI56pY9oVKxCYqIiKikGNxUFHnBjaFWDYvBru2es0VtiIiIHloMbiqCzEzpCikAd4OjjYOFCnBs3qOoqYiIiKgQDG4qgh07AAA6VwWyqpkG62r5AQ4ONqoUERHRw4lXS9laRgYwYwYA4FZ/gVx30yhlszY2qhQREdHDi5kbW1uyBIiOhi7IHTdGA+pqTY2jVE1b265eREREDykGN7aW96yoW6PcYHAEXEIeM41r3NhGlSIiInp4MbixtbyrpJJ9bgJQwLV2b9M4BjdERESlxj43tpYX3Gh9AQeHENiHNJOGu7oCderYsGJEREQPJwY3tpSRASQlAZCCGxdNCBAYCHz/PeDnJz1ugYiIiEqFwY0t3boFADA4q6F3zoGDQ7A0fMQIG1aKiIjo4cY+N7aU1ySVG+AMKACNpsY9JiAiIqJ7YXBjS3nBTY6f1Pzk4MDghoiI6EExuLGlm9LDMbN99AAAjSbYlrUhIiKqFBjc2FJe5ibLKwsAm6WIiIisgcGNLcnBjXc2ADZLERERWQODG1syu8eNSuUGOzs3G1eIiIjo4cdLwW3h2jXgxx+BM2cAAFo/Zm2IiIishcGNLUybBvz2m/Gj1hdwY38bIiIiq2CzVHlLSwMiIowfhZ0SOhdeKUVERGQtzNyUt127gJwcoGZNoFs33Ak4DyiOmu5OTERERA+EmZvy9ssv0t9Bg4CvvkLMECcAgINDTdvViYiIqBJhcFOecnOBbduk9/37AwCysyMBMLghIiKyFgY35SUpCejTB0hIALy9gQ4dYDDokJ0t3aWYwQ0REZF1MLgpL9OnSx2JnZ2Bb74B7O2Rk3MLgB4KhT3U6kBb15CIiKhSYIfi8pCTA2zeLL3/+WegWzcAQHZ2FADAwSEECgXjTCIiImvgEbU8/P67dAm4vz/w2GPGwabgpqZt6kVERFQJMbgpD/IVUk88AShNm5zBDRERkfUxuClrQgBbt0rv866QkjG4ISIisj4GN2Xt6lUgKgrQaIDu3S1GMbghIiKyPgY3Ze3yZelv48bSlVJmGNwQERFZH4ObsvbPP9Lf2rUtBvMeN0RERGWDwU1Zu35d+lunjsVgrTYavMcNERGR9TG4KWtFZG6SknYDAFxdW/IeN0RERFbEo2pZKyK4SUiQrqDy9n6yvGtERERUqTG4KUsGAxApPRjTvFlKp0tHUlIEAMDHh8ENERGRNTG4KUu3bwNaLWBnB1SvbhyclLQbQmjh4FAbTk4NbVhBIiKiyofBTVmSm6RCQqQAB4AQety69RkAKWujUChsVTsiIqJKicFNWSrkSql//vkvkpP3Qql0RGDgeBtVjIiIqPKyeXCzdOlS1KxZEw4ODmjbti2OHTtWbPklS5agXr16cHR0RHBwMGbMmIHs7Oxyqm0p5etMnJ0djZs3FwIA6tVbBWdnNkkRERFZm02Dm/Xr12PmzJmYN28eTp06hWbNmqFXr164c+dOoeW///57zJo1C/PmzcOlS5ewcuVKrF+/Hv/973/LueYllC+4ycqSPjs61oW//3Bb1YqIiKhSs2lws3jxYowfPx7jxo1Dw4YNsXz5cjg5OWHVqlWFlj98+DA6dOiAkSNHombNmujZsydGjBhxz2yPzSQkSH/9/QEAublS0KZW+9uqRkRERJWezYKbnJwcnDx5Et3NHiapVCrRvXt3HDlypNBp2rdvj5MnTxqDmX/++Qfbt29H3759y6XOpZaZKf3Ne6ZUTo4U3Njb+9mqRkRERJWena0WnJCQAL1eD39/yyyGv78/LssPm8xn5MiRSEhIQMeOHSGEgE6nw8SJE4ttltJqtdBqtcbPqamp1lmBkpCDGycnAOaZGwY3REREZcXmHYpLY//+/Xj33Xfx+eef49SpU9i0aRO2bduGt99+u8hpFixYAHd3d+MrODi4/CqckSH9NQY38QCYuSEiIipLNsvc+Pj4QKVSIS4uzmJ4XFwcAgICCp1mzpw5eOaZZ/D8888DAJo0aYKMjAxMmDABr7/+OpTKgrHa7NmzMXPmTOPn1NTU8gtwimiWYuaGiIio7Ngsc6NWq9GyZUtEREQYhxkMBkRERKBdu3aFTpOZmVkggFGpVAAAIUSh02g0Gri5uVm8yk0RzVLM3BAREZUdm2VuAGDmzJkIDw9Hq1at0KZNGyxZsgQZGRkYN24cAGDMmDGoVq0aFixYAADo378/Fi9ejEceeQRt27bFtWvXMGfOHPTv398Y5FQo+ZqlTJkbX1vViIiIqNKzaXAzbNgwxMfHY+7cuYiNjUXz5s2xY8cOYyfj6Ohoi0zNG2+8AYVCgTfeeAO3bt2Cr68v+vfvj3feecdWq1A0gwGQby6Y1yzFzA0REVHZU4ii2nMqqdTUVLi7uyMlJaVsm6gyMgAXF+l9ejoMjvb4/XcNAKBDhwTY23uX3bKJiIgqmdIcvx+qq6UeKnKTFAA4OiI3N++GflDBzs7TJlUiIiKqChjclBW5M7GDA6BUWvS3USi42YmIiMoKj7JlJd9l4OxvQ0REVD4Y3JSVfJeB8x43RERE5YPBTVkpcHdiZm6IiIjKA4ObssK7ExMREdkEg5uywrsTExER2QSDm7JS4O7EsQAAtdq/qCmIiIjIChjclJV8zVLZ2TcAABpNDVvViIiIqEpgcFNWzJqlhBDQaqMBAA4ODG6IiIjKEoObsmIW3Oh0SdDr0wEAGk2wDStFRERU+TG4KStynxtnZ2RnS1kbe3s/qFSONqwUERFR5cfgpqyYZW60Wqm/DZukiIiIyh6Dm7JiFtzImRuNJsSGFSIiIqoaGNyUFYtmKWZuiIiIyotdSQqlpqbCzc3N+L44crkqz6JZSs7cMLghIiIqayUKbjw9PRETEwM/Pz94eHhAoVAUKCOEgEKhgF6vt3olH0oWzVJy5obNUkRERGWtRMHN3r174eXlBQDYt29fmVao0jC7iR/vcUNERFR+ShTcdOnSpdD3VIy8Pjd6jZ3x0QtsliIiIip7pe5QvGPHDhw8eND4eenSpWjevDlGjhyJpKQkq1buoZaXucm1l27ep1Q6wt7ex5Y1IiIiqhJKHdy88sorxk7F586dw8yZM9G3b19ERkZi5syZVq/gQ0sObtRZAAC1OqDQvkpERERkXSVqljIXGRmJhg0bAgB++ukn9O/fH++++y5OnTqFvn37Wr2CD628ZimdWgtoAXt7bxtXiIiIqGoodeZGrVYjMy8rsWfPHvTs2RMA4OXldc/LxKsUY7NUNgDAzs7LlrUhIiKqMkqduenYsSNmzpyJDh064NixY1i/fj0A4O+//0b16tWtXsGHkl4PaLUAgFy1FOTY2zO4ISIiKg+lztx89tlnsLOzw8aNG7Fs2TJUq1YNAPDbb7+hd+/eVq/gQykry/hW7lBsZ+dpq9oQERFVKaXO3NSoUQO//vprgeEfffSRVSpUKciPXlAokKOUmurYLEVERFQ+Sh3cmMvOzkZOTo7FMD5+ARZ3J9bpkwGwWYqIiKi8lLpZKiMjA1OmTIGfnx+cnZ3h6elp8SJYBje6RADM3BAREZWXUgc3r776Kvbu3Ytly5ZBo9Hgq6++wptvvomgoCB88803ZVHHh0+61M8Gzs7IzZWCG2ZuiIiIykepm6V++eUXfPPNN+jatSvGjRuHTp06ITQ0FCEhIVi7di1GjRpVFvV8uKSkSH/d3Zm5ISIiKmelztwkJiaidu3aAKT+NYmJ0sG7Y8eO+P33361bu4dVcrL018ODmRsiIqJyVurgpnbt2oiMjAQA1K9fHz/++CMAKaPj4eFh1co9tPIyN8LdDTpdMgBeCk5ERFReSh3cjBs3Dn/99RcAYNasWVi6dCkcHBwwY8YMvPLKK1av4EMpL3MjXJ0AGAAwuCEiIiovpe5zM2PGDOP77t274/Llyzh58iRCQ0PRtGlTq1buoZWXuTG4aQAASqUTVCoHW9aIiIioyihV5iY3NxfdunXD1atXjcNCQkIwaNAgBjbm8jI3ehcpdmR/GyIiovJTquDG3t4eZ8+eLau6VB55mRu9qwoAr5QiIiIqT6XuczN69GisXLmyLOpSeeRlbnTOCgDM3BAREZWnUve50el0WLVqFfbs2YOWLVvC2dnZYvzixYutVrmHVl7mJteZnYmJiIjKW6mDm/Pnz6NFixYAgL///ttinEKhsE6tHnbGzI0OAJuliIiIylOpg5t9+/aVRT0ql7zMTY6T9FBRNksRERGVn1L3uaESyMvc5DhmAWDmhoiIqDyVOnPz2GOPFdv8tHfv3geq0EPPYADS0gAA2rzgxt6efW6IiIjKS6mDm+bNm1t8zs3NxZkzZ3D+/HmEh4dbq14Pr9RUQAgAQK5TJpAJ2Nl52LZOREREVUipg5uPPvqo0OHz589Henr6A1fooSc/EdzBAblKaXuoVG42rBAREVHVYrU+N6NHj8aqVausNbuHl/xEcHd36PWpAAA7OwY3RERE5cVqwc2RI0fg4MDnJxkzNx4e0Omk4IaZGyIiovJT6mapQYMGWXwWQiAmJgYnTpzAnDlzrFaxh5ZF5uYGAGZuiIiIylOpgxt3d3eLz0qlEvXq1cNbb72Fnj17Wq1iD628zI1wd4PBkA0AUKnci5uCiIiIrKjUwc3HH38MN7fCMxHXrl1DaGjoA1fqoZaXuTG4ORkH2dm52qgyREREVU+p+9z069cPWq22wPArV66ga9eu1qjTw03O3Lg5AgCUSmcoFCpb1oiIiKhKKXVw4+LigoEDB0Kn0xmHXbp0CV27dsXgwYOtWrmHkpy5cdUAYH8bIiKi8lbq4GbTpk1ISUnBqFGjIITA+fPn0bVrV4wYMQIff/zxfVVi6dKlqFmzJhwcHNC2bVscO3asyLJdu3aFQqEo8OrXr999Ldvq8jI3elc1AF4pRUREVN5KHdw4Ojpi27ZtuHLlCoYOHYpu3bphzJgxWLx48X1VYP369Zg5cybmzZuHU6dOoVmzZujVqxfu3LlTaPlNmzYhJibG+Dp//jxUKhWefvrp+1q+1eVlbvQuUlOUnR07ExMREZWnEgU3qampFi+lUon169fj6NGjGDx4MObMmWMcV1qLFy/G+PHjMW7cODRs2BDLly+Hk5NTkTcE9PLyQkBAgPG1e/duODk5VZzgJu8uzXpH6REMbJYiIiIqXyW6WsrDw6PQh2UKIbB8+XJ88cUXEEJAoVBAr9eXeOE5OTk4efIkZs+ebRymVCrRvXt3HDlypETzWLlyJYYPHw5nZ+cSL7dM5a2/XiF1umazFBERUfkqUXCzb9++Es3s3LlzpVp4QkIC9Ho9/P39LYb7+/vj8uXL95z+2LFjOH/+PFauXFlkGa1Wa3F11/1kl0olL7gx5AU3zNwQERGVrxIFN126dClyXFpaGn744Qd89dVXOHnyJKZMmWK1yt3LypUr0aRJE7Rp06bIMgsWLMCbb75ZbnUyZm4g38CPwQ0REVF5uu9nS/3+++8IDw9HYGAgPvjgAzz++OP4888/SzUPHx8fqFQqxMXFWQyPi4tDQEBAsdNmZGRg3bp1eO6554otN3v2bKSkpBhfN2/eLFUdS82YuZGCG3YoJiIiKl+lukNxbGws1qxZg5UrVyI1NRVDhw6FVqvFli1b0LBhw1IvXK1Wo2XLloiIiMCAAQMAAAaDAREREffMAG3YsAFarRajR48utpxGo4FGoyl13e5bXnCjE1kAmLkhIiIqbyXO3PTv3x/16tXD2bNnsWTJEty+fRuffvrpA1dg5syZWLFiBb7++mtcunQJL774IjIyMjBu3DgAwJgxYyw6HMtWrlyJAQMGwNvb+4HrYFX5mqXY54aIiKh8lThz89tvv2HatGl48cUXERYWZrUKDBs2DPHx8Zg7dy5iY2PRvHlz7Nixw9jJODo6GkqlZQx25coVHDx4ELt27bJaPazGGNwwc0NERGQLJQ5uDh48iJUrV6Jly5Zo0KABnnnmGQwfPtwqlZgyZUqRzVD79+8vMKxevXoQQlhl2VZnMAAA9CITADM3RERE5a3EzVKPPvooVqxYgZiYGLzwwgtYt24dgoKCYDAYsHv3bqSlpZVlPR8exsyNFNwwc0NERFS+Sn21lLOzM5599lkcPHgQ586dw3/+8x+899578PPzw5NPPlkWdXy45GuW4tVSRERE5eu+LwUHpOahhQsX4t9//8UPP/xgrTo93IzBTQYANksRERGVtwcKbmQqlQoDBgzA1q1brTG7h5vxPjc5ANgsRUREVN6sEtyQmbzgRuRtWZXK1YaVISIiqnoY3FibWXCjVDpBqSzVfRKJiIjoATG4sTb5qehK9rchIiKyBQY31pZ3nxuhZJMUERGRLTC4sTazZimVysXGlSEiIqp6GNxYG4MbIiIim2JwY21ynxsVoFI527YuREREVRCDG2tj5oaIiMimGNxYG4MbIiIim2JwY20W97lhsxQREVF5Y3BjbXmXgoOZGyIiIptgcGNtbJYiIiKyKQY31mYR3LBZioiIqLwxuLEmuUkKyLsUnJkbIiKi8sbgxprke9yAzVJERES2wuDGmgoEN2yWIiIiKm8MbqyJmRsiIiKbY3BjTWbBDS8FJyIisg0GN9Zk1qGYzVJERES2weDGmtgsRUREZHMMbqyJzVJEREQ2x+DGmsxu4AcFny1FRERkCwxurMk8uIEKSqXGptUhIiKqihjcWFO+50opFAobV4iIiKjqYXBjTXKfG14pRUREZDMMbqwp71JwXilFRERkOwxurClfsxQRERGVPwY31sRmKSIiIptjcGNNcuZGxcwNERGRrTC4sSY2SxEREdkcgxtrsghu2CxFRERkCwxurMmizw0zN0RERLbA4MaaeCk4ERGRzTG4sSazZik+V4qIiMg2GNxYE5uliIiIbI7BjTWxQzEREZHNMbixJrP73CiVDjauDBERUdXE4MaazDI3CoWdjStDRERUNTG4sSazPjcMboiIiGyDwY01WWRuVDauDBERUdXE4MaazO5zw8wNERGRbTC4sSY2SxEREdkcgxtrYrMUERGRzTG4sSazS8GZuSEiIrINBjfWxEvBiYiIbI7BjTWxzw0REZHNMbixJrPMDcA+N0RERLZg8+Bm6dKlqFmzJhwcHNC2bVscO3as2PLJycmYPHkyAgMDodFoULduXWzfvr2cansPvBSciIjI5mx6BF6/fj1mzpyJ5cuXo23btliyZAl69eqFK1euwM/Pr0D5nJwc9OjRA35+fti4cSOqVauGGzduwMPDo/wrXxg2SxEREdmcTY/Aixcvxvjx4zFu3DgAwPLly7Ft2zasWrUKs2bNKlB+1apVSExMxOHDh2Fvbw8AqFmzZnlWuXi8FJyIiMjmbNYslZOTg5MnT6J79+6myiiV6N69O44cOVLoNFu3bkW7du0wefJk+Pv7o3Hjxnj33XehlzMmhdBqtUhNTbV4lRleLUVERGRzNgtuEhISoNfr4e/vbzHc398fsbGxhU7zzz//YOPGjdDr9di+fTvmzJmDDz/8EP/73/+KXM6CBQvg7u5ufAUHB1t1PSzwPjdEREQ2Z/MOxaVhMBjg5+eHL7/8Ei1btsSwYcPw+uuvY/ny5UVOM3v2bKSkpBhfN2/eLLsKss8NERGRzdnsCOzj4wOVSoW4uDiL4XFxcQgICCh0msDAQNjb20OlMvVnadCgAWJjY5GTkwO1Wl1gGo1GA41GY93KF4V9boiIiGzOZpkbtVqNli1bIiIiwjjMYDAgIiIC7dq1K3SaDh064Nq1azDkXXINAH///TcCAwMLDWzKHS8FJyIisjmbNkvNnDkTK1aswNdff41Lly7hxRdfREZGhvHqqTFjxmD27NnG8i+++CISExPx0ksv4e+//8a2bdvw7rvvYvLkybZaBQtCp5PeMLghIiKyGZsegYcNG4b4+HjMnTsXsbGxaN68OXbs2GHsZBwdHQ2l0hR/BQcHY+fOnZgxYwaaNm2KatWq4aWXXsJrr71mq1WwpJeCGzZLERER2Y5CCCFsXYnylJqaCnd3d6SkpMDNzc2q8xbz3oDirXdw60nA/6dk2Nm5W3X+REREVVVpjt8P1dVSFZ3cLMVLwYmIiGyHwY016XOlv3xwJhERkc0wuLEioZOCG14tRUREZDsMbqyJHYqJiIhsjsGNFQmDfIdiBRQKhW0rQ0REVEUxuLEmuVlKxcCGiIjIVhjcWJNevokfNysREZGt8ChsRUIOblTsb0NERGQrDG6sSX78ApuliIiIbIbBjTXJV0sxc0NERGQzDG6sydgsxc1KRERkKzwKW5PxUnBuViIiIlvhUdiaeLUUERGRzfEobEVCn5e5YZ8bIiIim2FwY028FJyIiMjmGNxYU17mRrBDMRERkc3wKGxNOmZuiIiIbI3BjTWxzw0REZHNMbixJmNww81KRERkKzwKW5PBAABQKJm5ISIishUGN9ZkzNzY2bYeREREVRiDG2tinxsiIiKbY3BjTQxuiIiIbI7BjTUxuCEiIrI5BjfWxD43RERENsfgxpryrpaCHTM3REREtsLgxpp4KTgREZHNMbixJjZLERER2RyDG2vS5zVLMbghIiKyGQY3VqRgcENERGRzDG6sycDghoiIyNYY3FgTMzdEREQ2x+DGmvKCG4UdgxsiIiJbYXBjTWyWIiIisjkGN1YkdyhWMLghIiKyGQY31mQQ0l8GN0RERDbD4MaajB2K7W1bDyIioiqMwY0VKQzsUExERGRrDG6sSS83SzFzQ0REZCsMbqxIkRfcMHNDRERkOwxurMnAPjdERES2xuDGmvKullIwuCEiIrIZBjdWpOCl4ERERDbH4MaajB2K1batBxERURXG4MZahIAiL7ZR2LFZioiIyFbYfmIter3pPfvcEFEFYTAYkJOTY+tqEJWIWq2GUvngeRcGN9ZiFtwwc0NEFUFOTg4iIyNhkK/kJKrglEolatWqBbX6wbp3MLixFovghn1uiMi2hBCIiYmBSqVCcHCwVc6GicqSwWDA7du3ERMTgxo1akChUNz3vBjcWIv5mZGSmRsisi2dTofMzEwEBQXBycnJ1tUhKhFfX1/cvn0bOp0O9vb3fyxlKG8tbJYiogpEn/eb9KDpfaLyJO+vevN+rPeBwY21sFmKiCqgB0ntE5U3a+2vFSK4Wbp0KWrWrAkHBwe0bdsWx44dK7LsmjVroFAoLF4ODg7lWNsiMLghIqqQatasiSVLlpS4/P79+6FQKJCcnFxmdSqN0ta/vFTUegEVILhZv349Zs6ciXnz5uHUqVNo1qwZevXqhTt37hQ5jZubG2JiYoyvGzdulGONi5AX3AgFoFCyKxMRUWnlP3HN/5o/f/59zff48eOYMGFCicu3b98eMTExcHd3v6/l3Y/69etDo9EgNja23Jb5oEq7XcuTzYObxYsXY/z48Rg3bhwaNmyI5cuXw8nJCatWrSpyGoVCgYCAAOPL39+/HGtcBDm4UQIKBYMbIqLSMj9pXbJkSYET2ZdfftlYVggBnU5Xovn6+vqWqlO1Wq1GQEBAuTXpHTx4EFlZWRgyZAi+/vrrMl+eXq+3yu0BSrtdy5NNg5ucnBycPHkS3bt3Nw5TKpXo3r07jhw5UuR06enpCAkJQXBwMJ566ilcuHChyLJarRapqakWrzIhN0sxuCEiui/mJ63u7u4WJ7KXL1+Gq6srfvvtN7Rs2RIajQYHDx7E9evX8dRTT8Hf3x8uLi5o3bo19uzZYzHf/M0nCoUCX331FQYOHAgnJyeEhYVh69atxvH5m6XWrFkDDw8P7Ny5Ew0aNICLiwt69+6NmJgY4zQ6nQ7Tpk2Dh4cHvL298dprryE8PBwDBgy453qvXLkSI0eOxDPPPFPsib3sq6++goeHByIiIgptQjtz5gwUCgWioqIs6r9161Y0bNgQGo0G0dHROH78OHr06AEfHx+4u7ujS5cuOHXqlHE+QgjMnz8fNWrUgEajQVBQEKZNm1bkdq1IbBrcJCQkQK/XF8i8+Pv7F5maq1evHlatWoWff/4Z3333HQwGA9q3b49///230PILFiyAu7u78RUcHGz19QBgvBRcqBjcEFHFI4SAXp9hk5cQwmrrMWvWLLz33nu4dOkSmjZtivT0dPTt2xcRERE4ffo0evfujf79+yM6OrrY+bz55psYOnQozp49i759+2LUqFFITEwssnxmZiY++OADfPvtt/j9998RHR1tkUl6//33sXbtWqxevRqHDh1CamoqtmzZcs/1SUtLw4YNGzB69Gj06NEDKSkp+OOPP4osv3DhQsyaNQu7du1Ct27d7jl/8/q///77+Oqrr3DhwgX4+fkhLS0N4eHhOHjwIP7880+EhYWhb9++SEtLAwD89NNP+Oijj/DFF1/g6tWr2LJlC5o0aVLiZdrSQ3cUbteuHdq1a2f83L59ezRo0ABffPEF3n777QLlZ8+ejZkzZxo/p6amlk2AY9bnBlBZf/5ERA/AYMjEH3+42GTZnTqlQ6Vytsq83nrrLfTo0cP42cvLC82aNTN+fvvtt7F582Zs3boVU6ZMKXI+Y8eOxYgRIwAA7777Lj755BMcO3YMvXv3LrR8bm4uli9fjjp16gAApkyZgrfeess4/tNPP8Xs2bMxcOBAAMBnn32G7du333N91q1bh7CwMDRq1AgAMHz4cKxcuRKdOnUqUPa1117Dt99+iwMHDhjLl1Rubi4+//xzi231+OOPW5T58ssv4eHhgQMHDuCJJ55AdHQ0AgIC0L17d9jb26NGjRpo06ZNqZZrKzbN3Pj4+EClUiEuLs5ieFxcHAICAko0D3t7ezzyyCO4du1aoeM1Gg3c3NwsXmWCzVJERGWuVatWFp/T09Px8ssvo0GDBvDw8ICLiwsuXbp0z8xN06ZNje+dnZ3h5uZW7IUsTk5OxsAGAAIDA43lU1JSEBcXZ3HgV6lUaNmy5T3XZ9WqVRg9erTx8+jRo7FhwwZj9kT24YcfYsWKFTh48GCpAxtA6kdkvs6AdKwdP348wsLC4O7uDjc3N6Snpxu33dNPP42srCzUrl0b48ePx+bNm0vcz8nWbHoUVqvVaNmyJSIiIoztkgaDAREREcVG3Ob0ej3OnTuHvn37lmFNS1QRAGyWIqKKSal0QqdO6TZbtrU4O1tmgF5++WXs3r0bH3zwAUJDQ+Ho6IghQ4bc82Gh+e9+q1Aoiu1kW1j5B21uu3jxIv78808cO3YMr732mnG4Xq/HunXrMH78eOOwTp06Ydu2bfjxxx8xa9Ys43D5sRrmdcnNzS2wLEdHxwIdpMPDw3H37l18/PHHCAkJgUajQbt27YzbLjg4GFeuXMGePXuwe/duTJo0CYsWLcKBAwce6O7B5cHmR+GZM2ciPDwcrVq1Qps2bbBkyRJkZGRg3LhxAIAxY8agWrVqWLBgAQApJfnoo48iNDQUycnJWLRoEW7cuIHnn3/elquR72opNksRUcWiUCis1jRUkRw6dAhjx441Ngelp6cbO9KWF3d3d/j7++P48ePo3LkzAClAOXXqFJo3b17kdCtXrkTnzp2xdOlSi+GrV6/GypUrLYKbNm3aYMqUKejduzfs7OyM/X18fX0BSFeaeXp6ApA6FJfEoUOH8PnnnxuTAzdv3kRCQoJFGUdHR/Tv3x/9+/fH5MmTUb9+fZw7dw4tWrQo0TJsxebBzbBhwxAfH4+5c+ciNjYWzZs3x44dO4ydjKOjoy0e+JaUlITx48cjNjYWnp6eaNmyJQ4fPoyGDRvaahUkvBSciKjchYWFYdOmTejfvz8UCgXmzJljk6egT506FQsWLEBoaCjq16+PTz/9FElJSUVeTp6bm4tvv/0Wb731Fho3bmwx7vnnn8fixYtx4cIFiyao9u3bY/v27ejTpw/s7Owwffp0hIaGIjg4GPPnz8c777yDv//+Gx9++GGJ6hwWFoZvv/0WrVq1QmpqKl555RU4Ojoax69ZswZ6vR5t27aFk5MTvvvuOzg6OiIkJOQ+tlD5svl9bgCpY9aNGzeg1Wpx9OhRtG3b1jhu//79WLNmjfHzRx99ZCwbGxuLbdu24ZFHHrFBrfNhnxsionK3ePFieHp6on379ujfvz969eplk6zCa6+9hhEjRmDMmDFo164dXFxc0KtXryLvoL9161bcvXvXmHEy16BBAzRo0AArV64sMK5jx47Ytm0b3njjDXz66aewt7fHDz/8gMuXL6Np06Z4//338b///a9EdV65ciWSkpLQokULPPPMM5g2bRr8/PyM4z08PLBixQp06NABTZs2xZ49e/DLL7/A29u7hFvFdhTCmtfoPQRSU1Ph7u6OlJQU63YuPn4caNMG2f6A4Z8rcHKqa715ExGVUnZ2NiIjI1GrVq2K8YiaKsZgMKBBgwYYOnRooVfyUuGK229Lc/xmisFa5PvcKJi5ISKqam7cuIFdu3ahS5cu0Gq1+OyzzxAZGYmRI0faumpVUoVolqoU2CxFRFRlKZVKrFmzBq1bt0aHDh1w7tw57NmzBw0aNLB11aokHoWtpXVrHNqkBBQGtGJwQ0RUpQQHB+PQoUO2rgbl4VHYSoSdHXI9paYpXgpORERkO2yWshrTpYdsliIiIrIdBjdWIoTpltQMboiIiGyHwY2VmAc3fHAmERGR7TC4sRIh9Mb3zNwQERHZDoMbK2GzFBERUcXA4MZKLIMbNksREdlK165dMX36dOPnmjVrYsmSJcVOo1AosGXLlgdetrXmYw0VqS7myqNeDG6sxBTcKIt8UBoRERWtf//+6N27d6Hj/vjjDygUCpw9e7bU8z1+/DgmTJjwoNWzMH/+/EKf+B0TE4M+ffpYdVlFycrKgpeXF3x8fKDVastlmdZQHtuIwY2VyH1u2CRFRHR/nnvuOezevRv//vtvgXGrV69Gq1at0LRp01LP19fXF05OTtao4j0FBARAo9GUy7J++uknNGrUCPXr1y+XDE1OTo5V5lMe24jBjZXImRsGN0RE9+eJJ56Ar68v1qxZYzE8PT0dGzZswHPPPYe7d+9ixIgRqFatGpycnNCkSRP88MMPxc43f7PU1atX0blzZzg4OKBhw4bYvXt3gWlee+011K1bF05OTqhduzbmzJmD3NxcAMCaNWvw5ptv4q+//oJCoYBCoTDWOX+Ty7lz5/D444/D0dER3t7emDBhAtLT043jx44diwEDBuCDDz5AYGAgvL29MXnyZOOyirNy5UqMHj0ao0ePLvQJ4vnNmzcPgYGBOHv2LNasWQMPDw+L8Vu2bLFoeZCzU1999ZXFgyx37NiBjh07wsPDA97e3njiiSdw/fp143Q5OTmYMmUKAgMD4eDggJCQECxYsMA4vjyapXgkthJTcMP+NkRUAQkBZGbaZtlOTkAJmuvt7OwwZswYrFmzBq+//rrxQLthwwbo9XqMGDEC6enpaNmyJV577TW4ublh27ZteOaZZ1CnTh20adPmnsswGAwYNGgQ/P39cfToUaSkpFj0z5G5urpizZo1CAoKwrlz5zB+/Hi4urri1VdfxbBhw3D+/Hns2LEDe/bsAQC4u7sXmEdGRgZ69eqFdu3a4fjx47hz5w6ef/55TJkyxSKA27dvHwIDA7Fv3z5cu3YNw4YNQ/PmzTF+/Pgi1+P69es4cuQINm3aBCEEZsyYgRs3biAkJKRAWSEEpk2bhl9//RV//PEHQkNDcerUqXtuKwC4du0afvrpJ2zatAkqlcq4XjNnzkTTpk2Rnp6OuXPnYuDAgThz5gyUSiU++eQTbN26FT/++CNq1KiBmzdv4ubNmyVanrUwuLESNksRUYWWmQm4uNhm2enpgLNziYo+++yzWLRoEQ4cOICuXbsCkJqkBg8eDHd3d7i7u+Pll182lp86dSp27tyJH3/8sUTBzZ49e3D58mXs3LkTQUFBAIB33323QB+QN954w/i+Zs2aePnll7Fu3Tq8+uqrcHR0hIuLC+zs7BAQEFDksr7//ntkZ2fjm2++gXPe+n/22Wfo378/3n//ffj7+wMAPD098dlnn0GlUqF+/fro168fIiIiig1uVq1ahT59+sDT0xMA0KtXL6xevRrz58+3KKfT6TB69GicPn0aBw8eRLVq1e65jczl5OTgm2++ga+vr3HY4MGDC9TF19cXFy9eROPGjREdHY2wsDB07NgRCoWi0ICrrLFZykrYLEVE9ODq16+P9u3bY9WqVQCkzMEff/yB5557DgCg1+vx9ttvo0mTJvDy8oKLiwt27tyJ6OjoEs3/0qVLCA4ONgY2ANCuXbsC5davX48OHTogICAALi4ueOONN0q8DPNlNWvWzBjYAECHDh1gMBhw5coV47BGjRoZsyIAEBgYiDt37hQ5X71ej6+//hqjR482Dhs9ejTWrFkDg8FgUXbGjBk4evQofv/991IHNgAQEhJiEdgAUrPeiBEjULt2bbi5uaFmzZoAYNw+Y8eOxZkzZ1CvXj1MmzYNu3btKvVyHxSDGythsxQRVWhOTlIGxRavUnbmfe655/DTTz8hLS0Nq1evRp06ddClSxcAwKJFi/Dxxx/jtddew759+3DmzBn06tXLap1dAeDIkSMYNWoU+vbti19//RWnT5/G66+/btVlmLO3t7f4rFAoCgQp5nbu3Ilbt25h2LBhsLOzg52dHYYPH44bN24gIiLComyPHj1w69Yt7Ny502K4UqmEEMJiWGH9fJwLybj1798fiYmJWLFiBY4ePYqjR48CMHU4btGiBSIjI/H2228jKysLQ4cOxZAhQ4rZAtbHNIOVMHNDRBWaQlHipiFbGzp0KF566SV8//33+Oabb/Diiy8a+98cOnQITz31lDFrYTAY8Pfff6Nhw4YlmneDBg1w8+ZNxMTEIDAwEADw559/WpQ5fPgwQkJC8PrrrxuH3bhxw6KMWq2GXq9HcRo0aIA1a9YgIyPDGCQcOnQISqUS9erVK1F9C7Ny5UoMHz7con4A8M4772DlypXo0aOHcdiTTz6J/v37Y+TIkVCpVBg+fDgA6QqytLQ0i7qdOXPmnsu+e/curly5ghUrVqBTp04AgIMHDxYo5+bmhmHDhmHYsGEYMmQIevfujcTERHh5ed3vapcKj8RWwz43RETW4OLigmHDhmH27NlITU3F2LFjjePCwsKwceNGHD58GJ6enli8eDHi4uJKHNx0794ddevWRXh4OBYtWoTU1NQCQUJYWBiio6Oxbt06tG7dGtu2bcPmzZstytSsWRORkZE4c+YMqlevDldX1wKXN48aNQrz5s1DeHg45s+fj/j4eEydOhXPPPOMsb9NacXHx+OXX37B1q1b0bhxY4txY8aMwcCBAwsEEQMHDsS3336LZ555BnZ2dhgyZAjatm0LJycn/Pe//8W0adNw9OjRAlepFcbT0xPe3t748ssvERgYiOjoaMyaNcuizOLFixEYGIhHHnkESqUSGzZsQEBAQIGrs8oSm6WsRAgBpdIJSqWjratCRPTQe+6555CUlIRevXpZ9I9544030KJFC/Tq1Qtdu3ZFQEAABgwYUOL5KpVKbN68GVlZWWjTpg2ef/55vPPOOxZlnnzyScyYMQNTpkxB8+bNcfjwYcyZM8eizODBg9G7d2889thj8PX1LfRydCcnJ+zcuROJiYlo3bo1hgwZgm7duuGzzz4r3cYwI3dO7tatW4Fx3bp1g6OjI7777rsC44YMGYKvv/4azzzzDDZt2gQvLy9899132L59u/Fy+vydkQujVCqxbt06nDx5Eo0bN8aMGTOwaNEiizKurq5YuHAhWrVqhdatWyMqKgrbt2+HUll+IYdC5G90q+RSU1Ph7u6OlJQUuLm52bo6RERlIjs7G5GRkRb3JyGq6Irbb0tz/GbmhoiIiCoVBjdERERUqTC4ISIiokqFwQ0RERFVKgxuiIiIqFJhcENEVIlVsQti6SFnrf2VwQ0RUSUkP6uorB4ZQFQW5P3V/Flb94O30yUiqoTs7Ozg5OSE+Ph42Nvbl+sN1Ijuh8FgQHx8PJycnGBn92DhCYMbIqJKSKFQIDAwEJGRkQWei0RUUSmVStSoUcP4LLH7xeCGiKiSUqvVCAsLY9MUPTTUarVVsowMboiIKjGlUsnHL1CVw0ZYIiIiqlQY3BAREVGlwuCGiIiIKpUq1+dGvkFQamqqjWtCREREJSUft0tyo78qF9ykpaUBAIKDg21cEyIiIiqttLQ0uLu7F1tGIarYvbkNBgNu374NV1fXB76OXpaamorg4GDcvHkTbm5uVplnZcbtVXLcVqXD7VVy3FYlx21VOmW1vYQQSEtLQ1BQ0D0vF69ymRulUonq1auXybzd3Ny445cCt1fJcVuVDrdXyXFblRy3VemUxfa6V8ZGxg7FREREVKkwuCEiIqJKhcGNFWg0GsybNw8ajcbWVXkocHuVHLdV6XB7lRy3VclxW5VORdheVa5DMREREVVuzNwQERFRpcLghoiIiCoVBjdERERUqTC4ISIiokqFwY0VLF26FDVr1oSDgwPatm2LY8eO2bpKNjd//nwoFAqLV/369Y3js7OzMXnyZHh7e8PFxQWDBw9GXFycDWtcfn7//Xf0798fQUFBUCgU2LJli8V4IQTmzp2LwMBAODo6onv37rh69apFmcTERIwaNQpubm7w8PDAc889h/T09HJci/Jzr+01duzYAvta7969LcpUle21YMECtG7dGq6urvDz88OAAQNw5coVizIl+e5FR0ejX79+cHJygp+fH1555RXodLryXJUyV5Jt1bVr1wL71sSJEy3KVIVtBQDLli1D06ZNjTfma9euHX777Tfj+Iq2XzG4eUDr16/HzJkzMW/ePJw6dQrNmjVDr169cOfOHVtXzeYaNWqEmJgY4+vgwYPGcTNmzMAvv/yCDRs24MCBA7h9+zYGDRpkw9qWn4yMDDRr1gxLly4tdPzChQvxySefYPny5Th69CicnZ3Rq1cvZGdnG8uMGjUKFy5cwO7du/Hrr7/i999/x4QJE8prFcrVvbYXAPTu3dtiX/vhhx8sxleV7XXgwAFMnjwZf/75J3bv3o3c3Fz07NkTGRkZxjL3+u7p9Xr069cPOTk5OHz4ML7++musWbMGc+fOtcUqlZmSbCsAGD9+vMW+tXDhQuO4qrKtAKB69ep47733cPLkSZw4cQKPP/44nnrqKVy4cAFABdyvBD2QNm3aiMmTJxs/6/V6ERQUJBYsWGDDWtnevHnzRLNmzQodl5ycLOzt7cWGDRuMwy5duiQAiCNHjpRTDSsGAGLz5s3GzwaDQQQEBIhFixYZhyUnJwuNRiN++OEHIYQQFy9eFADE8ePHjWV+++03oVAoxK1bt8qt7raQf3sJIUR4eLh46qmnipymKm+vO3fuCADiwIEDQoiSffe2b98ulEqliI2NNZZZtmyZcHNzE1qttnxXoBzl31ZCCNGlSxfx0ksvFTlNVd1WMk9PT/HVV19VyP2KmZsHkJOTg5MnT6J79+7GYUqlEt27d8eRI0dsWLOK4erVqwgKCkLt2rUxatQoREdHAwBOnjyJ3Nxci+1Wv3591KhRo8pvt8jISMTGxlpsG3d3d7Rt29a4bY4cOQIPDw+0atXKWKZ79+5QKpU4evRoude5Iti/fz/8/PxQr149vPjii7h7965xXFXeXikpKQAALy8vACX77h05cgRNmjSBv7+/sUyvXr2QmppqPEuvjPJvK9natWvh4+ODxo0bY/bs2cjMzDSOq6rbSq/XY926dcjIyEC7du0q5H5V5R6caU0JCQnQ6/UW/ywA8Pf3x+XLl21Uq4qhbdu2WLNmDerVq4eYmBi8+eab6NSpE86fP4/Y2Fio1Wp4eHhYTOPv74/Y2FjbVLiCkNe/sH1KHhcbGws/Pz+L8XZ2dvDy8qqS2693794YNGgQatWqhevXr+O///0v+vTpgyNHjkClUlXZ7WUwGDB9+nR06NABjRs3BoASffdiY2ML3f/kcZVRYdsKAEaOHImQkBAEBQXh7NmzeO2113DlyhVs2rQJQNXbVufOnUO7du2QnZ0NFxcXbN68GQ0bNsSZM2cq3H7F4IbKRJ8+fYzvmzZtirZt2yIkJAQ//vgjHB0dbVgzqmyGDx9ufN+kSRM0bdoUderUwf79+9GtWzcb1sy2Jk+ejPPnz1v0daPCFbWtzPtlNWnSBIGBgejWrRuuX7+OOnXqlHc1ba5evXo4c+YMUlJSsHHjRoSHh+PAgQO2rlah2Cz1AHx8fKBSqQr0CI+Li0NAQICNalUxeXh4oG7durh27RoCAgKQk5OD5ORkizLcbjCuf3H7VEBAQIEO6zqdDomJiVV++wFA7dq14ePjg2vXrgGomttrypQp+PXXX7Fv3z5Ur17dOLwk372AgIBC9z95XGVT1LYqTNu2bQHAYt+qSttKrVYjNDQULVu2xIIFC9CsWTN8/PHHFXK/YnDzANRqNVq2bImIiAjjMIPBgIiICLRr186GNat40tPTcf36dQQGBqJly5awt7e32G5XrlxBdHR0ld9utWrVQkBAgMW2SU1NxdGjR43bpl27dkhOTsbJkyeNZfbu3QuDwWD88a3K/v33X9y9exeBgYEAqtb2EkJgypQp2Lx5M/bu3YtatWpZjC/Jd69du3Y4d+6cRUC4e/duuLm5oWHDhuWzIuXgXtuqMGfOnAEAi32rKmyrohgMBmi12oq5X1m9i3IVs27dOqHRaMSaNWvExYsXxYQJE4SHh4dFj/Cq6D//+Y/Yv3+/iIyMFIcOHRLdu3cXPj4+4s6dO0IIISZOnChq1Kgh9u7dK06cOCHatWsn2rVrZ+Nal4+0tDRx+vRpcfr0aQFALF68WJw+fVrcuHFDCCHEe++9Jzw8PMTPP/8szp49K5566ilRq1YtkZWVZZxH7969xSOPPCKOHj0qDh48KMLCwsSIESNstUplqrjtlZaWJl5++WVx5MgRERkZKfbs2SNatGghwsLCRHZ2tnEeVWV7vfjii8Ld3V3s379fxMTEGF+ZmZnGMvf67ul0OtG4cWPRs2dPcebMGbFjxw7h6+srZs+ebYtVKjP32lbXrl0Tb731ljhx4oSIjIwUP//8s6hdu7bo3LmzcR5VZVsJIcSsWbPEgQMHRGRkpDh79qyYNWuWUCgUYteuXUKIirdfMbixgk8//VTUqFFDqNVq0aZNG/Hnn3/auko2N2zYMBEYGCjUarWoVq2aGDZsmLh27ZpxfFZWlpg0aZLw9PQUTk5OYuDAgSImJsaGNS4/+/btEwAKvMLDw4UQ0uXgc+bMEf7+/kKj0Yhu3bqJK1euWMzj7t27YsSIEcLFxUW4ubmJcePGibS0NBusTdkrbntlZmaKnj17Cl9fX2Fvby9CQkLE+PHjC5xcVJXtVdh2AiBWr15tLFOS715UVJTo06ePcHR0FD4+PuI///mPyM3NLee1KVv32lbR0dGic+fOwsvLS2g0GhEaGipeeeUVkZKSYjGfqrCthBDi2WefFSEhIUKtVgtfX1/RrVs3Y2AjRMXbrxRCCGH9fBARERGRbbDPDREREVUqDG6IiIioUmFwQ0RERJUKgxsiIiKqVBjcEBERUaXC4IaIiIgqFQY3REREVKkwuCGiKkmhUGDLli22rgYRlQEGN0RU7saOHQuFQlHg1bt3b1tXjYgqATtbV4CIqqbevXtj9erVFsM0Go2NakNElQkzN0RkExqNBgEBARYvT09PAFKT0bJly9CnTx84Ojqidu3a2Lhxo8X0586dw+OPPw5HR0d4e3tjwoQJSE9PtyizatUqNGrUCBqNBoGBgZgyZYrF+ISEBAwcOBBOTk4ICwvD1q1bjeOSkpIwatQo+Pr6wtHREWFhYQWCMSKqmBjcEFGFNGfOHAwePBh//fUXRo0aheHDh+PSpUsAgIyMDPTq1Quenp44fvw4NmzYgD179lgEL8uWLcPkyZMxYcIEnDt3Dlu3bkVoaKjFMt58800MHToUZ8+eRd++fTFq1CgkJiYal3/x4kX89ttvuHTpEpYtWwYfH5/y2wBEdP/K5HGcRETFCA8PFyqVSjg7O1u83nnnHSGE9MTmiRMnWkzTtm1b8eKLLwohhPjyyy+Fp6enSE9PN47ftm2bUCqVxieCBwUFiddff73IOgAQb7zxhvFzenq6ACB+++03IYQQ/fv3F+PGjbPOChNRuWKfGyKyicceewzLli2zGObl5WV8365dO4tx7dq1w5kzZwAAly5dQrNmzeDs7Gwc36FDBxgMBly5cgUKhQK3b99Gt27diq1D06ZNje+dnZ3h5uaGO3fuAABefPFFDB48GKdOnULPnj0xYMAAtG/f/r7WlYjKF4MbIrIJZ2fnAs1E1uLo6Fiicvb29hafFQoFDAYDAKBPnz64ceMGtm/fjt27d6Nbt26YPHkyPvjgA6vXl4isi31uiKhC+vPPPwt8btCgAQCgQYMG+Ouvv5CRkWEcf+jQISiVStSrVw+urq6oWbMmIiIiHqgOvr6+CA8Px3fffYclS5bgyy+/fKD5EVH5YOaGiGxCq9UiNjbWYpidnZ2x0+6GDRvQqlUrdOzYEWvXrsWxY8ewcuVKAMCoUaMwb948hIeHY/78+YiPj8fUqVPxzDPPwN/fHwAwf/58TJw4EX5+fujTpw/S0tJw6NAhTJ06tUT1mzt3Llq2bIlGjRpBq9Xi119/NQZXRFSxMbghIpvYsWMHAgMDLYbVq1cPly9fBiBdybRu3TpMmjQJgYGB+OGHH9CwYUMAgJOTE3bu3ImXXnoJrVu3hpOTEwYPHozFixcb5xUeHo7s7Gx89NFHePnll+Hj44MhQ4aUuH5qtRqzZ89GVFQUHB0d0alTJ6xbt84Ka05EZU0hhBC2rgQRkTmFQoHNmzdjwIABtq4KET2E2OeGiIiIKhUGN0RERFSpsM8NEVU4bC0nogfBzA0RERFVKgxuiIiIqFJhcENERESVCoMbIiIiqlQY3BAREVGlwuCGiIiIKhUGN0RERFSpMLghIiKiSoXBDREREVUq/weA4tbmyRzqWQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_json=resnet_model.to_json()\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch300.json\", \"w\") as json_file:\n",
        "    json_file.write(classifier_json)\n",
        "# serialize weights to HDF5\n",
        "    resnet_model.save_weights(\"/content/drive/MyDrive/Colab Notebooks/Models/my_model_Batch300.h5\")\n",
        "    resnet_model.save(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch300.h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244d45a6-5ac0-4f0b-8b40-4142ce7c8031",
        "id": "drtZsU0MeKfN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = (train_generator.class_indices)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b693336-6f45-4cab-feba-a46edc1deec3",
        "id": "f2txcb7geKfN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Pa Lulun Pao': 0, 'Pa Somba': 1, 'Pa Tangke Lumu': 2, 'Pa Tumuru': 3, 'Tidak Terdeteksi': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/Models/model_Batch300.h5', compile=False)"
      ],
      "metadata": {
        "id": "UbwvJIhneKfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memprediksi kelas menggunakan model pada data validasi\n",
        "valid_predictions = model.predict_generator(valid_generator)\n",
        "\n",
        "# Mengambil indeks kelas dengan nilai probabilitas terbesar\n",
        "valid_predicted_classes = np.argmax(valid_predictions, axis=1)\n",
        "\n",
        "# Mengambil daftar nama kelas\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Menghitung ground truth kelas pada data validasi\n",
        "valid_true_classes = valid_generator.classes\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat classification report\n",
        "classification_rep = classification_report(valid_true_classes, valid_predicted_classes, target_names=class_names)\n",
        "\n",
        "# Menghitung akurasi\n",
        "accuracy = accuracy_score(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f88a5e-4110-4672-8193-452d417df423",
        "id": "MZr5-sR7eKfO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[11 22 22 29 16]\n",
            " [18 18 19 17 28]\n",
            " [19 21 17 23 20]\n",
            " [25 19 16 23 17]\n",
            " [20 19 25 18 18]]\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    Pa Lulun Pao       0.12      0.11      0.11       100\n",
            "        Pa Somba       0.18      0.18      0.18       100\n",
            "  Pa Tangke Lumu       0.17      0.17      0.17       100\n",
            "       Pa Tumuru       0.21      0.23      0.22       100\n",
            "Tidak Terdeteksi       0.18      0.18      0.18       100\n",
            "\n",
            "        accuracy                           0.17       500\n",
            "       macro avg       0.17      0.17      0.17       500\n",
            "    weighted avg       0.17      0.17      0.17       500\n",
            "\n",
            "\n",
            "Accuracy: 0.174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat label untuk sumbu x dan y\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Membuat plot menggunakan heatmap dari seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"True Class\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nJf6_OCzeKfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvkOuuRneKfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EPOCH 400"
      ],
      "metadata": {
        "id": "zKdbEpn5xp75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows = 224\n",
        "img_cols = 224\n",
        "input_shape = (img_rows,img_cols,3)\n",
        "epochs = 400\n",
        "batch_size = 8\n",
        "num_of_classes = 5\n",
        "num_of_train_samples = 2000\n",
        "num_of_valid_samples = 500\n",
        "\n",
        "lr = 0.00001\n",
        "print('Learning rate: ', lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e260cc3d-2df4-4f69-8088-ab023a180dc4",
        "id": "g0jmo0Afxp7-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate:  1e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=train_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)\n",
        "\n",
        "\n",
        "valid_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=valid_directory,\n",
        "                                                                         class_mode='categorical',\n",
        "                                                                         batch_size=batch_size,\n",
        "                                                                         target_size=(img_rows, img_cols),\n",
        "                                                                         color_mode=\"rgb\",\n",
        "                                                                         shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b56bcdd-c8df-44bc-ebee-53e758408985",
        "id": "sZ8PtKzfxp7_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 5 classes.\n",
            "Found 500 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def getResNet50Model(lastFourTrainable=False):\n",
        "  resnet_model = ResNet50(weights='imagenet', input_shape=input_shape, include_top=True)\n",
        "  for layer in resnet_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  output = resnet_model.get_layer('avg_pool').output\n",
        "  output = Flatten(name='new_flatten')(output)\n",
        "  output = Dense(units=1024, activation='relu', name='new_fc')(output)\n",
        "  output = Dense(units=5, activation='softmax')(output)\n",
        "  resnet_model = Model(resnet_model.input, output)\n",
        "\n",
        "  if lastFourTrainable == True:\n",
        "    resnet_model.get_layer('conv5_block3_2_bn').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_conv').trainable = True\n",
        "    resnet_model.get_layer('conv5_block3_3_bn').trainable = True\n",
        "    resnet_model.get_layer('new_fc').trainable = True\n",
        "\n",
        "\n",
        "  resnet_model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "  resnet_model.summary()\n",
        "\n",
        "\n",
        "  return resnet_model"
      ],
      "metadata": {
        "id": "bgxNID1nxp7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = getResNet50Model(lastFourTrainable=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9db7ba-7b12-496f-c047-562ed7c0f014",
        "id": "hieEkvOZxp7_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " new_flatten (Flatten)          (None, 2048)         0           ['avg_pool[0][0]']               \n",
            "                                                                                                  \n",
            " new_fc (Dense)                 (None, 1024)         2098176     ['new_flatten[0][0]']            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 5)            5125        ['new_fc[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 25,691,013\n",
            "Trainable params: 3,159,045\n",
            "Non-trainable params: 22,531,968\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Menjalankan proses training dan menyimpan hasil history\n",
        "history = resnet_model.fit_generator(train_generator,\n",
        "                                     epochs=epochs,\n",
        "                                     steps_per_epoch=num_of_train_samples//batch_size,\n",
        "                                     validation_data=valid_generator,\n",
        "                                     validation_steps=num_of_valid_samples // batch_size)\n",
        "\n",
        "# Membuat DataFrame dari history\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "# Menyimpan DataFrame ke dalam file CSV\n",
        "history_df.to_csv('/content/drive/MyDrive/Colab Notebooks/History/history(0.01).csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb5e67a-9770-4d87-b582-3252adfcd296",
        "id": "NSR0VQxixp7_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "250/250 [==============================] - 20s 66ms/step - loss: 1.3337 - accuracy: 0.4665 - val_loss: 1.2866 - val_accuracy: 0.5323\n",
            "Epoch 2/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 1.0744 - accuracy: 0.6000 - val_loss: 0.9493 - val_accuracy: 0.6976\n",
            "Epoch 3/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.9308 - accuracy: 0.6790 - val_loss: 0.8198 - val_accuracy: 0.7298\n",
            "Epoch 4/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.8360 - accuracy: 0.7240 - val_loss: 0.7279 - val_accuracy: 0.7843\n",
            "Epoch 5/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.7603 - accuracy: 0.7475 - val_loss: 0.6752 - val_accuracy: 0.8145\n",
            "Epoch 6/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.7117 - accuracy: 0.7825 - val_loss: 0.6299 - val_accuracy: 0.8206\n",
            "Epoch 7/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.6506 - accuracy: 0.8020 - val_loss: 0.5839 - val_accuracy: 0.8367\n",
            "Epoch 8/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.6153 - accuracy: 0.8095 - val_loss: 0.5558 - val_accuracy: 0.8548\n",
            "Epoch 9/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.5774 - accuracy: 0.8200 - val_loss: 0.5337 - val_accuracy: 0.8448\n",
            "Epoch 10/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.5354 - accuracy: 0.8445 - val_loss: 0.5066 - val_accuracy: 0.8649\n",
            "Epoch 11/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.5048 - accuracy: 0.8510 - val_loss: 0.4829 - val_accuracy: 0.8528\n",
            "Epoch 12/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.4923 - accuracy: 0.8540 - val_loss: 0.4562 - val_accuracy: 0.8710\n",
            "Epoch 13/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.4639 - accuracy: 0.8585 - val_loss: 0.4410 - val_accuracy: 0.8770\n",
            "Epoch 14/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.4286 - accuracy: 0.8765 - val_loss: 0.4089 - val_accuracy: 0.8831\n",
            "Epoch 15/400\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.4248 - accuracy: 0.8710 - val_loss: 0.4409 - val_accuracy: 0.8629\n",
            "Epoch 16/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.3815 - accuracy: 0.8950 - val_loss: 0.4003 - val_accuracy: 0.8750\n",
            "Epoch 17/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.3765 - accuracy: 0.8895 - val_loss: 0.3819 - val_accuracy: 0.9012\n",
            "Epoch 18/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.3703 - accuracy: 0.8930 - val_loss: 0.3780 - val_accuracy: 0.8750\n",
            "Epoch 19/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.3486 - accuracy: 0.8965 - val_loss: 0.3594 - val_accuracy: 0.8831\n",
            "Epoch 20/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.3312 - accuracy: 0.9045 - val_loss: 0.3411 - val_accuracy: 0.9012\n",
            "Epoch 21/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.3160 - accuracy: 0.9145 - val_loss: 0.3407 - val_accuracy: 0.9052\n",
            "Epoch 22/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.3072 - accuracy: 0.9100 - val_loss: 0.3761 - val_accuracy: 0.8669\n",
            "Epoch 23/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.2930 - accuracy: 0.9175 - val_loss: 0.3247 - val_accuracy: 0.9012\n",
            "Epoch 24/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.2841 - accuracy: 0.9175 - val_loss: 0.3275 - val_accuracy: 0.8952\n",
            "Epoch 25/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.2743 - accuracy: 0.9225 - val_loss: 0.3185 - val_accuracy: 0.8911\n",
            "Epoch 26/400\n",
            "250/250 [==============================] - 17s 67ms/step - loss: 0.2478 - accuracy: 0.9405 - val_loss: 0.3246 - val_accuracy: 0.8891\n",
            "Epoch 27/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.2533 - accuracy: 0.9300 - val_loss: 0.2966 - val_accuracy: 0.9173\n",
            "Epoch 28/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.2399 - accuracy: 0.9290 - val_loss: 0.2884 - val_accuracy: 0.9173\n",
            "Epoch 29/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.2545 - accuracy: 0.9290 - val_loss: 0.2889 - val_accuracy: 0.9113\n",
            "Epoch 30/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.2349 - accuracy: 0.9380 - val_loss: 0.2815 - val_accuracy: 0.9173\n",
            "Epoch 31/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.2213 - accuracy: 0.9370 - val_loss: 0.2736 - val_accuracy: 0.9194\n",
            "Epoch 32/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.2237 - accuracy: 0.9380 - val_loss: 0.2759 - val_accuracy: 0.9274\n",
            "Epoch 33/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.2143 - accuracy: 0.9420 - val_loss: 0.2579 - val_accuracy: 0.9113\n",
            "Epoch 34/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.2012 - accuracy: 0.9415 - val_loss: 0.2645 - val_accuracy: 0.9133\n",
            "Epoch 35/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1954 - accuracy: 0.9515 - val_loss: 0.2552 - val_accuracy: 0.9113\n",
            "Epoch 36/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.1906 - accuracy: 0.9490 - val_loss: 0.2505 - val_accuracy: 0.9315\n",
            "Epoch 37/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.1861 - accuracy: 0.9545 - val_loss: 0.2462 - val_accuracy: 0.9274\n",
            "Epoch 38/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1818 - accuracy: 0.9510 - val_loss: 0.2565 - val_accuracy: 0.9234\n",
            "Epoch 39/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.1810 - accuracy: 0.9515 - val_loss: 0.2417 - val_accuracy: 0.9315\n",
            "Epoch 40/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1718 - accuracy: 0.9515 - val_loss: 0.2484 - val_accuracy: 0.9113\n",
            "Epoch 41/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1752 - accuracy: 0.9510 - val_loss: 0.2431 - val_accuracy: 0.9274\n",
            "Epoch 42/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1609 - accuracy: 0.9610 - val_loss: 0.2318 - val_accuracy: 0.9234\n",
            "Epoch 43/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.1556 - accuracy: 0.9585 - val_loss: 0.2239 - val_accuracy: 0.9274\n",
            "Epoch 44/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1459 - accuracy: 0.9620 - val_loss: 0.2172 - val_accuracy: 0.9234\n",
            "Epoch 45/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.1464 - accuracy: 0.9600 - val_loss: 0.2221 - val_accuracy: 0.9294\n",
            "Epoch 46/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1400 - accuracy: 0.9635 - val_loss: 0.2285 - val_accuracy: 0.9214\n",
            "Epoch 47/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.1384 - accuracy: 0.9680 - val_loss: 0.2172 - val_accuracy: 0.9375\n",
            "Epoch 48/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1346 - accuracy: 0.9655 - val_loss: 0.2201 - val_accuracy: 0.9355\n",
            "Epoch 49/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1358 - accuracy: 0.9655 - val_loss: 0.2192 - val_accuracy: 0.9375\n",
            "Epoch 50/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1377 - accuracy: 0.9620 - val_loss: 0.2179 - val_accuracy: 0.9315\n",
            "Epoch 51/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.1285 - accuracy: 0.9710 - val_loss: 0.2053 - val_accuracy: 0.9315\n",
            "Epoch 52/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.1213 - accuracy: 0.9730 - val_loss: 0.2147 - val_accuracy: 0.9214\n",
            "Epoch 53/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.1180 - accuracy: 0.9700 - val_loss: 0.2158 - val_accuracy: 0.9294\n",
            "Epoch 54/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1069 - accuracy: 0.9770 - val_loss: 0.1947 - val_accuracy: 0.9415\n",
            "Epoch 55/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1142 - accuracy: 0.9720 - val_loss: 0.1997 - val_accuracy: 0.9395\n",
            "Epoch 56/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1066 - accuracy: 0.9770 - val_loss: 0.2027 - val_accuracy: 0.9355\n",
            "Epoch 57/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.1066 - accuracy: 0.9730 - val_loss: 0.1911 - val_accuracy: 0.9355\n",
            "Epoch 58/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.1038 - accuracy: 0.9735 - val_loss: 0.1969 - val_accuracy: 0.9375\n",
            "Epoch 59/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.1023 - accuracy: 0.9765 - val_loss: 0.2023 - val_accuracy: 0.9335\n",
            "Epoch 60/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0973 - accuracy: 0.9770 - val_loss: 0.1915 - val_accuracy: 0.9355\n",
            "Epoch 61/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0939 - accuracy: 0.9780 - val_loss: 0.1894 - val_accuracy: 0.9375\n",
            "Epoch 62/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0899 - accuracy: 0.9785 - val_loss: 0.2057 - val_accuracy: 0.9335\n",
            "Epoch 63/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0791 - accuracy: 0.9860 - val_loss: 0.2143 - val_accuracy: 0.9274\n",
            "Epoch 64/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0864 - accuracy: 0.9795 - val_loss: 0.1965 - val_accuracy: 0.9335\n",
            "Epoch 65/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0907 - accuracy: 0.9785 - val_loss: 0.1872 - val_accuracy: 0.9395\n",
            "Epoch 66/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0941 - accuracy: 0.9745 - val_loss: 0.1983 - val_accuracy: 0.9435\n",
            "Epoch 67/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0844 - accuracy: 0.9795 - val_loss: 0.1812 - val_accuracy: 0.9355\n",
            "Epoch 68/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0798 - accuracy: 0.9815 - val_loss: 0.1945 - val_accuracy: 0.9415\n",
            "Epoch 69/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0733 - accuracy: 0.9850 - val_loss: 0.1884 - val_accuracy: 0.9274\n",
            "Epoch 70/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0804 - accuracy: 0.9825 - val_loss: 0.1709 - val_accuracy: 0.9456\n",
            "Epoch 71/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0758 - accuracy: 0.9810 - val_loss: 0.1912 - val_accuracy: 0.9274\n",
            "Epoch 72/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0681 - accuracy: 0.9875 - val_loss: 0.1900 - val_accuracy: 0.9315\n",
            "Epoch 73/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0721 - accuracy: 0.9840 - val_loss: 0.1769 - val_accuracy: 0.9435\n",
            "Epoch 74/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0687 - accuracy: 0.9865 - val_loss: 0.1834 - val_accuracy: 0.9375\n",
            "Epoch 75/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0710 - accuracy: 0.9825 - val_loss: 0.1917 - val_accuracy: 0.9315\n",
            "Epoch 76/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0627 - accuracy: 0.9835 - val_loss: 0.2204 - val_accuracy: 0.9274\n",
            "Epoch 77/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.0729 - accuracy: 0.9825 - val_loss: 0.1785 - val_accuracy: 0.9435\n",
            "Epoch 78/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0612 - accuracy: 0.9875 - val_loss: 0.1752 - val_accuracy: 0.9456\n",
            "Epoch 79/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0643 - accuracy: 0.9870 - val_loss: 0.1702 - val_accuracy: 0.9415\n",
            "Epoch 80/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0540 - accuracy: 0.9905 - val_loss: 0.1753 - val_accuracy: 0.9435\n",
            "Epoch 81/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0544 - accuracy: 0.9895 - val_loss: 0.1782 - val_accuracy: 0.9355\n",
            "Epoch 82/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0516 - accuracy: 0.9910 - val_loss: 0.1786 - val_accuracy: 0.9375\n",
            "Epoch 83/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0557 - accuracy: 0.9885 - val_loss: 0.1747 - val_accuracy: 0.9435\n",
            "Epoch 84/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0594 - accuracy: 0.9850 - val_loss: 0.1811 - val_accuracy: 0.9375\n",
            "Epoch 85/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0519 - accuracy: 0.9860 - val_loss: 0.2191 - val_accuracy: 0.9274\n",
            "Epoch 86/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0553 - accuracy: 0.9870 - val_loss: 0.1748 - val_accuracy: 0.9415\n",
            "Epoch 87/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0488 - accuracy: 0.9905 - val_loss: 0.1710 - val_accuracy: 0.9395\n",
            "Epoch 88/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0526 - accuracy: 0.9895 - val_loss: 0.1980 - val_accuracy: 0.9335\n",
            "Epoch 89/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0462 - accuracy: 0.9905 - val_loss: 0.1645 - val_accuracy: 0.9395\n",
            "Epoch 90/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0466 - accuracy: 0.9870 - val_loss: 0.1740 - val_accuracy: 0.9456\n",
            "Epoch 91/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0443 - accuracy: 0.9920 - val_loss: 0.1758 - val_accuracy: 0.9456\n",
            "Epoch 92/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0488 - accuracy: 0.9905 - val_loss: 0.1589 - val_accuracy: 0.9496\n",
            "Epoch 93/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0371 - accuracy: 0.9930 - val_loss: 0.1679 - val_accuracy: 0.9375\n",
            "Epoch 94/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0451 - accuracy: 0.9920 - val_loss: 0.1801 - val_accuracy: 0.9315\n",
            "Epoch 95/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0401 - accuracy: 0.9920 - val_loss: 0.1667 - val_accuracy: 0.9516\n",
            "Epoch 96/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0436 - accuracy: 0.9915 - val_loss: 0.1711 - val_accuracy: 0.9435\n",
            "Epoch 97/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0385 - accuracy: 0.9945 - val_loss: 0.1795 - val_accuracy: 0.9395\n",
            "Epoch 98/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0389 - accuracy: 0.9910 - val_loss: 0.1736 - val_accuracy: 0.9375\n",
            "Epoch 99/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0326 - accuracy: 0.9955 - val_loss: 0.2042 - val_accuracy: 0.9234\n",
            "Epoch 100/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0421 - accuracy: 0.9915 - val_loss: 0.1956 - val_accuracy: 0.9435\n",
            "Epoch 101/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0391 - accuracy: 0.9925 - val_loss: 0.1721 - val_accuracy: 0.9476\n",
            "Epoch 102/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0366 - accuracy: 0.9940 - val_loss: 0.1673 - val_accuracy: 0.9435\n",
            "Epoch 103/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0330 - accuracy: 0.9940 - val_loss: 0.1657 - val_accuracy: 0.9476\n",
            "Epoch 104/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0293 - accuracy: 0.9955 - val_loss: 0.1876 - val_accuracy: 0.9476\n",
            "Epoch 105/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0375 - accuracy: 0.9920 - val_loss: 0.1867 - val_accuracy: 0.9435\n",
            "Epoch 106/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0277 - accuracy: 0.9965 - val_loss: 0.1793 - val_accuracy: 0.9435\n",
            "Epoch 107/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.0356 - accuracy: 0.9930 - val_loss: 0.1980 - val_accuracy: 0.9335\n",
            "Epoch 108/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0295 - accuracy: 0.9935 - val_loss: 0.1776 - val_accuracy: 0.9415\n",
            "Epoch 109/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0272 - accuracy: 0.9965 - val_loss: 0.1791 - val_accuracy: 0.9456\n",
            "Epoch 110/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0311 - accuracy: 0.9930 - val_loss: 0.1729 - val_accuracy: 0.9395\n",
            "Epoch 111/400\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.0309 - accuracy: 0.9945 - val_loss: 0.1628 - val_accuracy: 0.9516\n",
            "Epoch 112/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0359 - accuracy: 0.9920 - val_loss: 0.1724 - val_accuracy: 0.9395\n",
            "Epoch 113/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0344 - accuracy: 0.9925 - val_loss: 0.1749 - val_accuracy: 0.9435\n",
            "Epoch 114/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0255 - accuracy: 0.9960 - val_loss: 0.1794 - val_accuracy: 0.9456\n",
            "Epoch 115/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0242 - accuracy: 0.9960 - val_loss: 0.1766 - val_accuracy: 0.9435\n",
            "Epoch 116/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0262 - accuracy: 0.9950 - val_loss: 0.1810 - val_accuracy: 0.9415\n",
            "Epoch 117/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0243 - accuracy: 0.9970 - val_loss: 0.1493 - val_accuracy: 0.9435\n",
            "Epoch 118/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0249 - accuracy: 0.9960 - val_loss: 0.1758 - val_accuracy: 0.9415\n",
            "Epoch 119/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0232 - accuracy: 0.9975 - val_loss: 0.1874 - val_accuracy: 0.9395\n",
            "Epoch 120/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0269 - accuracy: 0.9925 - val_loss: 0.1774 - val_accuracy: 0.9375\n",
            "Epoch 121/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0245 - accuracy: 0.9945 - val_loss: 0.1658 - val_accuracy: 0.9456\n",
            "Epoch 122/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0203 - accuracy: 0.9980 - val_loss: 0.1812 - val_accuracy: 0.9456\n",
            "Epoch 123/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0210 - accuracy: 0.9980 - val_loss: 0.1728 - val_accuracy: 0.9415\n",
            "Epoch 124/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0223 - accuracy: 0.9970 - val_loss: 0.1937 - val_accuracy: 0.9375\n",
            "Epoch 125/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0249 - accuracy: 0.9955 - val_loss: 0.1790 - val_accuracy: 0.9415\n",
            "Epoch 126/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0185 - accuracy: 0.9970 - val_loss: 0.1642 - val_accuracy: 0.9435\n",
            "Epoch 127/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0184 - accuracy: 0.9980 - val_loss: 0.1617 - val_accuracy: 0.9435\n",
            "Epoch 128/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0208 - accuracy: 0.9960 - val_loss: 0.1647 - val_accuracy: 0.9395\n",
            "Epoch 129/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0219 - accuracy: 0.9950 - val_loss: 0.1785 - val_accuracy: 0.9415\n",
            "Epoch 130/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0165 - accuracy: 0.9990 - val_loss: 0.1664 - val_accuracy: 0.9456\n",
            "Epoch 131/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0182 - accuracy: 0.9980 - val_loss: 0.1676 - val_accuracy: 0.9456\n",
            "Epoch 132/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0175 - accuracy: 0.9970 - val_loss: 0.1706 - val_accuracy: 0.9435\n",
            "Epoch 133/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0194 - accuracy: 0.9965 - val_loss: 0.1631 - val_accuracy: 0.9496\n",
            "Epoch 134/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0185 - accuracy: 0.9985 - val_loss: 0.1768 - val_accuracy: 0.9415\n",
            "Epoch 135/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0153 - accuracy: 0.9975 - val_loss: 0.1661 - val_accuracy: 0.9456\n",
            "Epoch 136/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0163 - accuracy: 0.9975 - val_loss: 0.1696 - val_accuracy: 0.9456\n",
            "Epoch 137/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0208 - accuracy: 0.9950 - val_loss: 0.1769 - val_accuracy: 0.9415\n",
            "Epoch 138/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0193 - accuracy: 0.9955 - val_loss: 0.1765 - val_accuracy: 0.9456\n",
            "Epoch 139/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0175 - accuracy: 0.9975 - val_loss: 0.1718 - val_accuracy: 0.9476\n",
            "Epoch 140/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0144 - accuracy: 0.9990 - val_loss: 0.1671 - val_accuracy: 0.9415\n",
            "Epoch 141/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0224 - accuracy: 0.9940 - val_loss: 0.1643 - val_accuracy: 0.9476\n",
            "Epoch 142/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0163 - accuracy: 0.9970 - val_loss: 0.1639 - val_accuracy: 0.9476\n",
            "Epoch 143/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0170 - accuracy: 0.9975 - val_loss: 0.1693 - val_accuracy: 0.9476\n",
            "Epoch 144/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0171 - accuracy: 0.9955 - val_loss: 0.1720 - val_accuracy: 0.9415\n",
            "Epoch 145/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0145 - accuracy: 0.9980 - val_loss: 0.1708 - val_accuracy: 0.9435\n",
            "Epoch 146/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0137 - accuracy: 0.9970 - val_loss: 0.1767 - val_accuracy: 0.9476\n",
            "Epoch 147/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0133 - accuracy: 0.9980 - val_loss: 0.1913 - val_accuracy: 0.9375\n",
            "Epoch 148/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.1850 - val_accuracy: 0.9476\n",
            "Epoch 149/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 0.1607 - val_accuracy: 0.9556\n",
            "Epoch 150/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.1631 - val_accuracy: 0.9496\n",
            "Epoch 151/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0120 - accuracy: 0.9990 - val_loss: 0.1663 - val_accuracy: 0.9496\n",
            "Epoch 152/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0108 - accuracy: 0.9985 - val_loss: 0.1687 - val_accuracy: 0.9476\n",
            "Epoch 153/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0135 - accuracy: 0.9990 - val_loss: 0.1832 - val_accuracy: 0.9456\n",
            "Epoch 154/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0132 - accuracy: 0.9980 - val_loss: 0.1801 - val_accuracy: 0.9476\n",
            "Epoch 155/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0145 - accuracy: 0.9970 - val_loss: 0.1715 - val_accuracy: 0.9516\n",
            "Epoch 156/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0127 - accuracy: 0.9980 - val_loss: 0.1758 - val_accuracy: 0.9415\n",
            "Epoch 157/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0099 - accuracy: 0.9990 - val_loss: 0.1818 - val_accuracy: 0.9435\n",
            "Epoch 158/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0101 - accuracy: 0.9990 - val_loss: 0.1605 - val_accuracy: 0.9456\n",
            "Epoch 159/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.1716 - val_accuracy: 0.9456\n",
            "Epoch 160/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0120 - accuracy: 0.9980 - val_loss: 0.1940 - val_accuracy: 0.9516\n",
            "Epoch 161/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0210 - accuracy: 0.9930 - val_loss: 0.1847 - val_accuracy: 0.9496\n",
            "Epoch 162/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0148 - accuracy: 0.9975 - val_loss: 0.1610 - val_accuracy: 0.9556\n",
            "Epoch 163/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.1693 - val_accuracy: 0.9456\n",
            "Epoch 164/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9415\n",
            "Epoch 165/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0135 - accuracy: 0.9975 - val_loss: 0.2031 - val_accuracy: 0.9335\n",
            "Epoch 166/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0134 - accuracy: 0.9980 - val_loss: 0.1607 - val_accuracy: 0.9496\n",
            "Epoch 167/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1652 - val_accuracy: 0.9415\n",
            "Epoch 168/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0084 - accuracy: 0.9995 - val_loss: 0.1838 - val_accuracy: 0.9456\n",
            "Epoch 169/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0112 - accuracy: 0.9975 - val_loss: 0.1698 - val_accuracy: 0.9516\n",
            "Epoch 170/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0082 - accuracy: 0.9995 - val_loss: 0.1864 - val_accuracy: 0.9375\n",
            "Epoch 171/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0121 - accuracy: 0.9980 - val_loss: 0.1826 - val_accuracy: 0.9496\n",
            "Epoch 172/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0084 - accuracy: 0.9995 - val_loss: 0.1742 - val_accuracy: 0.9476\n",
            "Epoch 173/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.1900 - val_accuracy: 0.9456\n",
            "Epoch 174/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.1849 - val_accuracy: 0.9456\n",
            "Epoch 175/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0075 - accuracy: 0.9995 - val_loss: 0.1852 - val_accuracy: 0.9456\n",
            "Epoch 176/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 0.1783 - val_accuracy: 0.9415\n",
            "Epoch 177/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0070 - accuracy: 0.9995 - val_loss: 0.1654 - val_accuracy: 0.9496\n",
            "Epoch 178/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.1827 - val_accuracy: 0.9496\n",
            "Epoch 179/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0089 - accuracy: 0.9990 - val_loss: 0.1674 - val_accuracy: 0.9435\n",
            "Epoch 180/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.1760 - val_accuracy: 0.9435\n",
            "Epoch 181/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0117 - accuracy: 0.9980 - val_loss: 0.1939 - val_accuracy: 0.9456\n",
            "Epoch 182/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0082 - accuracy: 0.9980 - val_loss: 0.1778 - val_accuracy: 0.9476\n",
            "Epoch 183/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 0.1692 - val_accuracy: 0.9476\n",
            "Epoch 184/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1965 - val_accuracy: 0.9456\n",
            "Epoch 185/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.1780 - val_accuracy: 0.9476\n",
            "Epoch 186/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 0.1899 - val_accuracy: 0.9375\n",
            "Epoch 187/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0074 - accuracy: 0.9995 - val_loss: 0.1859 - val_accuracy: 0.9435\n",
            "Epoch 188/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1791 - val_accuracy: 0.9496\n",
            "Epoch 189/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.1887 - val_accuracy: 0.9415\n",
            "Epoch 190/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0104 - accuracy: 0.9975 - val_loss: 0.2071 - val_accuracy: 0.9435\n",
            "Epoch 191/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1931 - val_accuracy: 0.9456\n",
            "Epoch 192/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 0.1908 - val_accuracy: 0.9496\n",
            "Epoch 193/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0060 - accuracy: 0.9995 - val_loss: 0.1732 - val_accuracy: 0.9395\n",
            "Epoch 194/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.1844 - val_accuracy: 0.9476\n",
            "Epoch 195/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.1881 - val_accuracy: 0.9456\n",
            "Epoch 196/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0063 - accuracy: 0.9995 - val_loss: 0.2216 - val_accuracy: 0.9456\n",
            "Epoch 197/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.1799 - val_accuracy: 0.9476\n",
            "Epoch 198/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0135 - accuracy: 0.9955 - val_loss: 0.2154 - val_accuracy: 0.9315\n",
            "Epoch 199/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0101 - accuracy: 0.9980 - val_loss: 0.1940 - val_accuracy: 0.9395\n",
            "Epoch 200/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 0.2116 - val_accuracy: 0.9415\n",
            "Epoch 201/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9476\n",
            "Epoch 202/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1963 - val_accuracy: 0.9456\n",
            "Epoch 203/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.1775 - val_accuracy: 0.9476\n",
            "Epoch 204/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.2129 - val_accuracy: 0.9456\n",
            "Epoch 205/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.1844 - val_accuracy: 0.9496\n",
            "Epoch 206/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.1809 - val_accuracy: 0.9516\n",
            "Epoch 207/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.1786 - val_accuracy: 0.9395\n",
            "Epoch 208/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0096 - accuracy: 0.9980 - val_loss: 0.2285 - val_accuracy: 0.9395\n",
            "Epoch 209/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.1813 - val_accuracy: 0.9516\n",
            "Epoch 210/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.1754 - val_accuracy: 0.9456\n",
            "Epoch 211/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.1836 - val_accuracy: 0.9476\n",
            "Epoch 212/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.2058 - val_accuracy: 0.9375\n",
            "Epoch 213/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9435\n",
            "Epoch 214/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.1663 - val_accuracy: 0.9516\n",
            "Epoch 215/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9516\n",
            "Epoch 216/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 0.1748 - val_accuracy: 0.9516\n",
            "Epoch 217/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0054 - accuracy: 0.9995 - val_loss: 0.1844 - val_accuracy: 0.9496\n",
            "Epoch 218/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1807 - val_accuracy: 0.9456\n",
            "Epoch 219/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.1912 - val_accuracy: 0.9496\n",
            "Epoch 220/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.1995 - val_accuracy: 0.9415\n",
            "Epoch 221/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.0045 - accuracy: 0.9995 - val_loss: 0.1996 - val_accuracy: 0.9435\n",
            "Epoch 222/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 0.2070 - val_accuracy: 0.9375\n",
            "Epoch 223/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0118 - accuracy: 0.9955 - val_loss: 0.2831 - val_accuracy: 0.9234\n",
            "Epoch 224/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.2012 - val_accuracy: 0.9435\n",
            "Epoch 225/400\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 0.2127 - val_accuracy: 0.9395\n",
            "Epoch 226/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.2038 - val_accuracy: 0.9395\n",
            "Epoch 227/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.1966 - val_accuracy: 0.9435\n",
            "Epoch 228/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0064 - accuracy: 0.9975 - val_loss: 0.2128 - val_accuracy: 0.9435\n",
            "Epoch 229/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1983 - val_accuracy: 0.9415\n",
            "Epoch 230/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.1996 - val_accuracy: 0.9516\n",
            "Epoch 231/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.1879 - val_accuracy: 0.9476\n",
            "Epoch 232/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9496\n",
            "Epoch 233/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.1903 - val_accuracy: 0.9435\n",
            "Epoch 234/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0149 - accuracy: 0.9950 - val_loss: 0.2065 - val_accuracy: 0.9395\n",
            "Epoch 235/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9415\n",
            "Epoch 236/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9415\n",
            "Epoch 237/400\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1946 - val_accuracy: 0.9556\n",
            "Epoch 238/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.1921 - val_accuracy: 0.9496\n",
            "Epoch 239/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9516\n",
            "Epoch 240/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.1991 - val_accuracy: 0.9496\n",
            "Epoch 241/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.2016 - val_accuracy: 0.9415\n",
            "Epoch 242/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.2598 - val_accuracy: 0.9335\n",
            "Epoch 243/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.2088 - val_accuracy: 0.9456\n",
            "Epoch 244/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1914 - val_accuracy: 0.9476\n",
            "Epoch 245/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.1977 - val_accuracy: 0.9375\n",
            "Epoch 246/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.1813 - val_accuracy: 0.9435\n",
            "Epoch 247/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.2201 - val_accuracy: 0.9395\n",
            "Epoch 248/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.2068 - val_accuracy: 0.9516\n",
            "Epoch 249/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9536\n",
            "Epoch 250/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.1972 - val_accuracy: 0.9516\n",
            "Epoch 251/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2149 - val_accuracy: 0.9435\n",
            "Epoch 252/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1859 - val_accuracy: 0.9556\n",
            "Epoch 253/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.1957 - val_accuracy: 0.9435\n",
            "Epoch 254/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.2084 - val_accuracy: 0.9476\n",
            "Epoch 255/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1994 - val_accuracy: 0.9435\n",
            "Epoch 256/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.1903 - val_accuracy: 0.9516\n",
            "Epoch 257/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2057 - val_accuracy: 0.9335\n",
            "Epoch 258/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.1840 - val_accuracy: 0.9516\n",
            "Epoch 259/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2062 - val_accuracy: 0.9476\n",
            "Epoch 260/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.2201 - val_accuracy: 0.9415\n",
            "Epoch 261/400\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.0087 - accuracy: 0.9975 - val_loss: 0.1966 - val_accuracy: 0.9456\n",
            "Epoch 262/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9536\n",
            "Epoch 263/400\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2258 - val_accuracy: 0.9456\n",
            "Epoch 264/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9496\n",
            "Epoch 265/400\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.1982 - val_accuracy: 0.9435\n",
            "Epoch 266/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 0.1944 - val_accuracy: 0.9456\n",
            "Epoch 267/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.2014 - val_accuracy: 0.9476\n",
            "Epoch 268/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.9435\n",
            "Epoch 269/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.2212 - val_accuracy: 0.9456\n",
            "Epoch 270/400\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.1993 - val_accuracy: 0.9516\n",
            "Epoch 271/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.1873 - val_accuracy: 0.9496\n",
            "Epoch 272/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.1800 - val_accuracy: 0.9617\n",
            "Epoch 273/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9556\n",
            "Epoch 274/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.1793 - val_accuracy: 0.9516\n",
            "Epoch 275/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9516\n",
            "Epoch 276/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1813 - val_accuracy: 0.9476\n",
            "Epoch 277/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.2010 - val_accuracy: 0.9476\n",
            "Epoch 278/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.1858 - val_accuracy: 0.9476\n",
            "Epoch 279/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.1884 - val_accuracy: 0.9496\n",
            "Epoch 280/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1810 - val_accuracy: 0.9456\n",
            "Epoch 281/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2748 - val_accuracy: 0.9294\n",
            "Epoch 282/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0051 - accuracy: 0.9995 - val_loss: 0.1814 - val_accuracy: 0.9516\n",
            "Epoch 283/400\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1961 - val_accuracy: 0.9536\n",
            "Epoch 284/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1977 - val_accuracy: 0.9456\n",
            "Epoch 285/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0091 - accuracy: 0.9975 - val_loss: 0.1840 - val_accuracy: 0.9496\n",
            "Epoch 286/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 0.2096 - val_accuracy: 0.9516\n",
            "Epoch 287/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.2110 - val_accuracy: 0.9476\n",
            "Epoch 288/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.2166 - val_accuracy: 0.9536\n",
            "Epoch 289/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0050 - accuracy: 0.9980 - val_loss: 0.2282 - val_accuracy: 0.9516\n",
            "Epoch 290/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1942 - val_accuracy: 0.9536\n",
            "Epoch 291/400\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1965 - val_accuracy: 0.9476\n",
            "Epoch 292/400\n",
            "250/250 [==============================] - 16s 65ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1983 - val_accuracy: 0.9435\n",
            "Epoch 293/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.1945 - val_accuracy: 0.9516\n",
            "Epoch 294/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1910 - val_accuracy: 0.9456\n",
            "Epoch 295/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1973 - val_accuracy: 0.9456\n",
            "Epoch 296/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1884 - val_accuracy: 0.9476\n",
            "Epoch 297/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.2004 - val_accuracy: 0.9496\n",
            "Epoch 298/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0050 - accuracy: 0.9995 - val_loss: 0.2372 - val_accuracy: 0.9415\n",
            "Epoch 299/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.2526 - val_accuracy: 0.9476\n",
            "Epoch 300/400\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.2149 - val_accuracy: 0.9435\n",
            "Epoch 301/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1870 - val_accuracy: 0.9496\n",
            "Epoch 302/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1983 - val_accuracy: 0.9476\n",
            "Epoch 303/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.2044 - val_accuracy: 0.9476\n",
            "Epoch 304/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9476\n",
            "Epoch 305/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.1843 - val_accuracy: 0.9577\n",
            "Epoch 306/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 0.1903 - val_accuracy: 0.9536\n",
            "Epoch 307/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2046 - val_accuracy: 0.9456\n",
            "Epoch 308/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2065 - val_accuracy: 0.9456\n",
            "Epoch 309/400\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 0.2884 - val_accuracy: 0.9415\n",
            "Epoch 310/400\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 0.2006 - val_accuracy: 0.9415\n",
            "Epoch 311/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 0.2530 - val_accuracy: 0.9294\n",
            "Epoch 312/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1958 - val_accuracy: 0.9415\n",
            "Epoch 313/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.1999 - val_accuracy: 0.9456\n",
            "Epoch 314/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9435\n",
            "Epoch 315/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1949 - val_accuracy: 0.9496\n",
            "Epoch 316/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 9.7391e-04 - accuracy: 1.0000 - val_loss: 0.1967 - val_accuracy: 0.9456\n",
            "Epoch 317/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 8.6548e-04 - accuracy: 1.0000 - val_loss: 0.2015 - val_accuracy: 0.9435\n",
            "Epoch 318/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9476\n",
            "Epoch 319/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.2028 - val_accuracy: 0.9577\n",
            "Epoch 320/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.1962 - val_accuracy: 0.9435\n",
            "Epoch 321/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1852 - val_accuracy: 0.9476\n",
            "Epoch 322/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9496\n",
            "Epoch 323/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2206 - val_accuracy: 0.9435\n",
            "Epoch 324/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1883 - val_accuracy: 0.9456\n",
            "Epoch 325/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.1789 - val_accuracy: 0.9496\n",
            "Epoch 326/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 0.9476\n",
            "Epoch 327/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0054 - accuracy: 0.9980 - val_loss: 0.1979 - val_accuracy: 0.9476\n",
            "Epoch 328/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.2043 - val_accuracy: 0.9415\n",
            "Epoch 329/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1906 - val_accuracy: 0.9536\n",
            "Epoch 330/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2207 - val_accuracy: 0.9415\n",
            "Epoch 331/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.2132 - val_accuracy: 0.9435\n",
            "Epoch 332/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2057 - val_accuracy: 0.9556\n",
            "Epoch 333/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9536\n",
            "Epoch 334/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1961 - val_accuracy: 0.9556\n",
            "Epoch 335/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2173 - val_accuracy: 0.9456\n",
            "Epoch 336/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2294 - val_accuracy: 0.9395\n",
            "Epoch 337/400\n",
            "250/250 [==============================] - 18s 71ms/step - loss: 0.0079 - accuracy: 0.9965 - val_loss: 0.2381 - val_accuracy: 0.9577\n",
            "Epoch 338/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.2211 - val_accuracy: 0.9476\n",
            "Epoch 339/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2002 - val_accuracy: 0.9516\n",
            "Epoch 340/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2021 - val_accuracy: 0.9516\n",
            "Epoch 341/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2026 - val_accuracy: 0.9496\n",
            "Epoch 342/400\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0023 - accuracy: 0.9995 - val_loss: 0.2098 - val_accuracy: 0.9476\n",
            "Epoch 343/400\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1961 - val_accuracy: 0.9536\n",
            "Epoch 344/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 9.6549e-04 - accuracy: 1.0000 - val_loss: 0.2045 - val_accuracy: 0.9577\n",
            "Epoch 345/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9536\n",
            "Epoch 346/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.2072 - val_accuracy: 0.9476\n",
            "Epoch 347/400\n",
            "250/250 [==============================] - 17s 68ms/step - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.2131 - val_accuracy: 0.9476\n",
            "Epoch 348/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.2434 - val_accuracy: 0.9415\n",
            "Epoch 349/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2104 - val_accuracy: 0.9415\n",
            "Epoch 350/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.2279 - val_accuracy: 0.9294\n",
            "Epoch 351/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.1919 - val_accuracy: 0.9456\n",
            "Epoch 352/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.2442 - val_accuracy: 0.9335\n",
            "Epoch 353/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9415\n",
            "Epoch 354/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0052 - accuracy: 0.9975 - val_loss: 0.2026 - val_accuracy: 0.9476\n",
            "Epoch 355/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1857 - val_accuracy: 0.9536\n",
            "Epoch 356/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9435\n",
            "Epoch 357/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.2053 - val_accuracy: 0.9456\n",
            "Epoch 358/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 8.3596e-04 - accuracy: 1.0000 - val_loss: 0.2101 - val_accuracy: 0.9516\n",
            "Epoch 359/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 0.2390 - val_accuracy: 0.9516\n",
            "Epoch 360/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 9.0443e-04 - accuracy: 1.0000 - val_loss: 0.2149 - val_accuracy: 0.9456\n",
            "Epoch 361/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 8.4759e-04 - accuracy: 1.0000 - val_loss: 0.2036 - val_accuracy: 0.9496\n",
            "Epoch 362/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 8.4736e-04 - accuracy: 1.0000 - val_loss: 0.2081 - val_accuracy: 0.9476\n",
            "Epoch 363/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0029 - accuracy: 0.9985 - val_loss: 0.2024 - val_accuracy: 0.9496\n",
            "Epoch 364/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 8.1925e-04 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9476\n",
            "Epoch 365/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2021 - val_accuracy: 0.9435\n",
            "Epoch 366/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 9.7483e-04 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9456\n",
            "Epoch 367/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 7.4434e-04 - accuracy: 1.0000 - val_loss: 0.2050 - val_accuracy: 0.9476\n",
            "Epoch 368/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 7.5419e-04 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9496\n",
            "Epoch 369/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 6.5796e-04 - accuracy: 1.0000 - val_loss: 0.2066 - val_accuracy: 0.9456\n",
            "Epoch 370/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 6.6642e-04 - accuracy: 1.0000 - val_loss: 0.2242 - val_accuracy: 0.9516\n",
            "Epoch 371/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0114 - accuracy: 0.9950 - val_loss: 0.4613 - val_accuracy: 0.9052\n",
            "Epoch 372/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.2113 - val_accuracy: 0.9355\n",
            "Epoch 373/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1901 - val_accuracy: 0.9456\n",
            "Epoch 374/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1958 - val_accuracy: 0.9435\n",
            "Epoch 375/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 7.9611e-04 - accuracy: 1.0000 - val_loss: 0.1978 - val_accuracy: 0.9496\n",
            "Epoch 376/400\n",
            "250/250 [==============================] - 15s 59ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1900 - val_accuracy: 0.9476\n",
            "Epoch 377/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2107 - val_accuracy: 0.9516\n",
            "Epoch 378/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9456\n",
            "Epoch 379/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.2048 - val_accuracy: 0.9476\n",
            "Epoch 380/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.2089 - val_accuracy: 0.9516\n",
            "Epoch 381/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0090 - accuracy: 0.9965 - val_loss: 0.2719 - val_accuracy: 0.9355\n",
            "Epoch 382/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.1967 - val_accuracy: 0.9496\n",
            "Epoch 383/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9516\n",
            "Epoch 384/400\n",
            "250/250 [==============================] - 15s 58ms/step - loss: 4.7223e-04 - accuracy: 1.0000 - val_loss: 0.1997 - val_accuracy: 0.9516\n",
            "Epoch 385/400\n",
            "250/250 [==============================] - 14s 54ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.2863 - val_accuracy: 0.9456\n",
            "Epoch 386/400\n",
            "250/250 [==============================] - 15s 60ms/step - loss: 8.5954e-04 - accuracy: 1.0000 - val_loss: 0.2127 - val_accuracy: 0.9496\n",
            "Epoch 387/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 6.2001e-04 - accuracy: 1.0000 - val_loss: 0.1981 - val_accuracy: 0.9516\n",
            "Epoch 388/400\n",
            "250/250 [==============================] - 14s 58ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9435\n",
            "Epoch 389/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.2010 - val_accuracy: 0.9456\n",
            "Epoch 390/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2120 - val_accuracy: 0.9456\n",
            "Epoch 391/400\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 7.7120e-04 - accuracy: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.9415\n",
            "Epoch 392/400\n",
            "250/250 [==============================] - 14s 55ms/step - loss: 9.2014e-04 - accuracy: 1.0000 - val_loss: 0.1988 - val_accuracy: 0.9496\n",
            "Epoch 393/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.2704 - val_accuracy: 0.9335\n",
            "Epoch 394/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2095 - val_accuracy: 0.9476\n",
            "Epoch 395/400\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 6.0069e-04 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9476\n",
            "Epoch 396/400\n",
            "250/250 [==============================] - 15s 62ms/step - loss: 0.0065 - accuracy: 0.9975 - val_loss: 0.2005 - val_accuracy: 0.9496\n",
            "Epoch 397/400\n",
            "250/250 [==============================] - 15s 61ms/step - loss: 9.2974e-04 - accuracy: 1.0000 - val_loss: 0.1965 - val_accuracy: 0.9476\n",
            "Epoch 398/400\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.2896 - val_accuracy: 0.9395\n",
            "Epoch 399/400\n",
            "250/250 [==============================] - 16s 62ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 0.2297 - val_accuracy: 0.9516\n",
            "Epoch 400/400\n",
            "250/250 [==============================] - 14s 56ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2126 - val_accuracy: 0.9577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs, acc, 'y', label='Training Akurasi')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Akurasi')\n",
        "plt.title('Akurasi Training dan Validasi')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Akurasi')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "b9a3eb62-1c30-4dc0-dcae-1acd33912c3b",
        "id": "56s4P6Evxp7_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_json=resnet_model.to_json()\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch400.json\", \"w\") as json_file:\n",
        "    json_file.write(classifier_json)\n",
        "# serialize weights to HDF5\n",
        "    resnet_model.save_weights(\"/content/drive/MyDrive/Colab Notebooks/Models/my_model_Batch400.h5\")\n",
        "    resnet_model.save(\"/content/drive/MyDrive/Colab Notebooks/Models/model_Batch400.h5\")\n",
        "    print(\"Saved model to disk\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0b5dfb-2805-446e-ba96-917e17aaef03",
        "id": "tINOKHAxxp7_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = (train_generator.class_indices)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7539e60-0af9-4c66-d302-0778dcee6183",
        "id": "oKhC1XbKxp7_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Pa Lulun Pao': 0, 'Pa Somba': 1, 'Pa Tangke Lumu': 2, 'Pa Tumuru': 3, 'Tidak Terdeteksi': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/Models/model_Batch400.h5', compile=False)"
      ],
      "metadata": {
        "id": "HTg4lNy2xp8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memprediksi kelas menggunakan model pada data validasi\n",
        "valid_predictions = model.predict_generator(valid_generator)\n",
        "\n",
        "# Mengambil indeks kelas dengan nilai probabilitas terbesar\n",
        "valid_predicted_classes = np.argmax(valid_predictions, axis=1)\n",
        "\n",
        "# Mengambil daftar nama kelas\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Menghitung ground truth kelas pada data validasi\n",
        "valid_true_classes = valid_generator.classes\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat classification report\n",
        "classification_rep = classification_report(valid_true_classes, valid_predicted_classes, target_names=class_names)\n",
        "\n",
        "# Menghitung akurasi\n",
        "accuracy = accuracy_score(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99aaee16-7c56-4dcb-9d33-cbe25f4ee1e3",
        "id": "jmacOAWLxp8A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[19 25 26 18 12]\n",
            " [18 17 24 18 23]\n",
            " [25 21 14 22 18]\n",
            " [16 22 14 27 21]\n",
            " [22 14 20 19 25]]\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    Pa Lulun Pao       0.19      0.19      0.19       100\n",
            "        Pa Somba       0.17      0.17      0.17       100\n",
            "  Pa Tangke Lumu       0.14      0.14      0.14       100\n",
            "       Pa Tumuru       0.26      0.27      0.26       100\n",
            "Tidak Terdeteksi       0.25      0.25      0.25       100\n",
            "\n",
            "        accuracy                           0.20       500\n",
            "       macro avg       0.20      0.20      0.20       500\n",
            "    weighted avg       0.20      0.20      0.20       500\n",
            "\n",
            "\n",
            "Accuracy: 0.204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat confusion matrix\n",
        "cm = confusion_matrix(valid_true_classes, valid_predicted_classes)\n",
        "\n",
        "# Membuat label untuk sumbu x dan y\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Membuat plot menggunakan heatmap dari seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "plt.ylabel(\"True Class\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BSmCuMXBxp8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-J8vCwRxp8A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}